<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Streamlining Vector Search with private model: Unifying Embedding Models with LiteLLM and Elasticsearch | ashish.one</title>
<meta name=keywords content="elasticsearch,vector search,litellm,ai,docker,embeddings,elastic"><meta name=description content="Learn how to host an embedding model using LiteLLM proxy on Docker and consume it directly from Elasticsearch for seamless vector search."><meta name=author content="Ashish Tiwari"><link rel=canonical href=https://ashish.one/blogs/vector-search-with-litellm-elasticsearch/><link crossorigin=anonymous href=https://ashish.one/assets/css/stylesheet.b5a3d000c258ff804bee14b4f9634393f86688aa1efda4b564100028be058206.css integrity="sha256-taPQAMJY/4BL7hS0+WNDk/hmiKoe/aS1ZBAAKL4FggY=" rel="preload stylesheet" as=style><link rel=icon href=https://ashish.one/favicon.png><link rel=icon type=image/png sizes=16x16 href=https://ashish.one/favicon.png><link rel=icon type=image/png sizes=32x32 href=https://ashish.one/favicon.png><link rel=apple-touch-icon href=https://ashish.one/favicon.png><link rel=mask-icon href=https://ashish.one/favicon.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://ashish.one/blogs/vector-search-with-litellm-elasticsearch/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-D2G6QZGNG1"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-D2G6QZGNG1")}</script><meta property="og:title" content="Streamlining Vector Search with private model: Unifying Embedding Models with LiteLLM and Elasticsearch"><meta property="og:description" content="Learn how to host an embedding model using LiteLLM proxy on Docker and consume it directly from Elasticsearch for seamless vector search."><meta property="og:type" content="article"><meta property="og:url" content="https://ashish.one/blogs/vector-search-with-litellm-elasticsearch/"><meta property="og:image" content="https://ashish.one/blogs/vector-search-with-litellm-elasticsearch/img/elastic/litellm-elasticsearch.png"><meta property="article:section" content="blogs"><meta property="article:published_time" content="2026-01-08T21:21:21+05:30"><meta property="article:modified_time" content="2026-01-08T21:21:21+05:30"><meta property="og:site_name" content="ashish.one"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://ashish.one/blogs/vector-search-with-litellm-elasticsearch/img/elastic/litellm-elasticsearch.png"><meta name=twitter:title content="Streamlining Vector Search with private model: Unifying Embedding Models with LiteLLM and Elasticsearch"><meta name=twitter:description content="Learn how to host an embedding model using LiteLLM proxy on Docker and consume it directly from Elasticsearch for seamless vector search."><meta name=twitter:site content="@_ashish_tiwari"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blogs","item":"https://ashish.one/blogs/"},{"@type":"ListItem","position":2,"name":"Streamlining Vector Search with private model: Unifying Embedding Models with LiteLLM and Elasticsearch","item":"https://ashish.one/blogs/vector-search-with-litellm-elasticsearch/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Streamlining Vector Search with private model: Unifying Embedding Models with LiteLLM and Elasticsearch","name":"Streamlining Vector Search with private model: Unifying Embedding Models with LiteLLM and Elasticsearch","description":"Learn how to host an embedding model using LiteLLM proxy on Docker and consume it directly from Elasticsearch for seamless vector search.","keywords":["elasticsearch","vector search","litellm","ai","docker","embeddings","elastic"],"articleBody":"In the rapidly evolving landscape of AI, managing the “plumbing” between your embedding models and your search engine is often a challenge. Developers frequently struggle with switching providers, managing API keys, and maintaining consistent API specifications.\nLiteLLM solves the model management problem by acting as a universal proxy, while Elasticsearch delivers high-performance Vector Search. By combining them, you can build a search architecture that is both flexible and powerful.\nIn this guide, we will walk through hosting an OpenAI-compatible embedding model using LiteLLM on Docker and consuming it directly from Elasticsearch to perform seamless vector search.\nPrerequisites OS: Ubuntu 24.04.3 LTS (or similar Linux environment) Docker: Ensure Docker is installed and running. Elasticsearch: An Elastic Cloud instance or a self-managed cluster. Step 1: Configure and Run LiteLLM First, we need to configure LiteLLM. This tool will act as our gateway, proxying requests to various models (like GPT-4 or specific embedding models) while presenting a unified OpenAI-compatible API to the outside world.\n1. Create the config.yaml file:\nmodel_list: - model_name: gpt-4o litellm_params: model: openai/gpt-4o api_key: os.environ/OPENAI_API_KEY - model_name: gpt-3.5-turbo litellm_params: model: openai/gpt-3.5-turbo api_key: os.environ/OPENAI_API_KEY - model_name: text-embedding-3-small litellm_params: model: openai/text-embedding-3-small api_key: os.environ/OPENAI_API_KEY 2. Run LiteLLM with Docker:\nWe will run the container with the config mounted. We also define a LITELLM_MASTER_KEY (sk-my-admin-key-123), which will serve as the static API key for our internal services to authenticate against this proxy.\ndocker run -d \\ -p 4000:4000 \\ -v $(pwd)/config.yaml:/app/config.yaml \\ -e OPENAI_API_KEY=\"enter your openai api key\" \\ -e LITELLM_MASTER_KEY=\"sk-my-admin-key-123\" \\ docker.litellm.ai/berriai/litellm:main-latest \\ --config /app/config.yaml --detailed_debug Step 2: Verify the Proxy Before we involve the search engine, let’s ensure our LiteLLM proxy is correctly handling requests.\nTest Chat Completion:\ncurl [http://0.0.0.0:4000/v1/chat/completions](http://0.0.0.0:4000/v1/chat/completions) \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer sk-my-admin-key-123\" \\ -d '{ \"model\": \"gpt-4o\", \"messages\": [ { \"role\": \"user\", \"content\": \"Hello from LiteLLM Docker!\" } ] }' Test Embedding Generation:\ncurl [http://0.0.0.0:4000/v1/embeddings](http://0.0.0.0:4000/v1/embeddings) \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer sk-my-admin-key-123\" \\ -d '{ \"model\": \"text-embedding-3-small\", \"input\": \"LiteLLM makes proxying easy.\" }' Step 3: Configure Elasticsearch Inference Now we connect the two pieces. Since LiteLLM provides an OpenAI-compatible interface, we can use the standard OpenAI connector in Elasticsearch but point it to our custom LiteLLM URL.\nFor more details on configuring these connectors, refer to the API doc\nCreate the Inference Endpoint\nRun the following in your Kibana Dev Tools. Note that the url points to our container and the api_key matches our LiteLLM master key.\nPUT _inference/text_embedding/litellm-embeddings { \"service\": \"openai\", \"service_settings\": { \"api_key\": \"sk-my-admin-key-123\", \"model_id\": \"text-embedding-3-small\", \"url\": \"http://litellm_endpoint:4000/v1/embeddings\", \"dimensions\": 128 } } Test the Inference Endpoint\nLet’s verify that Elasticsearch can generate embeddings via LiteLLM:\nPOST _inference/text_embedding/litellm-embeddings { \"input\": \"The sky above the port was the color of television tuned to a dead channel.\" } Output:\n{ \"text_embedding\": [ { \"embedding\": [ -0.023704717, -0.016358491, ... 0.12764767 ] } ] } Step 4: End-to-End Vector Search Now that the pipeline is ready, we can perform Semantic Search. We will use the semantic_text field type, which abstracts the embedding generation process—Elasticsearch calls LiteLLM automatically during data ingestion and query time.\n1. Create the Index\nPUT /litellm-test-index { \"mappings\": { \"properties\": { \"content\": { \"type\": \"semantic_text\", \"inference_id\": \"litellm-embeddings\" }, \"category\": { \"type\": \"keyword\" } } } } 2. Ingest Data\nPOST /litellm-test-index/_bulk { \"index\": {} } { \"content\": \"The quick brown fox jumps over the lazy dog.\", \"category\": \"animals\" } { \"index\": {} } { \"content\": \"Artificial intelligence is transforming software development.\", \"category\": \"tech\" } { \"index\": {} } { \"content\": \"Docker containers make deployment consistent and easy.\", \"category\": \"tech\" } 3. Verify Data\nYou can now search the index.\nGET litellm-test-index/_search Note: You will not see the embedding vectors in the _source output. This is by design to save storage space. To learn more about this behavior, check out the Elastic Search Labs blog on excluding vectors from source.\n4. Perform Semantic Query\nFinally, let’s run a natural language search. The query “tools for deploying software” does not exist in our text, but the vector search identifies the semantic relationship with “Docker containers.”\nGET /litellm-test-index/_search { \"query\": { \"semantic\": { \"field\": \"content\", \"query\": \"tools for deploying software\" } } } Conclusion This architecture demonstrates the power of decoupling your model provider from your search engine. By using LiteLLM as a standardized proxy, you gain the flexibility to swap underlying models without breaking your application. Simultaneously, Elasticsearch’s Vector Search leverages these embeddings to provide deep semantic understanding.\nTogether, LiteLLM and Elasticsearch create a robust, future-proof foundation for building intelligent search applications that go far beyond simple keyword matching.\nHappy searching!\n","wordCount":"756","inLanguage":"en","image":"https://ashish.one/blogs/vector-search-with-litellm-elasticsearch/img/elastic/litellm-elasticsearch.png","datePublished":"2026-01-08T21:21:21+05:30","dateModified":"2026-01-08T21:21:21+05:30","author":{"@type":"Person","name":"Ashish Tiwari"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://ashish.one/blogs/vector-search-with-litellm-elasticsearch/"},"publisher":{"@type":"Organization","name":"ashish.one","logo":{"@type":"ImageObject","url":"https://ashish.one/favicon.png"}}}</script></head><body class=dark id=top><script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://ashish.one/ accesskey=h title="Ashish Tiwari (Alt + H)">Ashish Tiwari</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://ashish.one/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://slides.ashish.one title=Slides><span>Slides</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li><li><a href=https://ashish.one/talks/ title=Talks><span>Talks</span></a></li><li><a href=https://ashish.one/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://ashish.one/archives/ title=Archives><span>Archives</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://ashish.one/>Home</a>&nbsp;»&nbsp;<a href=https://ashish.one/blogs/>Blogs</a></div><h1 class="post-title entry-hint-parent">Streamlining Vector Search with private model: Unifying Embedding Models with LiteLLM and Elasticsearch</h1><div class=post-description>Learn how to host an embedding model using LiteLLM proxy on Docker and consume it directly from Elasticsearch for seamless vector search.</div><div class=post-meta><span title='2026-01-08 21:21:21 +0530 IST'>January 8, 2026</span>&nbsp;·&nbsp;4 min&nbsp;·&nbsp;Ashish Tiwari&nbsp;|&nbsp;<a href=https://github.com/ashishtiwari1993/ashish.one/tree/master/content/blogs/vector-search-with-litellm-elasticsearch.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><figure class=entry-cover><img loading=eager src=https://ashish.one/img/elastic/litellm-elasticsearch.png alt="LiteLLM with Elasticsearch"></figure><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#prerequisites aria-label=Prerequisites>Prerequisites</a></li><li><a href=#step-1-configure-and-run-litellm aria-label="Step 1: Configure and Run LiteLLM">Step 1: Configure and Run LiteLLM</a></li><li><a href=#step-2-verify-the-proxy aria-label="Step 2: Verify the Proxy">Step 2: Verify the Proxy</a></li><li><a href=#step-3-configure-elasticsearch-inference aria-label="Step 3: Configure Elasticsearch Inference">Step 3: Configure Elasticsearch Inference</a></li><li><a href=#step-4-end-to-end-vector-search aria-label="Step 4: End-to-End Vector Search">Step 4: End-to-End Vector Search</a></li><li><a href=#conclusion aria-label=Conclusion>Conclusion</a></li></ul></div></details></div><div class=post-content><p>In the rapidly evolving landscape of AI, managing the &ldquo;plumbing&rdquo; between your embedding models and your search engine is often a challenge. Developers frequently struggle with switching providers, managing API keys, and maintaining consistent API specifications.</p><p><strong>LiteLLM</strong> solves the model management problem by acting as a universal proxy, while <strong>Elasticsearch</strong> delivers high-performance Vector Search. By combining them, you can build a search architecture that is both flexible and powerful.</p><p>In this guide, we will walk through hosting an OpenAI-compatible embedding model using LiteLLM on Docker and consuming it directly from Elasticsearch to perform seamless vector search.</p><h3 id=prerequisites>Prerequisites<a hidden class=anchor aria-hidden=true href=#prerequisites>#</a></h3><ul><li><strong>OS:</strong> Ubuntu 24.04.3 LTS (or similar Linux environment)</li><li><strong>Docker:</strong> Ensure Docker is installed and running.</li><li><strong>Elasticsearch:</strong> An Elastic Cloud instance or a self-managed cluster.</li></ul><hr><h3 id=step-1-configure-and-run-litellm>Step 1: Configure and Run LiteLLM<a hidden class=anchor aria-hidden=true href=#step-1-configure-and-run-litellm>#</a></h3><p>First, we need to configure LiteLLM. This tool will act as our gateway, proxying requests to various models (like GPT-4 or specific embedding models) while presenting a unified OpenAI-compatible API to the outside world.</p><p><strong>1. Create the <code>config.yaml</code> file:</strong></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>model_list</span>:
</span></span><span style=display:flex><span>  - <span style=color:#f92672>model_name</span>: <span style=color:#ae81ff>gpt-4o</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>litellm_params</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>model</span>: <span style=color:#ae81ff>openai/gpt-4o</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>api_key</span>: <span style=color:#ae81ff>os.environ/OPENAI_API_KEY</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  - <span style=color:#f92672>model_name</span>: <span style=color:#ae81ff>gpt-3.5-turbo</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>litellm_params</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>model</span>: <span style=color:#ae81ff>openai/gpt-3.5-turbo</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>api_key</span>: <span style=color:#ae81ff>os.environ/OPENAI_API_KEY</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  - <span style=color:#f92672>model_name</span>: <span style=color:#ae81ff>text-embedding-3-small</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>litellm_params</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>model</span>: <span style=color:#ae81ff>openai/text-embedding-3-small</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>api_key</span>: <span style=color:#ae81ff>os.environ/OPENAI_API_KEY</span>
</span></span></code></pre></div><p><strong>2. Run LiteLLM with Docker:</strong></p><p>We will run the container with the config mounted. We also define a <code>LITELLM_MASTER_KEY</code> (<code>sk-my-admin-key-123</code>), which will serve as the static API key for our internal services to authenticate against this proxy.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>docker run -d <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  -p 4000:4000 <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  -v <span style=color:#66d9ef>$(</span>pwd<span style=color:#66d9ef>)</span>/config.yaml:/app/config.yaml <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  -e OPENAI_API_KEY<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;enter your openai api key&#34;</span> <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  -e LITELLM_MASTER_KEY<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;sk-my-admin-key-123&#34;</span> <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  docker.litellm.ai/berriai/litellm:main-latest <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  --config /app/config.yaml --detailed_debug
</span></span></code></pre></div><hr><h3 id=step-2-verify-the-proxy>Step 2: Verify the Proxy<a hidden class=anchor aria-hidden=true href=#step-2-verify-the-proxy>#</a></h3><p>Before we involve the search engine, let’s ensure our LiteLLM proxy is correctly handling requests.</p><p><strong>Test Chat Completion:</strong></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>curl <span style=color:#f92672>[</span>http://0.0.0.0:4000/v1/chat/completions<span style=color:#f92672>](</span>http://0.0.0.0:4000/v1/chat/completions<span style=color:#f92672>)</span> <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  -H <span style=color:#e6db74>&#34;Content-Type: application/json&#34;</span> <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  -H <span style=color:#e6db74>&#34;Authorization: Bearer sk-my-admin-key-123&#34;</span> <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  -d <span style=color:#e6db74>&#39;{
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#34;model&#34;: &#34;gpt-4o&#34;,
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#34;messages&#34;: [
</span></span></span><span style=display:flex><span><span style=color:#e6db74>      { &#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;Hello from LiteLLM Docker!&#34; }
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    ]
</span></span></span><span style=display:flex><span><span style=color:#e6db74>  }&#39;</span>
</span></span></code></pre></div><p><strong>Test Embedding Generation:</strong></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>curl <span style=color:#f92672>[</span>http://0.0.0.0:4000/v1/embeddings<span style=color:#f92672>](</span>http://0.0.0.0:4000/v1/embeddings<span style=color:#f92672>)</span> <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  -H <span style=color:#e6db74>&#34;Content-Type: application/json&#34;</span> <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  -H <span style=color:#e6db74>&#34;Authorization: Bearer sk-my-admin-key-123&#34;</span> <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  -d <span style=color:#e6db74>&#39;{
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#34;model&#34;: &#34;text-embedding-3-small&#34;,
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#34;input&#34;: &#34;LiteLLM makes proxying easy.&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>  }&#39;</span>
</span></span></code></pre></div><hr><h3 id=step-3-configure-elasticsearch-inference>Step 3: Configure Elasticsearch Inference<a hidden class=anchor aria-hidden=true href=#step-3-configure-elasticsearch-inference>#</a></h3><p>Now we connect the two pieces. Since LiteLLM provides an OpenAI-compatible interface, we can use the standard OpenAI connector in Elasticsearch but point it to our custom LiteLLM URL.</p><p><em>For more details on configuring these connectors, refer to the <a href=https://www.elastic.co/docs/api/doc/elasticsearch/operation/operation-inference-put-openai>API doc</a></em></p><p><strong>Create the Inference Endpoint</strong></p><p>Run the following in your Kibana Dev Tools. Note that the <code>url</code> points to our container and the <code>api_key</code> matches our LiteLLM master key.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span><span style=color:#960050;background-color:#1e0010>PUT</span> <span style=color:#960050;background-color:#1e0010>_inference/text_embedding/litellm-embeddings</span>
</span></span><span style=display:flex><span>{
</span></span><span style=display:flex><span>   <span style=color:#f92672>&#34;service&#34;</span>: <span style=color:#e6db74>&#34;openai&#34;</span>,
</span></span><span style=display:flex><span>   <span style=color:#f92672>&#34;service_settings&#34;</span>: {
</span></span><span style=display:flex><span>       <span style=color:#f92672>&#34;api_key&#34;</span>: <span style=color:#e6db74>&#34;sk-my-admin-key-123&#34;</span>,
</span></span><span style=display:flex><span>       <span style=color:#f92672>&#34;model_id&#34;</span>: <span style=color:#e6db74>&#34;text-embedding-3-small&#34;</span>,
</span></span><span style=display:flex><span>       <span style=color:#f92672>&#34;url&#34;</span>: <span style=color:#e6db74>&#34;http://litellm_endpoint:4000/v1/embeddings&#34;</span>,
</span></span><span style=display:flex><span>       <span style=color:#f92672>&#34;dimensions&#34;</span>: <span style=color:#ae81ff>128</span>
</span></span><span style=display:flex><span>   }
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p><strong>Test the Inference Endpoint</strong></p><p>Let&rsquo;s verify that Elasticsearch can generate embeddings via LiteLLM:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span><span style=color:#960050;background-color:#1e0010>POST</span> <span style=color:#960050;background-color:#1e0010>_inference/text_embedding/litellm-embeddings</span>
</span></span><span style=display:flex><span>{
</span></span><span style=display:flex><span> <span style=color:#f92672>&#34;input&#34;</span>: <span style=color:#e6db74>&#34;The sky above the port was the color of television tuned to a dead channel.&#34;</span>
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p><strong>Output:</strong></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>{
</span></span><span style=display:flex><span> <span style=color:#f92672>&#34;text_embedding&#34;</span>: [
</span></span><span style=display:flex><span>   {
</span></span><span style=display:flex><span>     <span style=color:#f92672>&#34;embedding&#34;</span>: [
</span></span><span style=display:flex><span>       <span style=color:#ae81ff>-0.023704717</span>,
</span></span><span style=display:flex><span>       <span style=color:#ae81ff>-0.016358491</span>,
</span></span><span style=display:flex><span>       <span style=color:#960050;background-color:#1e0010>...</span>
</span></span><span style=display:flex><span>       <span style=color:#ae81ff>0.12764767</span>
</span></span><span style=display:flex><span>     ]
</span></span><span style=display:flex><span>   }
</span></span><span style=display:flex><span> ]
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><hr><h3 id=step-4-end-to-end-vector-search>Step 4: End-to-End Vector Search<a hidden class=anchor aria-hidden=true href=#step-4-end-to-end-vector-search>#</a></h3><p>Now that the pipeline is ready, we can perform <strong>Semantic Search</strong>. We will use the <code>semantic_text</code> field type, which abstracts the embedding generation process—Elasticsearch calls LiteLLM automatically during data ingestion and query time.</p><p><strong>1. Create the Index</strong></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span><span style=color:#960050;background-color:#1e0010>PUT</span> <span style=color:#960050;background-color:#1e0010>/litellm-test-index</span>
</span></span><span style=display:flex><span>{
</span></span><span style=display:flex><span> <span style=color:#f92672>&#34;mappings&#34;</span>: {
</span></span><span style=display:flex><span>   <span style=color:#f92672>&#34;properties&#34;</span>: {
</span></span><span style=display:flex><span>     <span style=color:#f92672>&#34;content&#34;</span>: {
</span></span><span style=display:flex><span>       <span style=color:#f92672>&#34;type&#34;</span>: <span style=color:#e6db74>&#34;semantic_text&#34;</span>,
</span></span><span style=display:flex><span>       <span style=color:#f92672>&#34;inference_id&#34;</span>: <span style=color:#e6db74>&#34;litellm-embeddings&#34;</span>
</span></span><span style=display:flex><span>     },
</span></span><span style=display:flex><span>     <span style=color:#f92672>&#34;category&#34;</span>: {
</span></span><span style=display:flex><span>       <span style=color:#f92672>&#34;type&#34;</span>: <span style=color:#e6db74>&#34;keyword&#34;</span>
</span></span><span style=display:flex><span>     }
</span></span><span style=display:flex><span>   }
</span></span><span style=display:flex><span> }
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p><strong>2. Ingest Data</strong></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span><span style=color:#960050;background-color:#1e0010>POST</span> <span style=color:#960050;background-color:#1e0010>/litellm-test-index/_bulk</span>
</span></span><span style=display:flex><span>{ <span style=color:#f92672>&#34;index&#34;</span>: {} }
</span></span><span style=display:flex><span>{ <span style=color:#f92672>&#34;content&#34;</span>: <span style=color:#e6db74>&#34;The quick brown fox jumps over the lazy dog.&#34;</span>, <span style=color:#f92672>&#34;category&#34;</span>: <span style=color:#e6db74>&#34;animals&#34;</span> }
</span></span><span style=display:flex><span>{ <span style=color:#f92672>&#34;index&#34;</span>: {} }
</span></span><span style=display:flex><span>{ <span style=color:#f92672>&#34;content&#34;</span>: <span style=color:#e6db74>&#34;Artificial intelligence is transforming software development.&#34;</span>, <span style=color:#f92672>&#34;category&#34;</span>: <span style=color:#e6db74>&#34;tech&#34;</span> }
</span></span><span style=display:flex><span>{ <span style=color:#f92672>&#34;index&#34;</span>: {} }
</span></span><span style=display:flex><span>{ <span style=color:#f92672>&#34;content&#34;</span>: <span style=color:#e6db74>&#34;Docker containers make deployment consistent and easy.&#34;</span>, <span style=color:#f92672>&#34;category&#34;</span>: <span style=color:#e6db74>&#34;tech&#34;</span> }
</span></span></code></pre></div><p><strong>3. Verify Data</strong></p><p>You can now search the index.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span><span style=color:#960050;background-color:#1e0010>GET</span> <span style=color:#960050;background-color:#1e0010>litellm-test-index/_search</span>
</span></span></code></pre></div><p><em>Note: You will not see the embedding vectors in the <code>_source</code> output. This is by design to save storage space. To learn more about this behavior, check out the <a href=https://www.elastic.co/search-labs/blog/elasticsearch-exclude-vectors-from-source>Elastic Search Labs blog on excluding vectors from source</a>.</em></p><p><strong>4. Perform Semantic Query</strong></p><p>Finally, let&rsquo;s run a natural language search. The query &ldquo;tools for deploying software&rdquo; does not exist in our text, but the vector search identifies the semantic relationship with &ldquo;Docker containers.&rdquo;</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span><span style=color:#960050;background-color:#1e0010>GET</span> <span style=color:#960050;background-color:#1e0010>/litellm-test-index/_search</span>
</span></span><span style=display:flex><span>{
</span></span><span style=display:flex><span> <span style=color:#f92672>&#34;query&#34;</span>: {
</span></span><span style=display:flex><span>   <span style=color:#f92672>&#34;semantic&#34;</span>: {
</span></span><span style=display:flex><span>     <span style=color:#f92672>&#34;field&#34;</span>: <span style=color:#e6db74>&#34;content&#34;</span>,
</span></span><span style=display:flex><span>     <span style=color:#f92672>&#34;query&#34;</span>: <span style=color:#e6db74>&#34;tools for deploying software&#34;</span>
</span></span><span style=display:flex><span>   }
</span></span><span style=display:flex><span> }
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><h3 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h3><p>This architecture demonstrates the power of decoupling your model provider from your search engine. By using <strong>LiteLLM</strong> as a standardized proxy, you gain the flexibility to swap underlying models without breaking your application. Simultaneously, <strong>Elasticsearch&rsquo;s Vector Search</strong> leverages these embeddings to provide deep semantic understanding.</p><p>Together, LiteLLM and Elasticsearch create a robust, future-proof foundation for building intelligent search applications that go far beyond simple keyword matching.</p><p>Happy searching!</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://ashish.one/tags/elasticsearch/>Elasticsearch</a></li><li><a href=https://ashish.one/tags/vector-search/>Vector-Search</a></li><li><a href=https://ashish.one/tags/litellm/>Litellm</a></li><li><a href=https://ashish.one/tags/ai/>Ai</a></li><li><a href=https://ashish.one/tags/docker/>Docker</a></li><li><a href=https://ashish.one/tags/embeddings/>Embeddings</a></li><li><a href=https://ashish.one/tags/elastic/>Elastic</a></li></ul><nav class=paginav><a class=next href=https://ashish.one/blogs/elastic/true-hybrid-search/><span class=title>Next »</span><br><span>Hybrid Search Done Right: Stop Calling Metadata Filters "Hybrid"</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Streamlining Vector Search with private model: Unifying Embedding Models with LiteLLM and Elasticsearch on x" href="https://x.com/intent/tweet/?text=Streamlining%20Vector%20Search%20with%20private%20model%3a%20Unifying%20Embedding%20Models%20with%20LiteLLM%20and%20Elasticsearch&amp;url=https%3a%2f%2fashish.one%2fblogs%2fvector-search-with-litellm-elasticsearch%2f&amp;hashtags=elasticsearch%2cvectorsearch%2clitellm%2cai%2cdocker%2cembeddings%2celastic"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Streamlining Vector Search with private model: Unifying Embedding Models with LiteLLM and Elasticsearch on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fashish.one%2fblogs%2fvector-search-with-litellm-elasticsearch%2f&amp;title=Streamlining%20Vector%20Search%20with%20private%20model%3a%20Unifying%20Embedding%20Models%20with%20LiteLLM%20and%20Elasticsearch&amp;summary=Streamlining%20Vector%20Search%20with%20private%20model%3a%20Unifying%20Embedding%20Models%20with%20LiteLLM%20and%20Elasticsearch&amp;source=https%3a%2f%2fashish.one%2fblogs%2fvector-search-with-litellm-elasticsearch%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Streamlining Vector Search with private model: Unifying Embedding Models with LiteLLM and Elasticsearch on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fashish.one%2fblogs%2fvector-search-with-litellm-elasticsearch%2f&title=Streamlining%20Vector%20Search%20with%20private%20model%3a%20Unifying%20Embedding%20Models%20with%20LiteLLM%20and%20Elasticsearch"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Streamlining Vector Search with private model: Unifying Embedding Models with LiteLLM and Elasticsearch on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fashish.one%2fblogs%2fvector-search-with-litellm-elasticsearch%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Streamlining Vector Search with private model: Unifying Embedding Models with LiteLLM and Elasticsearch on whatsapp" href="https://api.whatsapp.com/send?text=Streamlining%20Vector%20Search%20with%20private%20model%3a%20Unifying%20Embedding%20Models%20with%20LiteLLM%20and%20Elasticsearch%20-%20https%3a%2f%2fashish.one%2fblogs%2fvector-search-with-litellm-elasticsearch%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Streamlining Vector Search with private model: Unifying Embedding Models with LiteLLM and Elasticsearch on telegram" href="https://telegram.me/share/url?text=Streamlining%20Vector%20Search%20with%20private%20model%3a%20Unifying%20Embedding%20Models%20with%20LiteLLM%20and%20Elasticsearch&amp;url=https%3a%2f%2fashish.one%2fblogs%2fvector-search-with-litellm-elasticsearch%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Streamlining Vector Search with private model: Unifying Embedding Models with LiteLLM and Elasticsearch on ycombinator" href="https://news.ycombinator.com/submitlink?t=Streamlining%20Vector%20Search%20with%20private%20model%3a%20Unifying%20Embedding%20Models%20with%20LiteLLM%20and%20Elasticsearch&u=https%3a%2f%2fashish.one%2fblogs%2fvector-search-with-litellm-elasticsearch%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer><script src=https://giscus.app/client.js data-repo=ashishtiwari1993/ashish.one data-repo-id="MDEwOlJlcG9zaXRvcnkyMTAxMjkzNjY=" data-category=General data-category-id=DIC_kwDODIZR1s4CP1P2 data-mapping=title data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=dark_high_contrast data-lang=en crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2026 <a href=https://ashish.one/>ashish.one</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>