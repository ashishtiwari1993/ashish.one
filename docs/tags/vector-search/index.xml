<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Vector-Search on ashish.one</title>
    <link>https://ashish.one/tags/vector-search/</link>
    <description>Recent content in Vector-Search on ashish.one</description>
    <image>
      <title>ashish.one</title>
      <url>https://ashish.one/img/speaker-pic/ashish.png</url>
      <link>https://ashish.one/img/speaker-pic/ashish.png</link>
    </image>
    <generator>Hugo -- 0.136.4</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 08 Jan 2026 21:38:15 +0530</lastBuildDate>
    <atom:link href="https://ashish.one/tags/vector-search/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Streamlining Vector Search with private model: Unifying Embedding Models with LiteLLM and Elasticsearch</title>
      <link>https://ashish.one/blogs/vector-search-with-litellm-elasticsearch/</link>
      <pubDate>Thu, 08 Jan 2026 21:21:21 +0530</pubDate>
      <guid>https://ashish.one/blogs/vector-search-with-litellm-elasticsearch/</guid>
      <description>Learn how to host an embedding model using LiteLLM proxy on Docker and consume it directly from Elasticsearch for seamless vector search.</description>
      <content:encoded><![CDATA[<p>In the rapidly evolving landscape of AI, managing the &ldquo;plumbing&rdquo; between your embedding models and your search engine is often a challenge. Developers frequently struggle with switching providers, managing API keys, and maintaining consistent API specifications.</p>
<p><strong>LiteLLM</strong> solves the model management problem by acting as a universal proxy, while <strong>Elasticsearch</strong> delivers high-performance Vector Search. By combining them, you can build a search architecture that is both flexible and powerful.</p>
<p>In this guide, we will walk through hosting an OpenAI-compatible embedding model using LiteLLM on Docker and consuming it directly from Elasticsearch to perform seamless vector search.</p>
<h3 id="prerequisites">Prerequisites</h3>
<ul>
<li><strong>OS:</strong> Ubuntu 24.04.3 LTS (or similar Linux environment)</li>
<li><strong>Docker:</strong> Ensure Docker is installed and running.</li>
<li><strong>Elasticsearch:</strong> An Elastic Cloud instance or a self-managed cluster.</li>
</ul>
<hr>
<h3 id="step-1-configure-and-run-litellm">Step 1: Configure and Run LiteLLM</h3>
<p>First, we need to configure LiteLLM. This tool will act as our gateway, proxying requests to various models (like GPT-4 or specific embedding models) while presenting a unified OpenAI-compatible API to the outside world.</p>
<p><strong>1. Create the <code>config.yaml</code> file:</strong></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#f92672">model_list</span>:
</span></span><span style="display:flex;"><span>  - <span style="color:#f92672">model_name</span>: <span style="color:#ae81ff">gpt-4o</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">litellm_params</span>:
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">model</span>: <span style="color:#ae81ff">openai/gpt-4o</span>
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">api_key</span>: <span style="color:#ae81ff">os.environ/OPENAI_API_KEY</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  - <span style="color:#f92672">model_name</span>: <span style="color:#ae81ff">gpt-3.5-turbo</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">litellm_params</span>:
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">model</span>: <span style="color:#ae81ff">openai/gpt-3.5-turbo</span>
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">api_key</span>: <span style="color:#ae81ff">os.environ/OPENAI_API_KEY</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  - <span style="color:#f92672">model_name</span>: <span style="color:#ae81ff">text-embedding-3-small</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">litellm_params</span>:
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">model</span>: <span style="color:#ae81ff">openai/text-embedding-3-small</span>
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">api_key</span>: <span style="color:#ae81ff">os.environ/OPENAI_API_KEY</span>
</span></span></code></pre></div><p><strong>2. Run LiteLLM with Docker:</strong></p>
<p>We will run the container with the config mounted. We also define a <code>LITELLM_MASTER_KEY</code> (<code>sk-my-admin-key-123</code>), which will serve as the static API key for our internal services to authenticate against this proxy.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>docker run -d <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  -p 4000:4000 <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  -v <span style="color:#66d9ef">$(</span>pwd<span style="color:#66d9ef">)</span>/config.yaml:/app/config.yaml <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  -e OPENAI_API_KEY<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;enter your openai api key&#34;</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  -e LITELLM_MASTER_KEY<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;sk-my-admin-key-123&#34;</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  docker.litellm.ai/berriai/litellm:main-latest <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --config /app/config.yaml --detailed_debug
</span></span></code></pre></div><hr>
<h3 id="step-2-verify-the-proxy">Step 2: Verify the Proxy</h3>
<p>let‚Äôs ensure our LiteLLM proxy is correctly handling requests.</p>
<p><strong>Test Chat Completion:</strong></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>curl http://0.0.0.0:4000/v1/chat/completions <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  -H <span style="color:#e6db74">&#34;Content-Type: application/json&#34;</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  -H <span style="color:#e6db74">&#34;Authorization: Bearer sk-my-admin-key-123&#34;</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  -d <span style="color:#e6db74">&#39;{
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#34;model&#34;: &#34;gpt-4o&#34;,
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#34;messages&#34;: [
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">      { &#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;Hello from LiteLLM Docker!&#34; }
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    ]
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">  }&#39;</span>
</span></span></code></pre></div><p><strong>Test Embedding Generation:</strong></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>curl http://0.0.0.0:4000/v1/embeddings <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  -H <span style="color:#e6db74">&#34;Content-Type: application/json&#34;</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  -H <span style="color:#e6db74">&#34;Authorization: Bearer sk-my-admin-key-123&#34;</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  -d <span style="color:#e6db74">&#39;{
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#34;model&#34;: &#34;text-embedding-3-small&#34;,
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#34;input&#34;: &#34;LiteLLM makes proxying easy.&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">  }&#39;</span>
</span></span></code></pre></div><hr>
<h3 id="step-3-configure-elasticsearch-inference">Step 3: Configure Elasticsearch Inference</h3>
<p>Now we connect the two pieces. Let&rsquo;s <a href="https://www.elastic.co/docs/deploy-manage/deploy/elastic-cloud/create-an-elastic-cloud-hosted-deployment">create elatic deployment</a> instance from Elastic cloud.</p>
<p>Since LiteLLM provides an OpenAI-compatible interface, we can use the standard OpenAI inference in Elasticsearch but point it to our custom LiteLLM URL.</p>
<p><em>For more details on configuring these connectors, refer to the <a href="https://www.elastic.co/docs/api/doc/elasticsearch/operation/operation-inference-put-openai">API doc</a></em></p>
<p><strong>Create the Inference Endpoint</strong></p>
<p>Run the following in your Kibana Dev Tools. Note that the <code>url</code> points to our LiteLLM endpoint and the <code>api_key</code> matches our LiteLLM master key.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-json" data-lang="json"><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">PUT</span> <span style="color:#960050;background-color:#1e0010">_inference/text_embedding/litellm-embeddings</span>
</span></span><span style="display:flex;"><span>{
</span></span><span style="display:flex;"><span>   <span style="color:#f92672">&#34;service&#34;</span>: <span style="color:#e6db74">&#34;openai&#34;</span>,
</span></span><span style="display:flex;"><span>   <span style="color:#f92672">&#34;service_settings&#34;</span>: {
</span></span><span style="display:flex;"><span>       <span style="color:#f92672">&#34;api_key&#34;</span>: <span style="color:#e6db74">&#34;sk-my-admin-key-123&#34;</span>,
</span></span><span style="display:flex;"><span>       <span style="color:#f92672">&#34;model_id&#34;</span>: <span style="color:#e6db74">&#34;text-embedding-3-small&#34;</span>,
</span></span><span style="display:flex;"><span>       <span style="color:#f92672">&#34;url&#34;</span>: <span style="color:#e6db74">&#34;http://litellm_endpoint:4000/v1/embeddings&#34;</span>,
</span></span><span style="display:flex;"><span>       <span style="color:#f92672">&#34;dimensions&#34;</span>: <span style="color:#ae81ff">128</span>
</span></span><span style="display:flex;"><span>   }
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><p><em>Output</em></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-json" data-lang="json"><span style="display:flex;"><span>{
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">&#34;inference_id&#34;</span>: <span style="color:#e6db74">&#34;litellm-embeddings&#34;</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">&#34;task_type&#34;</span>: <span style="color:#e6db74">&#34;text_embedding&#34;</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">&#34;service&#34;</span>: <span style="color:#e6db74">&#34;openai&#34;</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">&#34;service_settings&#34;</span>: {
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&#34;model_id&#34;</span>: <span style="color:#e6db74">&#34;text-embedding-3-small&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&#34;url&#34;</span>: <span style="color:#e6db74">&#34;http://litellm_endpoint:4000/v1/embeddings&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&#34;similarity&#34;</span>: <span style="color:#e6db74">&#34;dot_product&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&#34;dimensions&#34;</span>: <span style="color:#ae81ff">128</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&#34;rate_limit&#34;</span>: {
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">&#34;requests_per_minute&#34;</span>: <span style="color:#ae81ff">3000</span>
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>  },
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">&#34;chunking_settings&#34;</span>: {
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&#34;strategy&#34;</span>: <span style="color:#e6db74">&#34;sentence&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&#34;max_chunk_size&#34;</span>: <span style="color:#ae81ff">250</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&#34;sentence_overlap&#34;</span>: <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>  }
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><p><strong>Test the Inference Endpoint</strong></p>
<p>Let&rsquo;s verify that Elasticsearch can generate embeddings via LiteLLM:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-json" data-lang="json"><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">POST</span> <span style="color:#960050;background-color:#1e0010">_inference/text_embedding/litellm-embeddings</span>
</span></span><span style="display:flex;"><span>{
</span></span><span style="display:flex;"><span> <span style="color:#f92672">&#34;input&#34;</span>: <span style="color:#e6db74">&#34;The sky above the port was the color of television tuned to a dead channel.&#34;</span>
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><p><strong>Output:</strong></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-json" data-lang="json"><span style="display:flex;"><span>{
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">&#34;text_embedding&#34;</span>: [
</span></span><span style="display:flex;"><span>    {
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">&#34;embedding&#34;</span>: [
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">-0.02372423</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">-0.016345346</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">-0.269933</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">-0.038996283</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">0.06547082</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">-0.04299877</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">0.059388835</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">-0.13049445</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">-0.019900626</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">0.09131928</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">0.024954043</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">0.08608698</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">-0.072268344</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">0.09623853</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">0.06260871</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">0.0264969</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">0.21161744</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">0.00086855615</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">-0.2389864</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">-0.07410188</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">-0.00018866465</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">-0.0704348</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">-0.08724971</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">0.14078017</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">-0.121192575</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">0.011660873</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">0.07177641</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">0.070211194</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">-0.01770932</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">0.13067332</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">0.02551305</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">-0.16734414</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">0.0144000035</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">-0.033562742</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">0.12226587</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">-0.097311825</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">0.08868077</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">-0.011236028</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">-0.08631058</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">-0.13541369</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">0.007585716</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">-0.08742859</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">0.19337147</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">0.072447225</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">-0.025155285</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">-0.07025592</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">-0.0071552815</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">0.03591057</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">0.118509345</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">0.028598765</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">-0.094628595</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">0.051115543</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">-0.03497144</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">0.053888213</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">0.046196286</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">-0.091945365</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">-0.013449693</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">-0.025759013</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">0.13764973</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">0.046554048</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">0.08841244</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">0.05460374</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">-0.025826093</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">0.030611187</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">-0.012197519</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">-0.1722634</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">-0.02372423</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">-0.0021340067</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">-0.025222367</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">0.06158014</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">0.09740127</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">-0.010352798</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">-0.05701865</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">-0.06381617</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">-0.0127341645</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">0.042149078</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">0.034144107</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">-0.012432301</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">0.1711901</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">-0.13112053</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">-0.0789317</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">0.12879506</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">-0.14677271</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">-0.02915777</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">-0.0157528</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">0.007390064</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">-0.04659877</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">-0.10473543</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">0.04152299</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">0.16662861</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">0.017060874</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">0.12333916</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">-0.06256399</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">-0.09167704</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">-0.023120502</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">0.03872796</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">-0.17852427</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">-0.020314291</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">0.023232304</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">-0.050578896</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">0.05983604</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">0.020470813</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">-0.14417891</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">-0.0002360055</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">-0.0051400634</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">0.07334163</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">0.12655903</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">0.0008245344</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">-0.057644736</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">-0.067080766</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">-0.02989566</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">-0.04223852</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">-0.14838265</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">0.1329988</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">0.009531058</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">0.038638517</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">0.08076524</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">-0.0014939444</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">0.24846715</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">-0.012711804</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">-0.003342858</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">0.014411184</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">-0.077232316</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">0.033629823</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">0.082598776</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">-0.020850938</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">0.08376151</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">0.12772177</span>
</span></span><span style="display:flex;"><span>      ]
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>  ]
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><hr>
<h3 id="step-4-end-to-end-vector-search">Step 4: End-to-End Vector Search</h3>
<p>Now that the pipeline is ready, we can perform <strong>Semantic Search</strong>. We will use the (semantic_text)[https://www.elastic.co/docs/reference/elasticsearch/mapping-reference/semantic-text] field type, which abstracts the embedding generation process‚ÄîElasticsearch calls LiteLLM automatically during data ingestion and query time.</p>
<p><strong>1. Create the Index</strong></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-json" data-lang="json"><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">PUT</span> <span style="color:#960050;background-color:#1e0010">/litellm-test-index</span>
</span></span><span style="display:flex;"><span>{
</span></span><span style="display:flex;"><span> <span style="color:#f92672">&#34;mappings&#34;</span>: {
</span></span><span style="display:flex;"><span>   <span style="color:#f92672">&#34;properties&#34;</span>: {
</span></span><span style="display:flex;"><span>     <span style="color:#f92672">&#34;content&#34;</span>: {
</span></span><span style="display:flex;"><span>       <span style="color:#f92672">&#34;type&#34;</span>: <span style="color:#e6db74">&#34;semantic_text&#34;</span>,
</span></span><span style="display:flex;"><span>       <span style="color:#f92672">&#34;inference_id&#34;</span>: <span style="color:#e6db74">&#34;litellm-embeddings&#34;</span>
</span></span><span style="display:flex;"><span>     },
</span></span><span style="display:flex;"><span>     <span style="color:#f92672">&#34;category&#34;</span>: {
</span></span><span style="display:flex;"><span>       <span style="color:#f92672">&#34;type&#34;</span>: <span style="color:#e6db74">&#34;keyword&#34;</span>
</span></span><span style="display:flex;"><span>     }
</span></span><span style="display:flex;"><span>   }
</span></span><span style="display:flex;"><span> }
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><p><strong>2. Ingest Data</strong></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-json" data-lang="json"><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">POST</span> <span style="color:#960050;background-color:#1e0010">/litellm-test-index/_bulk</span>
</span></span><span style="display:flex;"><span>{ <span style="color:#f92672">&#34;index&#34;</span>: {} }
</span></span><span style="display:flex;"><span>{ <span style="color:#f92672">&#34;content&#34;</span>: <span style="color:#e6db74">&#34;The quick brown fox jumps over the lazy dog.&#34;</span>, <span style="color:#f92672">&#34;category&#34;</span>: <span style="color:#e6db74">&#34;animals&#34;</span> }
</span></span><span style="display:flex;"><span>{ <span style="color:#f92672">&#34;index&#34;</span>: {} }
</span></span><span style="display:flex;"><span>{ <span style="color:#f92672">&#34;content&#34;</span>: <span style="color:#e6db74">&#34;Artificial intelligence is transforming software development.&#34;</span>, <span style="color:#f92672">&#34;category&#34;</span>: <span style="color:#e6db74">&#34;tech&#34;</span> }
</span></span><span style="display:flex;"><span>{ <span style="color:#f92672">&#34;index&#34;</span>: {} }
</span></span><span style="display:flex;"><span>{ <span style="color:#f92672">&#34;content&#34;</span>: <span style="color:#e6db74">&#34;Docker containers make deployment consistent and easy.&#34;</span>, <span style="color:#f92672">&#34;category&#34;</span>: <span style="color:#e6db74">&#34;tech&#34;</span> }
</span></span></code></pre></div><p><strong>3. Verify Data</strong></p>
<p>You can now search the index.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-json" data-lang="json"><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">GET</span> <span style="color:#960050;background-color:#1e0010">litellm-test-index/_search</span>
</span></span></code></pre></div><p><em>Note: You will not see the embedding vectors in the <code>_source</code> output. This is by design to save storage space. To learn more about this behavior, check out the <a href="https://www.elastic.co/search-labs/blog/elasticsearch-exclude-vectors-from-source">Elastic Search Labs blog on excluding vectors from source</a>.</em></p>
<p><strong>4. Perform Semantic Query</strong></p>
<p>Finally, let&rsquo;s run a natural language search. The query &ldquo;tools for deploying software&rdquo; does not exist in our text, but the vector search identifies the semantic relationship with &ldquo;Docker containers.&rdquo;</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-json" data-lang="json"><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">GET</span> <span style="color:#960050;background-color:#1e0010">/litellm-test-index/_search</span>
</span></span><span style="display:flex;"><span>{
</span></span><span style="display:flex;"><span> <span style="color:#f92672">&#34;query&#34;</span>: {
</span></span><span style="display:flex;"><span>   <span style="color:#f92672">&#34;semantic&#34;</span>: {
</span></span><span style="display:flex;"><span>     <span style="color:#f92672">&#34;field&#34;</span>: <span style="color:#e6db74">&#34;content&#34;</span>,
</span></span><span style="display:flex;"><span>     <span style="color:#f92672">&#34;query&#34;</span>: <span style="color:#e6db74">&#34;tools for deploying software&#34;</span>
</span></span><span style="display:flex;"><span>   }
</span></span><span style="display:flex;"><span> }
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><p><em>Output</em></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-json" data-lang="json"><span style="display:flex;"><span>{
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">&#34;took&#34;</span>: <span style="color:#ae81ff">501</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">&#34;timed_out&#34;</span>: <span style="color:#66d9ef">false</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">&#34;_shards&#34;</span>: {
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&#34;total&#34;</span>: <span style="color:#ae81ff">10</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&#34;successful&#34;</span>: <span style="color:#ae81ff">10</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&#34;skipped&#34;</span>: <span style="color:#ae81ff">0</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&#34;failed&#34;</span>: <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>  },
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">&#34;hits&#34;</span>: {
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&#34;total&#34;</span>: {
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">&#34;value&#34;</span>: <span style="color:#ae81ff">3</span>,
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">&#34;relation&#34;</span>: <span style="color:#e6db74">&#34;eq&#34;</span>
</span></span><span style="display:flex;"><span>    },
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&#34;max_score&#34;</span>: <span style="color:#ae81ff">0.7422092</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&#34;hits&#34;</span>: [
</span></span><span style="display:flex;"><span>      {
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">&#34;_index&#34;</span>: <span style="color:#e6db74">&#34;litellm-test-index&#34;</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">&#34;_id&#34;</span>: <span style="color:#e6db74">&#34;UZR4pJsBGjew_vztklFP&#34;</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">&#34;_score&#34;</span>: <span style="color:#ae81ff">0.7422092</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">&#34;_source&#34;</span>: {
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">&#34;content&#34;</span>: <span style="color:#e6db74">&#34;Docker containers make deployment consistent and easy.&#34;</span>,
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">&#34;category&#34;</span>: <span style="color:#e6db74">&#34;tech&#34;</span>
</span></span><span style="display:flex;"><span>        }
</span></span><span style="display:flex;"><span>      },
</span></span><span style="display:flex;"><span>      {
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">&#34;_index&#34;</span>: <span style="color:#e6db74">&#34;litellm-test-index&#34;</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">&#34;_id&#34;</span>: <span style="color:#e6db74">&#34;UJR4pJsBGjew_vztklFP&#34;</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">&#34;_score&#34;</span>: <span style="color:#ae81ff">0.68897665</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">&#34;_source&#34;</span>: {
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">&#34;content&#34;</span>: <span style="color:#e6db74">&#34;Artificial intelligence is transforming software development.&#34;</span>,
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">&#34;category&#34;</span>: <span style="color:#e6db74">&#34;tech&#34;</span>
</span></span><span style="display:flex;"><span>        }
</span></span><span style="display:flex;"><span>      },
</span></span><span style="display:flex;"><span>      {
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">&#34;_index&#34;</span>: <span style="color:#e6db74">&#34;litellm-test-index&#34;</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">&#34;_id&#34;</span>: <span style="color:#e6db74">&#34;T5R4pJsBGjew_vztklFP&#34;</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">&#34;_score&#34;</span>: <span style="color:#ae81ff">0.5035514</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">&#34;_source&#34;</span>: {
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">&#34;content&#34;</span>: <span style="color:#e6db74">&#34;The quick brown fox jumps over the lazy dog.&#34;</span>,
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">&#34;category&#34;</span>: <span style="color:#e6db74">&#34;animals&#34;</span>
</span></span><span style="display:flex;"><span>        }
</span></span><span style="display:flex;"><span>      }
</span></span><span style="display:flex;"><span>    ]
</span></span><span style="display:flex;"><span>  }
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><h3 id="conclusion">Conclusion</h3>
<p>This architecture demonstrates the power of decoupling your model provider from your search engine. By using <strong>LiteLLM</strong> as a standardized proxy, you gain the flexibility to swap underlying models without breaking your application. Simultaneously, <strong>Elasticsearch&rsquo;s Vector Search</strong> leverages these embeddings to provide deep semantic understanding.</p>
<p>Together, LiteLLM and Elasticsearch create a robust, future-proof foundation for building intelligent search applications that go far beyond simple keyword matching.</p>
<p>Happy searching!</p>
]]></content:encoded>
    </item>
    <item>
      <title>Hybrid Search Done Right: Stop Calling Metadata Filters &#34;Hybrid&#34;</title>
      <link>https://ashish.one/blogs/elastic/true-hybrid-search/</link>
      <pubDate>Mon, 25 Aug 2025 12:09:27 +0530</pubDate>
      <guid>https://ashish.one/blogs/elastic/true-hybrid-search/</guid>
      <description>&lt;h1 id=&#34;hybrid-search-done-right-stop-calling-metadata-filters-hybrid&#34;&gt;Hybrid Search Done Right: Stop Calling Metadata Filters &amp;ldquo;Hybrid&amp;rdquo;&lt;/h1&gt;
&lt;p&gt;Everyone‚Äôs talking about &lt;strong&gt;hybrid search&lt;/strong&gt; right now.
But here‚Äôs the uncomfortable truth:&lt;/p&gt;
&lt;p&gt;üëâ Just because you glued vector search onto your database and added metadata filters doesn‚Äôt mean you‚Äôve built &lt;em&gt;true&lt;/em&gt; hybrid search.&lt;/p&gt;
&lt;p&gt;That‚Äôs like duct-taping a spoiler on a hatchback and calling it a race car. üöóüí®&lt;/p&gt;
&lt;p&gt;Hybrid search is more than just ‚Äúkeyword + vector + filter.‚Äù
It‚Äôs about &lt;strong&gt;field-level design, reranking, scoring, and scale&lt;/strong&gt;.&lt;/p&gt;</description>
      <content:encoded><![CDATA[<h1 id="hybrid-search-done-right-stop-calling-metadata-filters-hybrid">Hybrid Search Done Right: Stop Calling Metadata Filters &ldquo;Hybrid&rdquo;</h1>
<p>Everyone‚Äôs talking about <strong>hybrid search</strong> right now.
But here‚Äôs the uncomfortable truth:</p>
<p>üëâ Just because you glued vector search onto your database and added metadata filters doesn‚Äôt mean you‚Äôve built <em>true</em> hybrid search.</p>
<p>That‚Äôs like duct-taping a spoiler on a hatchback and calling it a race car. üöóüí®</p>
<p>Hybrid search is more than just ‚Äúkeyword + vector + filter.‚Äù
It‚Äôs about <strong>field-level design, reranking, scoring, and scale</strong>.</p>
<p>Let‚Äôs break this down.</p>
<hr>
<h2 id="field-level-truth-not-every-field-deserves-semantic-search">Field-Level Truth: Not Every Field Deserves Semantic Search</h2>
<p>The biggest mistake I see:
Teams running semantic + lexical search on <em>every</em> field in their JSON docs.</p>
<p>That‚Äôs how you kill relevance.</p>
<h3 id="lexical-only-fields">Lexical-Only Fields</h3>
<p>These work best with exact match:</p>
<ul>
<li><code>title</code></li>
<li><code>name</code></li>
<li><code>issue_id</code></li>
<li><code>category</code></li>
<li><code>tags</code></li>
</ul>
<p>If I search for <code>issue_id: 1245</code>, I want <strong>1245</strong> ‚Äî not ‚Äúsimilar looking IDs.‚Äù</p>
<h3 id="semantic-only-fields">Semantic-Only Fields</h3>
<p>These benefit from embeddings:</p>
<ul>
<li><code>product_description</code></li>
<li><code>movie_storyline</code></li>
<li><code>customer_feedback</code></li>
<li><code>reviews</code></li>
</ul>
<p>This is where meaning &gt; keywords.
‚ÄúBattery dies too quickly‚Äù should match with ‚Äúpoor battery life.‚Äù</p>
<h3 id="the-formula-that-works">The Formula That Works</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-text" data-lang="text"><span style="display:flex;"><span>Final Score = RRF(
</span></span><span style="display:flex;"><span>   Lexical(title, name, category) +
</span></span><span style="display:flex;"><span>   Semantic(description, feedback, reviews)
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div><p>Hybrid works when <strong>each field is treated for what it is</strong>.</p>
<h3 id="quick-reference-table">Quick Reference Table</h3>
<table>
  <thead>
      <tr>
          <th>Field Type</th>
          <th>Example Fields</th>
          <th>Best Search Method</th>
          <th>Why?</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Metadata / Identifiers</td>
          <td><code>id</code>, <code>issue_id</code>, <code>category</code></td>
          <td>Lexical</td>
          <td>Needs exact matching, filtering</td>
      </tr>
      <tr>
          <td>Structured Short Text</td>
          <td><code>title</code>, <code>name</code>, <code>tags</code></td>
          <td>Lexical</td>
          <td>Precision &gt; meaning</td>
      </tr>
      <tr>
          <td>Unstructured Text</td>
          <td><code>description</code>, <code>reviews</code></td>
          <td>Semantic</td>
          <td>Context + meaning matter</td>
      </tr>
  </tbody>
</table>
<hr>
<h2 id="metadata-filters-keep-them-lexical">Metadata Filters: Keep Them Lexical</h2>
<p>One of the biggest anti-patterns: vectorizing filters.</p>
<p>Filters are not semantic. They should remain <strong>exact</strong>.</p>
<ul>
<li>Price ‚Üí numeric filters</li>
<li>Stock availability ‚Üí boolean</li>
<li>Categories, IDs ‚Üí keyword</li>
<li>Timestamps, geo ‚Üí structured fields</li>
</ul>
<p>Search is about <em>precision here</em>. If you fuzzy up filters with embeddings, you‚Äôll tank user trust.</p>
<hr>
<h2 id="hybrid--metadata-filters--vectors">Hybrid ‚â† Metadata Filters + Vectors</h2>
<p>Here‚Äôs the thing. Some vector DBs shout <em>‚Äúwe support keyword search now!‚Äù</em>
But if you look closer, it‚Äôs often:</p>
<ul>
<li>A bolted-on library</li>
<li>Or patched-in metadata filters</li>
</ul>
<p>Yeah, technically it works.
But <strong>that‚Äôs not hybrid</strong>.</p>
<p>Search is not just filters.</p>
<hr>
<h2 id="what-real-hybrid-search-looks-like">What Real Hybrid Search Looks Like</h2>
<p>Let‚Äôs talk the real stuff beyond filters + vectors:</p>
<ul>
<li><strong>Autocomplete</strong> ‚Äî search-as-you-type, phrase suggesters, boosting, user-friendly completion.</li>
<li><strong>Facets</strong> ‚Äî powerful aggregations: geo boundaries, date histograms, bucketing. Not just string counts.</li>
<li><strong>Native rescoring</strong> ‚Äî query rescorer, rank features, boosting for freshness, popularity, personalization.</li>
<li><strong>Rich documents</strong> ‚Äî nested JSON, arrays, geo points, IPs. Search isn‚Äôt flat.</li>
<li><strong>Geo Search</strong> ‚Äî aggregations, polygons, proximity scoring. Beyond just ‚Äúin radius.‚Äù</li>
<li><strong>Access control</strong> ‚Äî index ‚Üí document ‚Üí field level security. For lexical <strong>and</strong> vector fields.</li>
<li><strong>Data enrichment pipelines</strong> ‚Äî entity extraction, tagging, embeddings, LLM-based enrichment.</li>
<li><strong>Scalability</strong> ‚Äî petabytes of data, tiered storage, lifecycle management. Search doesn‚Äôt stop at 10M docs.</li>
</ul>
<p>This is hybrid.</p>
<hr>
<h2 id="vectors-ask-the-hard-questions">Vectors: Ask the Hard Questions</h2>
<p>Not all vector support is equal. Check:</p>
<ul>
<li>Is vector search <strong>native to the engine</strong> or bolted on?</li>
<li>Query types: KNN, ANN, hybrid filtering?</li>
<li>Can it run at scale with good recall + low latency?</li>
<li>Quantization: int4, int8, binary, advanced methods like <strong>BBQ (Better Binary Quantization)</strong>?</li>
<li>Filters on HNSW: is it pre-filtering or filter-aware indexing (like ACORN)?</li>
<li>Can it blend semantic + lexical scoring at query time?
<ul>
<li>RRF (Reciprocal Rank Fusion)</li>
<li>Linear retrievers</li>
<li>Semantic reranking with LLMs</li>
</ul>
</li>
</ul>
<p>If your engine can‚Äôt do these, it‚Äôs not serious about hybrid.</p>
<hr>
<h2 id="implementation-note-one-query-should-do-it">Implementation Note: One Query Should Do It</h2>
<p>If you need to stitch 3‚Äì4 services together just to get:</p>
<ul>
<li>Facets</li>
<li>Filters</li>
<li>Semantic results</li>
<li>Lexical results</li>
<li>Reranking</li>
</ul>
<p>‚Ä¶you‚Äôre adding <strong>latency + complexity</strong>.</p>
<p>A true hybrid engine should give you everything in <strong>one structured query</strong>.</p>
<hr>
<h2 id="practical-takeaways">Practical Takeaways</h2>
<ol>
<li><strong>Not all fields deserve semantic.</strong> Treat fields differently.</li>
<li><strong>Filters are lexical.</strong> Stop semantic-izing metadata.</li>
<li><strong>Hybrid = lexical + semantic + rescoring + scale.</strong> Not just bolted-on keyword support.</li>
<li><strong>Evaluate vectors deeply.</strong> Native support, quantization, filtering, reranking.</li>
<li><strong>One query &gt; stitched services.</strong> Hybrid done right = clean pipeline, low latency.</li>
</ol>
<hr>
<h2 id="closing-thoughts">Closing Thoughts</h2>
<p>Hybrid search is not a checkbox.
It‚Äôs about designing for <strong>relevance + scale + control</strong>.</p>
<p>If your ‚Äúhybrid‚Äù is just vectors + filters, you‚Äôve built a demo ‚Äî not a real search engine.</p>
<p>True hybrid is <strong>Lexical + Semantic + Scoring + Access + Scale + Control</strong>.
That‚Äôs when search feels natural, accurate, and production-ready.</p>
<hr>
]]></content:encoded>
    </item>
    <item>
      <title>Laracon 2024</title>
      <link>https://ashish.one/talks/laracon-2024/</link>
      <pubDate>Mon, 25 Aug 2025 11:29:16 +0530</pubDate>
      <guid>https://ashish.one/talks/laracon-2024/</guid>
      <description>&lt;h1 id=&#34;-talk-summary-no-code-rag-chatbot-with-php-llms--elasticsearch&#34;&gt;üé§ Talk Summary: No-Code RAG Chatbot with PHP, LLMs &amp;amp; Elasticsearch&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Speaker:&lt;/strong&gt; Ashish Diwali (Senior Developer Advocate, Elastic)&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-introduction&#34;&gt;üîë Introduction&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Topic: Integrating &lt;strong&gt;Generative AI (LLMs)&lt;/strong&gt; with &lt;strong&gt;PHP&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Goal: Show how to build &lt;strong&gt;chat assistants, semantic search, and vector search&lt;/strong&gt; without heavy ML expertise.&lt;/li&gt;
&lt;li&gt;Demo focus: Using &lt;strong&gt;Elasticsearch + PHP + LLM (LLaMA 3.1)&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-core-concepts&#34;&gt;üß© Core Concepts&lt;/h2&gt;
&lt;h3 id=&#34;1-prompt-engineering&#34;&gt;1. Prompt Engineering&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;LLMs generate responses based on prompts ‚Üí predicting next words.&lt;/li&gt;
&lt;li&gt;Techniques:
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Zero-shot inference&lt;/strong&gt; ‚Üí direct classification or tagging.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;One-shot inference&lt;/strong&gt; ‚Üí provide one example in the prompt.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Few-shot inference&lt;/strong&gt; ‚Üí multiple examples ‚Üí useful for structured outputs (SQL, JSON, XML).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Iteration + context = &lt;strong&gt;In-context learning (ICL)&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;2-llm-limitations&#34;&gt;2. LLM Limitations&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;‚ùå Hallucinations (wrong answers).&lt;/li&gt;
&lt;li&gt;‚ùå Complex to build/train from scratch.&lt;/li&gt;
&lt;li&gt;‚ùå No real-time / private data access.&lt;/li&gt;
&lt;li&gt;‚ùå Privacy &amp;amp; security concerns (especially in banking, public sector).&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;3-rag-retrieval-augmented-generation&#34;&gt;3. RAG (Retrieval-Augmented Generation)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Solution to limitations.&lt;/li&gt;
&lt;li&gt;Workflow:
&lt;ol&gt;
&lt;li&gt;User query ‚Üí hits database/vector DB (e.g., Elasticsearch).&lt;/li&gt;
&lt;li&gt;Retrieve &lt;strong&gt;top 5‚Äì10 relevant docs&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Pass as context window ‚Üí LLM generates accurate answer.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Benefits:
&lt;ul&gt;
&lt;li&gt;Grounded responses.&lt;/li&gt;
&lt;li&gt;Works with private data.&lt;/li&gt;
&lt;li&gt;Avoids retraining large models.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-semantic--vector-search&#34;&gt;üîç Semantic &amp;amp; Vector Search&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Semantic Search:&lt;/strong&gt; Understands meaning, not just keywords.
&lt;ul&gt;
&lt;li&gt;Example: ‚Äúbest city‚Äù ‚Üî ‚Äúbeautiful city.‚Äù&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Vector Search:&lt;/strong&gt; Text, images, and audio converted into embeddings (arrays of floats).
&lt;ul&gt;
&lt;li&gt;Enables image search, recommendation systems, music search (via humming).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Similarity algorithms:&lt;/strong&gt; cosine similarity, dot product, nearest neighbors.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-tools--demo&#34;&gt;üõ†Ô∏è Tools &amp;amp; Demo&lt;/h2&gt;
&lt;h3 id=&#34;elephant-library-php&#34;&gt;Elephant Library (PHP)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Open-source PHP library for GenAI apps.&lt;/li&gt;
&lt;li&gt;Supports:
&lt;ul&gt;
&lt;li&gt;LLMs: OpenAI, Mistral, Anthropic, LLaMA.&lt;/li&gt;
&lt;li&gt;Vector DBs: Elasticsearch, Pinecone, Chroma, etc.&lt;/li&gt;
&lt;li&gt;Features: document chunking, embedding generation, semantic retrieval, Q&amp;amp;A (RAG).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;demo-flow&#34;&gt;Demo Flow&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Ingestion&lt;/strong&gt;:&lt;/p&gt;</description>
      <content:encoded><![CDATA[<h1 id="-talk-summary-no-code-rag-chatbot-with-php-llms--elasticsearch">üé§ Talk Summary: No-Code RAG Chatbot with PHP, LLMs &amp; Elasticsearch</h1>
<p><strong>Speaker:</strong> Ashish Diwali (Senior Developer Advocate, Elastic)</p>
<hr>
<h2 id="-introduction">üîë Introduction</h2>
<ul>
<li>Topic: Integrating <strong>Generative AI (LLMs)</strong> with <strong>PHP</strong>.</li>
<li>Goal: Show how to build <strong>chat assistants, semantic search, and vector search</strong> without heavy ML expertise.</li>
<li>Demo focus: Using <strong>Elasticsearch + PHP + LLM (LLaMA 3.1)</strong>.</li>
</ul>
<hr>
<h2 id="-core-concepts">üß© Core Concepts</h2>
<h3 id="1-prompt-engineering">1. Prompt Engineering</h3>
<ul>
<li>LLMs generate responses based on prompts ‚Üí predicting next words.</li>
<li>Techniques:
<ul>
<li><strong>Zero-shot inference</strong> ‚Üí direct classification or tagging.</li>
<li><strong>One-shot inference</strong> ‚Üí provide one example in the prompt.</li>
<li><strong>Few-shot inference</strong> ‚Üí multiple examples ‚Üí useful for structured outputs (SQL, JSON, XML).</li>
</ul>
</li>
<li>Iteration + context = <strong>In-context learning (ICL)</strong>.</li>
</ul>
<h3 id="2-llm-limitations">2. LLM Limitations</h3>
<ul>
<li>‚ùå Hallucinations (wrong answers).</li>
<li>‚ùå Complex to build/train from scratch.</li>
<li>‚ùå No real-time / private data access.</li>
<li>‚ùå Privacy &amp; security concerns (especially in banking, public sector).</li>
</ul>
<h3 id="3-rag-retrieval-augmented-generation">3. RAG (Retrieval-Augmented Generation)</h3>
<ul>
<li>Solution to limitations.</li>
<li>Workflow:
<ol>
<li>User query ‚Üí hits database/vector DB (e.g., Elasticsearch).</li>
<li>Retrieve <strong>top 5‚Äì10 relevant docs</strong>.</li>
<li>Pass as context window ‚Üí LLM generates accurate answer.</li>
</ol>
</li>
<li>Benefits:
<ul>
<li>Grounded responses.</li>
<li>Works with private data.</li>
<li>Avoids retraining large models.</li>
</ul>
</li>
</ul>
<hr>
<h2 id="-semantic--vector-search">üîç Semantic &amp; Vector Search</h2>
<ul>
<li><strong>Semantic Search:</strong> Understands meaning, not just keywords.
<ul>
<li>Example: ‚Äúbest city‚Äù ‚Üî ‚Äúbeautiful city.‚Äù</li>
</ul>
</li>
<li><strong>Vector Search:</strong> Text, images, and audio converted into embeddings (arrays of floats).
<ul>
<li>Enables image search, recommendation systems, music search (via humming).</li>
</ul>
</li>
<li><strong>Similarity algorithms:</strong> cosine similarity, dot product, nearest neighbors.</li>
</ul>
<hr>
<h2 id="-tools--demo">üõ†Ô∏è Tools &amp; Demo</h2>
<h3 id="elephant-library-php">Elephant Library (PHP)</h3>
<ul>
<li>Open-source PHP library for GenAI apps.</li>
<li>Supports:
<ul>
<li>LLMs: OpenAI, Mistral, Anthropic, LLaMA.</li>
<li>Vector DBs: Elasticsearch, Pinecone, Chroma, etc.</li>
<li>Features: document chunking, embedding generation, semantic retrieval, Q&amp;A (RAG).</li>
</ul>
</li>
</ul>
<h3 id="demo-flow">Demo Flow</h3>
<ol>
<li>
<p><strong>Ingestion</strong>:</p>
<ul>
<li>Chunk PDF into smaller pieces (800 chars).</li>
<li>Generate embeddings with LLaMA.</li>
<li>Store text + vectors in Elasticsearch.</li>
</ul>
</li>
<li>
<p><strong>Querying</strong>:</p>
<ul>
<li>User question ‚Üí hits Elasticsearch.</li>
<li>Retrieve top 10 docs.</li>
<li>Send docs + query ‚Üí LLaMA ‚Üí response.</li>
</ul>
</li>
<li>
<p><strong>Examples</strong>:</p>
<ul>
<li><em>‚ÄúWho won the Nobel Prize in Physics 2024?‚Äù</em> ‚Üí Retrieved correct answer from PDF context.</li>
<li><em>‚ÄúHow do brain neural networks work?‚Äù</em> ‚Üí Summarized based on provided docs.</li>
<li><em>‚ÄúWho won ICC Championship 2025?‚Äù</em> ‚Üí No irrelevant hallucination (kept within context).</li>
</ul>
</li>
</ol>
<hr>
<h2 id="-key-takeaways">üéØ Key Takeaways</h2>
<ul>
<li><strong>Don‚Äôt train your own LLM</strong> ‚Üí use <strong>RAG + search</strong> to build assistants on private data.</li>
<li><strong>Elasticsearch</strong> is a powerful vector DB for semantic + hybrid search.</li>
<li><strong>PHP + Elephant</strong> makes building RAG chatbots accessible for web developers.</li>
<li>RAG powers most modern chat assistants today.</li>
</ul>
<hr>


    
    <div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
      <iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="allowfullscreen" loading="eager" referrerpolicy="strict-origin-when-cross-origin" src="https://www.youtube.com/embed/qDFct7oeRss?autoplay=0&controls=1&end=0&loop=0&mute=0&start=0" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="YouTube video"
      ></iframe>
    </div>

]]></content:encoded>
    </item>
    <item>
      <title>AWS Community Day Mumbai 2024</title>
      <link>https://ashish.one/talks/aws-communtiy-day-mumbai-2024/</link>
      <pubDate>Mon, 25 Aug 2025 10:18:25 +0530</pubDate>
      <guid>https://ashish.one/talks/aws-communtiy-day-mumbai-2024/</guid>
      <description>&lt;h1 id=&#34;-no-code-chatbot-with-elasticsearch--aws-bedrock-talk-summary&#34;&gt;üöÄ No-Code Chatbot with Elasticsearch + AWS Bedrock (Talk Summary)&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Speaker:&lt;/strong&gt; Ashish (Senior Developer Advocate, Elastic)&lt;br&gt;
&lt;strong&gt;Event:&lt;/strong&gt; AWS Community Day Mumbai 2024&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-why-search-still-matters-with-llms&#34;&gt;üîë Why Search Still Matters with LLMs&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;LLMs (like ChatGPT) are powerful but face:
&lt;ul&gt;
&lt;li&gt;‚ùå Hallucinations&lt;/li&gt;
&lt;li&gt;üí∞ High cost per query&lt;/li&gt;
&lt;li&gt;üîí No access to private / real-time data&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;‚úÖ Search grounds LLMs in &lt;strong&gt;reliable, domain-specific info&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-elasticsearch-capabilities&#34;&gt;‚ö° Elasticsearch Capabilities&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Traditional &lt;strong&gt;keyword search&lt;/strong&gt; + modern &lt;strong&gt;vector search&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Real-world use cases:
&lt;ul&gt;
&lt;li&gt;üìç Geospatial queries (ride-sharing, food delivery)&lt;/li&gt;
&lt;li&gt;‚ù§Ô∏è Matchmaking&lt;/li&gt;
&lt;li&gt;üìä Observability dashboards&lt;/li&gt;
&lt;li&gt;üìù Centralized logging (Elastic Stack: Elasticsearch, Kibana, Beats, Logstash)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-retrieval-augmented-generation-rag&#34;&gt;ü§ñ Retrieval-Augmented Generation (RAG)&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Workflow:&lt;/strong&gt;&lt;/p&gt;</description>
      <content:encoded><![CDATA[<h1 id="-no-code-chatbot-with-elasticsearch--aws-bedrock-talk-summary">üöÄ No-Code Chatbot with Elasticsearch + AWS Bedrock (Talk Summary)</h1>
<p><strong>Speaker:</strong> Ashish (Senior Developer Advocate, Elastic)<br>
<strong>Event:</strong> AWS Community Day Mumbai 2024</p>
<hr>
<h2 id="-why-search-still-matters-with-llms">üîë Why Search Still Matters with LLMs</h2>
<ul>
<li>LLMs (like ChatGPT) are powerful but face:
<ul>
<li>‚ùå Hallucinations</li>
<li>üí∞ High cost per query</li>
<li>üîí No access to private / real-time data</li>
</ul>
</li>
<li>‚úÖ Search grounds LLMs in <strong>reliable, domain-specific info</strong>.</li>
</ul>
<hr>
<h2 id="-elasticsearch-capabilities">‚ö° Elasticsearch Capabilities</h2>
<ul>
<li>Traditional <strong>keyword search</strong> + modern <strong>vector search</strong>.</li>
<li>Real-world use cases:
<ul>
<li>üìç Geospatial queries (ride-sharing, food delivery)</li>
<li>‚ù§Ô∏è Matchmaking</li>
<li>üìä Observability dashboards</li>
<li>üìù Centralized logging (Elastic Stack: Elasticsearch, Kibana, Beats, Logstash)</li>
</ul>
</li>
</ul>
<hr>
<h2 id="-retrieval-augmented-generation-rag">ü§ñ Retrieval-Augmented Generation (RAG)</h2>
<p><strong>Workflow:</strong></p>
<ol>
<li>User query ‚Üí</li>
<li>Elasticsearch retrieves relevant documents ‚Üí</li>
<li>Context passed to AWS Bedrock LLM ‚Üí</li>
<li>LLM generates grounded answers</li>
</ol>
<p>üëâ Reduces hallucination, enables <strong>chatbots on private data</strong>.</p>
<hr>
<h2 id="-demo-stack">üõ†Ô∏è Demo Stack</h2>
<ul>
<li><strong>Flowise AI</strong> ‚Üí No-code, drag &amp; drop RAG builder</li>
<li><strong>Steps:</strong>
<ol>
<li>Ingest data (JSON, PDFs, web crawl)</li>
<li>Chunk text</li>
<li>Generate embeddings (Amazon Titan)</li>
<li>Store embeddings + text in Elasticsearch</li>
<li>Connect Bedrock LLM for Q&amp;A</li>
</ol>
</li>
</ul>
<hr>
<h2 id="-extensions">üåê Extensions</h2>
<ul>
<li>Text search ‚úÖ</li>
<li>Image search üñºÔ∏è</li>
<li>Audio search (e.g., humming a tune) üéµ</li>
<li>Multimodal experiences ü§ù</li>
</ul>
<hr>
<h2 id="-key-takeaway">üéØ Key Takeaway</h2>
<p>With <strong>Elasticsearch + AWS Bedrock + Flowise AI</strong>, developers can build <strong>domain-specific, no-code RAG chatbots</strong> that combine:</p>
<ul>
<li>The <strong>precision of search</strong> üîç</li>
<li>The <strong>fluency of LLMs</strong> üí¨</li>
</ul>
<hr>
<h1 id="talk-video">Talk video</h1>


    
    <div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
      <iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="allowfullscreen" loading="eager" referrerpolicy="strict-origin-when-cross-origin" src="https://www.youtube.com/embed/ODy_xZIKU-s?autoplay=0&controls=1&end=0&loop=0&mute=0&start=0" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="YouTube video"
      ></iframe>
    </div>

]]></content:encoded>
    </item>
    <item>
      <title>GIDS 2024 - Smart Search with RAG: Elasticsearch Meets Language Models</title>
      <link>https://ashish.one/talks/gids-2024-rag/</link>
      <pubDate>Mon, 03 Jun 2024 12:12:06 +0530</pubDate>
      <guid>https://ashish.one/talks/gids-2024-rag/</guid>
      <description>&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;In today&amp;rsquo;s data-driven world, just having a search engine is not enough; the key is making it smart. Enter Elasticsearch Relevance Engine (ESRE) augmented with Retrieval Augmented Generation (RAG), a powerful solution
that marries Elasticsearch‚Äôs superior search capabilities with Large Language Models (LLMs) like ChatGPT for precise, contextual querying over proprietary datasets. This session is a hands-on guide that will show you how to amplify the power of Elasticsearch with advanced LLMs.&lt;/p&gt;</description>
      <content:encoded><![CDATA[<h1 id="introduction">Introduction</h1>
<p>In today&rsquo;s data-driven world, just having a search engine is not enough; the key is making it smart. Enter Elasticsearch Relevance Engine (ESRE) augmented with Retrieval Augmented Generation (RAG), a powerful solution
that marries Elasticsearch‚Äôs superior search capabilities with Large Language Models (LLMs) like ChatGPT for precise, contextual querying over proprietary datasets. This session is a hands-on guide that will show you how to amplify the power of Elasticsearch with advanced LLMs.</p>
<p>Key Takeaways:</p>
<ul>
<li>Learn how to supercharge Elasticsearch&rsquo;s BM25 algorithm with semantic search for results that are not just relevant but contextually accurate.</li>
<li>Discover how to plug in Large Language Models like OpenAI&rsquo;s ChatGPT to enable context-aware question-answering over your proprietary data.</li>
<li>Gain insights into the latest advancements in vector search within Lucene and Elasticsearch.</li>
<li>A quick live demo: Experience first-hand how ESRE, empowered by RAG, transforms a basic search query into a context-rich, highly relevant result..</li>
</ul>
<p>This talk is for you if you&rsquo;re grappling with search relevance issues and are looking for innovative ways to make your search smarter and more efficient. Whether you&rsquo;re a software developer, data engineer, or ML enthusiast, this session will equip you with the skills you need to build next-generation search capabilities.</p>
<h2 id="talk-video">Talk video</h2>


    
    <div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
      <iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="allowfullscreen" loading="eager" referrerpolicy="strict-origin-when-cross-origin" src="https://www.youtube.com/embed/r1jO4TglsEg?autoplay=0&controls=1&end=0&loop=0&mute=0&start=0" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="YouTube video"
      ></iframe>
    </div>

]]></content:encoded>
    </item>
    <item>
      <title>Elasticsearch: Vector and Hybrid Search</title>
      <link>https://ashish.one/talks/vector-hybrid-search/</link>
      <pubDate>Tue, 29 Aug 2023 21:41:03 +0530</pubDate>
      <guid>https://ashish.one/talks/vector-hybrid-search/</guid>
      <description>&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;Search is not just traditional TF/IDF any more but the current trend of machine learning and models has opened another dimension for search.&lt;/p&gt;
&lt;p&gt;This talk gives an overview of:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Classic&lt;/strong&gt; search and its limitations.&lt;/li&gt;
&lt;li&gt;What is a model and how can you use it.&lt;/li&gt;
&lt;li&gt;How to use vector search or hybrid search in Elasticsearch.&lt;/li&gt;
&lt;li&gt;Where OpenAI&amp;rsquo;s ChatGPT or similar LLMs come into play to with Elastic.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Check how to leverage &lt;a href=&#34;https://ashish.one/talks/chatgpt-elasticsearch/&#34;&gt;Leverage ChatGPT with Elasticsearch&lt;/a&gt;.&lt;/p&gt;</description>
      <content:encoded><![CDATA[<h1 id="introduction">Introduction</h1>
<p>Search is not just traditional TF/IDF any more but the current trend of machine learning and models has opened another dimension for search.</p>
<p>This talk gives an overview of:</p>
<ul>
<li><strong>Classic</strong> search and its limitations.</li>
<li>What is a model and how can you use it.</li>
<li>How to use vector search or hybrid search in Elasticsearch.</li>
<li>Where OpenAI&rsquo;s ChatGPT or similar LLMs come into play to with Elastic.</li>
</ul>
<p>Check how to leverage <a href="https://ashish.one/talks/chatgpt-elasticsearch/">Leverage ChatGPT with Elasticsearch</a>.</p>
<h2 id="talk-video">Talk Video</h2>


    
    <div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
      <iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="allowfullscreen" loading="eager" referrerpolicy="strict-origin-when-cross-origin" src="https://www.youtube.com/embed/AljarsLZRW0?autoplay=0&controls=1&end=0&loop=0&mute=0&start=0" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="YouTube video"
      ></iframe>
    </div>

]]></content:encoded>
    </item>
  </channel>
</rss>
