<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Docker on ashish.one</title>
    <link>https://ashish.one/tags/docker/</link>
    <description>Recent content in Docker on ashish.one</description>
    <image>
      <title>ashish.one</title>
      <url>https://ashish.one/img/speaker-pic/ashish.png</url>
      <link>https://ashish.one/img/speaker-pic/ashish.png</link>
    </image>
    <generator>Hugo -- 0.136.4</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 08 Jan 2026 21:21:21 +0530</lastBuildDate>
    <atom:link href="https://ashish.one/tags/docker/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Streamlining Vector Search with private model: Unifying Embedding Models with LiteLLM and Elasticsearch</title>
      <link>https://ashish.one/blogs/vector-search-with-litellm-elasticsearch/</link>
      <pubDate>Thu, 08 Jan 2026 21:21:21 +0530</pubDate>
      <guid>https://ashish.one/blogs/vector-search-with-litellm-elasticsearch/</guid>
      <description>Learn how to host an embedding model using LiteLLM proxy on Docker and consume it directly from Elasticsearch for seamless vector search.</description>
      <content:encoded><![CDATA[<p>In the rapidly evolving landscape of AI, managing the &ldquo;plumbing&rdquo; between your embedding models and your search engine is often a challenge. Developers frequently struggle with switching providers, managing API keys, and maintaining consistent API specifications.</p>
<p><strong>LiteLLM</strong> solves the model management problem by acting as a universal proxy, while <strong>Elasticsearch</strong> delivers high-performance Vector Search. By combining them, you can build a search architecture that is both flexible and powerful.</p>
<p>In this guide, we will walk through hosting an OpenAI-compatible embedding model using LiteLLM on Docker and consuming it directly from Elasticsearch to perform seamless vector search.</p>
<h3 id="prerequisites">Prerequisites</h3>
<ul>
<li><strong>OS:</strong> Ubuntu 24.04.3 LTS (or similar Linux environment)</li>
<li><strong>Docker:</strong> Ensure Docker is installed and running.</li>
<li><strong>Elasticsearch:</strong> An Elastic Cloud instance or a self-managed cluster.</li>
</ul>
<hr>
<h3 id="step-1-configure-and-run-litellm">Step 1: Configure and Run LiteLLM</h3>
<p>First, we need to configure LiteLLM. This tool will act as our gateway, proxying requests to various models (like GPT-4 or specific embedding models) while presenting a unified OpenAI-compatible API to the outside world.</p>
<p><strong>1. Create the <code>config.yaml</code> file:</strong></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#f92672">model_list</span>:
</span></span><span style="display:flex;"><span>  - <span style="color:#f92672">model_name</span>: <span style="color:#ae81ff">gpt-4o</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">litellm_params</span>:
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">model</span>: <span style="color:#ae81ff">openai/gpt-4o</span>
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">api_key</span>: <span style="color:#ae81ff">os.environ/OPENAI_API_KEY</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  - <span style="color:#f92672">model_name</span>: <span style="color:#ae81ff">gpt-3.5-turbo</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">litellm_params</span>:
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">model</span>: <span style="color:#ae81ff">openai/gpt-3.5-turbo</span>
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">api_key</span>: <span style="color:#ae81ff">os.environ/OPENAI_API_KEY</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  - <span style="color:#f92672">model_name</span>: <span style="color:#ae81ff">text-embedding-3-small</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">litellm_params</span>:
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">model</span>: <span style="color:#ae81ff">openai/text-embedding-3-small</span>
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">api_key</span>: <span style="color:#ae81ff">os.environ/OPENAI_API_KEY</span>
</span></span></code></pre></div><p><strong>2. Run LiteLLM with Docker:</strong></p>
<p>We will run the container with the config mounted. We also define a <code>LITELLM_MASTER_KEY</code> (<code>sk-my-admin-key-123</code>), which will serve as the static API key for our internal services to authenticate against this proxy.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>docker run -d <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  -p 4000:4000 <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  -v <span style="color:#66d9ef">$(</span>pwd<span style="color:#66d9ef">)</span>/config.yaml:/app/config.yaml <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  -e OPENAI_API_KEY<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;enter your openai api key&#34;</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  -e LITELLM_MASTER_KEY<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;sk-my-admin-key-123&#34;</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  docker.litellm.ai/berriai/litellm:main-latest <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --config /app/config.yaml --detailed_debug
</span></span></code></pre></div><hr>
<h3 id="step-2-verify-the-proxy">Step 2: Verify the Proxy</h3>
<p>Before we involve the search engine, let’s ensure our LiteLLM proxy is correctly handling requests.</p>
<p><strong>Test Chat Completion:</strong></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>curl <span style="color:#f92672">[</span>http://0.0.0.0:4000/v1/chat/completions<span style="color:#f92672">](</span>http://0.0.0.0:4000/v1/chat/completions<span style="color:#f92672">)</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  -H <span style="color:#e6db74">&#34;Content-Type: application/json&#34;</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  -H <span style="color:#e6db74">&#34;Authorization: Bearer sk-my-admin-key-123&#34;</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  -d <span style="color:#e6db74">&#39;{
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#34;model&#34;: &#34;gpt-4o&#34;,
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#34;messages&#34;: [
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">      { &#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;Hello from LiteLLM Docker!&#34; }
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    ]
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">  }&#39;</span>
</span></span></code></pre></div><p><strong>Test Embedding Generation:</strong></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>curl <span style="color:#f92672">[</span>http://0.0.0.0:4000/v1/embeddings<span style="color:#f92672">](</span>http://0.0.0.0:4000/v1/embeddings<span style="color:#f92672">)</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  -H <span style="color:#e6db74">&#34;Content-Type: application/json&#34;</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  -H <span style="color:#e6db74">&#34;Authorization: Bearer sk-my-admin-key-123&#34;</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  -d <span style="color:#e6db74">&#39;{
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#34;model&#34;: &#34;text-embedding-3-small&#34;,
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#34;input&#34;: &#34;LiteLLM makes proxying easy.&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">  }&#39;</span>
</span></span></code></pre></div><hr>
<h3 id="step-3-configure-elasticsearch-inference">Step 3: Configure Elasticsearch Inference</h3>
<p>Now we connect the two pieces. Since LiteLLM provides an OpenAI-compatible interface, we can use the standard OpenAI connector in Elasticsearch but point it to our custom LiteLLM URL.</p>
<p><em>For more details on configuring these connectors, refer to the <a href="https://www.elastic.co/docs/api/doc/elasticsearch/operation/operation-inference-put-openai">API doc</a></em></p>
<p><strong>Create the Inference Endpoint</strong></p>
<p>Run the following in your Kibana Dev Tools. Note that the <code>url</code> points to our container and the <code>api_key</code> matches our LiteLLM master key.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-json" data-lang="json"><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">PUT</span> <span style="color:#960050;background-color:#1e0010">_inference/text_embedding/litellm-embeddings</span>
</span></span><span style="display:flex;"><span>{
</span></span><span style="display:flex;"><span>   <span style="color:#f92672">&#34;service&#34;</span>: <span style="color:#e6db74">&#34;openai&#34;</span>,
</span></span><span style="display:flex;"><span>   <span style="color:#f92672">&#34;service_settings&#34;</span>: {
</span></span><span style="display:flex;"><span>       <span style="color:#f92672">&#34;api_key&#34;</span>: <span style="color:#e6db74">&#34;sk-my-admin-key-123&#34;</span>,
</span></span><span style="display:flex;"><span>       <span style="color:#f92672">&#34;model_id&#34;</span>: <span style="color:#e6db74">&#34;text-embedding-3-small&#34;</span>,
</span></span><span style="display:flex;"><span>       <span style="color:#f92672">&#34;url&#34;</span>: <span style="color:#e6db74">&#34;http://litellm_endpoint:4000/v1/embeddings&#34;</span>,
</span></span><span style="display:flex;"><span>       <span style="color:#f92672">&#34;dimensions&#34;</span>: <span style="color:#ae81ff">128</span>
</span></span><span style="display:flex;"><span>   }
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><p><strong>Test the Inference Endpoint</strong></p>
<p>Let&rsquo;s verify that Elasticsearch can generate embeddings via LiteLLM:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-json" data-lang="json"><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">POST</span> <span style="color:#960050;background-color:#1e0010">_inference/text_embedding/litellm-embeddings</span>
</span></span><span style="display:flex;"><span>{
</span></span><span style="display:flex;"><span> <span style="color:#f92672">&#34;input&#34;</span>: <span style="color:#e6db74">&#34;The sky above the port was the color of television tuned to a dead channel.&#34;</span>
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><p><strong>Output:</strong></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-json" data-lang="json"><span style="display:flex;"><span>{
</span></span><span style="display:flex;"><span> <span style="color:#f92672">&#34;text_embedding&#34;</span>: [
</span></span><span style="display:flex;"><span>   {
</span></span><span style="display:flex;"><span>     <span style="color:#f92672">&#34;embedding&#34;</span>: [
</span></span><span style="display:flex;"><span>       <span style="color:#ae81ff">-0.023704717</span>,
</span></span><span style="display:flex;"><span>       <span style="color:#ae81ff">-0.016358491</span>,
</span></span><span style="display:flex;"><span>       <span style="color:#960050;background-color:#1e0010">...</span>
</span></span><span style="display:flex;"><span>       <span style="color:#ae81ff">0.12764767</span>
</span></span><span style="display:flex;"><span>     ]
</span></span><span style="display:flex;"><span>   }
</span></span><span style="display:flex;"><span> ]
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><hr>
<h3 id="step-4-end-to-end-vector-search">Step 4: End-to-End Vector Search</h3>
<p>Now that the pipeline is ready, we can perform <strong>Semantic Search</strong>. We will use the <code>semantic_text</code> field type, which abstracts the embedding generation process—Elasticsearch calls LiteLLM automatically during data ingestion and query time.</p>
<p><strong>1. Create the Index</strong></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-json" data-lang="json"><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">PUT</span> <span style="color:#960050;background-color:#1e0010">/litellm-test-index</span>
</span></span><span style="display:flex;"><span>{
</span></span><span style="display:flex;"><span> <span style="color:#f92672">&#34;mappings&#34;</span>: {
</span></span><span style="display:flex;"><span>   <span style="color:#f92672">&#34;properties&#34;</span>: {
</span></span><span style="display:flex;"><span>     <span style="color:#f92672">&#34;content&#34;</span>: {
</span></span><span style="display:flex;"><span>       <span style="color:#f92672">&#34;type&#34;</span>: <span style="color:#e6db74">&#34;semantic_text&#34;</span>,
</span></span><span style="display:flex;"><span>       <span style="color:#f92672">&#34;inference_id&#34;</span>: <span style="color:#e6db74">&#34;litellm-embeddings&#34;</span>
</span></span><span style="display:flex;"><span>     },
</span></span><span style="display:flex;"><span>     <span style="color:#f92672">&#34;category&#34;</span>: {
</span></span><span style="display:flex;"><span>       <span style="color:#f92672">&#34;type&#34;</span>: <span style="color:#e6db74">&#34;keyword&#34;</span>
</span></span><span style="display:flex;"><span>     }
</span></span><span style="display:flex;"><span>   }
</span></span><span style="display:flex;"><span> }
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><p><strong>2. Ingest Data</strong></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-json" data-lang="json"><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">POST</span> <span style="color:#960050;background-color:#1e0010">/litellm-test-index/_bulk</span>
</span></span><span style="display:flex;"><span>{ <span style="color:#f92672">&#34;index&#34;</span>: {} }
</span></span><span style="display:flex;"><span>{ <span style="color:#f92672">&#34;content&#34;</span>: <span style="color:#e6db74">&#34;The quick brown fox jumps over the lazy dog.&#34;</span>, <span style="color:#f92672">&#34;category&#34;</span>: <span style="color:#e6db74">&#34;animals&#34;</span> }
</span></span><span style="display:flex;"><span>{ <span style="color:#f92672">&#34;index&#34;</span>: {} }
</span></span><span style="display:flex;"><span>{ <span style="color:#f92672">&#34;content&#34;</span>: <span style="color:#e6db74">&#34;Artificial intelligence is transforming software development.&#34;</span>, <span style="color:#f92672">&#34;category&#34;</span>: <span style="color:#e6db74">&#34;tech&#34;</span> }
</span></span><span style="display:flex;"><span>{ <span style="color:#f92672">&#34;index&#34;</span>: {} }
</span></span><span style="display:flex;"><span>{ <span style="color:#f92672">&#34;content&#34;</span>: <span style="color:#e6db74">&#34;Docker containers make deployment consistent and easy.&#34;</span>, <span style="color:#f92672">&#34;category&#34;</span>: <span style="color:#e6db74">&#34;tech&#34;</span> }
</span></span></code></pre></div><p><strong>3. Verify Data</strong></p>
<p>You can now search the index.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-json" data-lang="json"><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">GET</span> <span style="color:#960050;background-color:#1e0010">litellm-test-index/_search</span>
</span></span></code></pre></div><p><em>Note: You will not see the embedding vectors in the <code>_source</code> output. This is by design to save storage space. To learn more about this behavior, check out the <a href="https://www.elastic.co/search-labs/blog/elasticsearch-exclude-vectors-from-source">Elastic Search Labs blog on excluding vectors from source</a>.</em></p>
<p><strong>4. Perform Semantic Query</strong></p>
<p>Finally, let&rsquo;s run a natural language search. The query &ldquo;tools for deploying software&rdquo; does not exist in our text, but the vector search identifies the semantic relationship with &ldquo;Docker containers.&rdquo;</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-json" data-lang="json"><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">GET</span> <span style="color:#960050;background-color:#1e0010">/litellm-test-index/_search</span>
</span></span><span style="display:flex;"><span>{
</span></span><span style="display:flex;"><span> <span style="color:#f92672">&#34;query&#34;</span>: {
</span></span><span style="display:flex;"><span>   <span style="color:#f92672">&#34;semantic&#34;</span>: {
</span></span><span style="display:flex;"><span>     <span style="color:#f92672">&#34;field&#34;</span>: <span style="color:#e6db74">&#34;content&#34;</span>,
</span></span><span style="display:flex;"><span>     <span style="color:#f92672">&#34;query&#34;</span>: <span style="color:#e6db74">&#34;tools for deploying software&#34;</span>
</span></span><span style="display:flex;"><span>   }
</span></span><span style="display:flex;"><span> }
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><h3 id="conclusion">Conclusion</h3>
<p>This architecture demonstrates the power of decoupling your model provider from your search engine. By using <strong>LiteLLM</strong> as a standardized proxy, you gain the flexibility to swap underlying models without breaking your application. Simultaneously, <strong>Elasticsearch&rsquo;s Vector Search</strong> leverages these embeddings to provide deep semantic understanding.</p>
<p>Together, LiteLLM and Elasticsearch create a robust, future-proof foundation for building intelligent search applications that go far beyond simple keyword matching.</p>
<p>Happy searching!</p>
]]></content:encoded>
    </item>
    <item>
      <title>Start a single node elastic cluster with Docker Compose</title>
      <link>https://ashish.one/blogs/elastic-docker-compose/</link>
      <pubDate>Wed, 08 Jun 2022 13:25:09 +0530</pubDate>
      <guid>https://ashish.one/blogs/elastic-docker-compose/</guid>
      <description>&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;In this gist, we will quickly try to spin Elastic stacks with Docker containers. We are going to use &lt;a href=&#34;https://docs.docker.com/compose/&#34;&gt;docker-compose&lt;/a&gt;. You can learn more about &lt;a href=&#34;https://www.docker.com/&#34;&gt;Docker&lt;/a&gt; &amp;amp; &lt;a href=&#34;https://docs.docker.com/compose/&#34;&gt;Docker Compose&lt;/a&gt;, Which will help you to understand the flow.&lt;/p&gt;
&lt;h1 id=&#34;prerequisite&#34;&gt;Prerequisite&lt;/h1&gt;
&lt;p&gt;Tested on the below configuration.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;docker:&lt;code&gt;Docker version 20.10.16, build aa7e414&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;docker-compose:&lt;code&gt;Docker version 20.10.16, build aa7e414&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;cluster&#34;&gt;Cluster&lt;/h1&gt;
&lt;p&gt;This setup will include&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Elasticsearch&lt;/li&gt;
&lt;li&gt;Kibana&lt;/li&gt;
&lt;li&gt;Logstash&lt;/li&gt;
&lt;li&gt;APM&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;setup&#34;&gt;Setup&lt;/h1&gt;
&lt;p&gt;Clone repo:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;git clone https://github.com/ashishtiwari1993/elastic-docker.git
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;cd elastic-docker
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Make changes in &lt;code&gt;.env&lt;/code&gt; file.&lt;/p&gt;</description>
      <content:encoded><![CDATA[<h1 id="introduction">Introduction</h1>
<p>In this gist, we will quickly try to spin Elastic stacks with Docker containers. We are going to use <a href="https://docs.docker.com/compose/">docker-compose</a>. You can learn more about <a href="https://www.docker.com/">Docker</a> &amp; <a href="https://docs.docker.com/compose/">Docker Compose</a>, Which will help you to understand the flow.</p>
<h1 id="prerequisite">Prerequisite</h1>
<p>Tested on the below configuration.</p>
<ul>
<li>docker:<code>Docker version 20.10.16, build aa7e414</code></li>
<li>docker-compose:<code>Docker version 20.10.16, build aa7e414</code></li>
</ul>
<h1 id="cluster">Cluster</h1>
<p>This setup will include</p>
<ul>
<li>Elasticsearch</li>
<li>Kibana</li>
<li>Logstash</li>
<li>APM</li>
</ul>
<h1 id="setup">Setup</h1>
<p>Clone repo:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-sh" data-lang="sh"><span style="display:flex;"><span>git clone https://github.com/ashishtiwari1993/elastic-docker.git
</span></span><span style="display:flex;"><span>cd elastic-docker
</span></span></code></pre></div><p>Make changes in <code>.env</code> file.</p>
<h1 id="start-the-cluster">Start the cluster</h1>
<h2 id="start">Start</h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-sh" data-lang="sh"><span style="display:flex;"><span>docker-compose up -d
</span></span></code></pre></div><p>Just visit to <code>localhost:5601</code>. You should see a kibana login page.</p>
<h2 id="stop">Stop</h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-sh" data-lang="sh"><span style="display:flex;"><span>docker-compose down
</span></span></code></pre></div><h2 id="stop-with-deleting-network-containers-and-volumes">Stop with deleting network, containers and volumes</h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-sh" data-lang="sh"><span style="display:flex;"><span>docker-compose down -v
</span></span></code></pre></div><h1 id="access-stacks">Access stacks</h1>
<h2 id="elasticsearch">Elasticsearch</h2>
<h3 id="access-via-curl-from-host-machine">Access via <code>curl</code> from host machine</h3>
<h4 id="copy-cacrt-file">Copy <code>ca.crt</code> file</h4>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-sh" data-lang="sh"><span style="display:flex;"><span>docker cp elastic-docker_es01_1:/usr/share/elasticsearch/config/certs/ca/ca.crt /tmp/
</span></span></code></pre></div><h4 id="curl-command">Curl command</h4>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-sh" data-lang="sh"><span style="display:flex;"><span>curl --cacert /tmp/ca.crt -u elastic:pass@123 https://localhost:9200
</span></span></code></pre></div><h2 id="logstash">Logstash</h2>
<ol>
<li>You need to have pipeline configuration files on <code>LOGSTASH_PIPELINE_PATH</code> location. If there will be no file, Logstash will throw an error and get exit.</li>
</ol>
<h1 id="note">NOTE</h1>
<blockquote>
<p>You can simply comment other stacks which is not needed. For example if you want to just run Elasticsearch &amp; Kibana, Just comment the APM or other stack specification.</p>
</blockquote>
]]></content:encoded>
    </item>
    <item>
      <title>[Part 1] Setup LEMP environment with Docker - Setup Nginx and PHP</title>
      <link>https://ashish.one/blogs/part-1-setup-lemp-environment-with-docker-setup-nginx-and-php/</link>
      <pubDate>Sat, 16 May 2020 17:01:47 +0530</pubDate>
      <guid>https://ashish.one/blogs/part-1-setup-lemp-environment-with-docker-setup-nginx-and-php/</guid>
      <description>&lt;p&gt;Hi guys, In this series, we are going to setup LEMP Stack (Linux, Nginx, MySQL, PHP). Mainly it is used by web developers. I am assuming you have a basic idea about Docker &amp;amp; How it works.&lt;/p&gt;
&lt;p&gt;In this blog, We are going to setup PHP and Nginx.&lt;/p&gt;
&lt;h3 id=&#34;why-docker&#34;&gt;Why Docker?&lt;/h3&gt;
&lt;p&gt;I will not go too much deep, You can find more resources over the internet about the docker.&lt;/p&gt;
&lt;p&gt;Docker makes the installation process very smooth and it gives your isolated environment as the container.&lt;/p&gt;</description>
      <content:encoded><![CDATA[<p>Hi guys, In this series, we are going to setup LEMP Stack (Linux, Nginx, MySQL, PHP). Mainly it is used by web developers. I am assuming you have a basic idea about Docker &amp; How it works.</p>
<p>In this blog, We are going to setup PHP and Nginx.</p>
<h3 id="why-docker">Why Docker?</h3>
<p>I will not go too much deep, You can find more resources over the internet about the docker.</p>
<p>Docker makes the installation process very smooth and it gives your isolated environment as the container.</p>
<p>With the docker, There will be no more excuses like:</p>
<blockquote>
<p><strong>”It&rsquo;s working on my machine.”  :D</strong></p>
</blockquote>
<p>Because your docker environment remains the same across the platforms, So above excuses will going to turn with:</p>
<blockquote>
<p><strong>”It&rsquo;s working on every machine” ;)</strong></p>
</blockquote>
<p>In Short, Docker makes tech person&rsquo;s life easy like the developer, tester, etc.</p>
<h3 id="my-machine-configuration">My Machine configuration:</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-sh" data-lang="sh"><span style="display:flex;"><span>Docker Version: 18.09.7
</span></span><span style="display:flex;"><span>Docker Compose Version: 1.24.1
</span></span><span style="display:flex;"><span>RAM: 8GB
</span></span><span style="display:flex;"><span>OS: Ubuntu 18.04
</span></span></code></pre></div><p>Make sure docker and docker-compose is installed on your machine.</p>
<h3 id="step-1-create-folders">Step 1: Create folders:</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-sh" data-lang="sh"><span style="display:flex;"><span>mkdir LEMP
</span></span><span style="display:flex;"><span>cd LEMP
</span></span><span style="display:flex;"><span>mkdir <span style="color:#f92672">{</span>public_html,nginx_conf<span style="color:#f92672">}</span>
</span></span></code></pre></div><p><code>public_html</code>: It will contains your PHP code.<br>
<code>nginx_conf</code>: It will contains your Nginx configuration file.</p>
<h3 id="step-2-create-nginx-conf-file">Step 2: Create Nginx conf file</h3>
<p>Go to <code>nginx_conf</code>:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-sh" data-lang="sh"><span style="display:flex;"><span>cd nginx_conf
</span></span></code></pre></div><p>Create file <code>lemp-docker.conf</code> (You can give any name according to your requirement) &amp; Paste below configuration block.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-sh" data-lang="sh"><span style="display:flex;"><span>server <span style="color:#f92672">{</span>
</span></span><span style="display:flex;"><span>		listen 80;
</span></span><span style="display:flex;"><span>		server_name _;
</span></span><span style="display:flex;"><span>		root /public_html;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>		location / <span style="color:#f92672">{</span>
</span></span><span style="display:flex;"><span>				index index.php index.html;
</span></span><span style="display:flex;"><span>		<span style="color:#f92672">}</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>		location ~* <span style="color:#ae81ff">\.</span>php$ <span style="color:#f92672">{</span>
</span></span><span style="display:flex;"><span>				fastcgi_pass    php:9000;
</span></span><span style="display:flex;"><span>				fastcgi_index   index.php;
</span></span><span style="display:flex;"><span>				include         fastcgi_params;
</span></span><span style="display:flex;"><span>				fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name;
</span></span><span style="display:flex;"><span>				fastcgi_param PATH_INFO $fastcgi_path_info;
</span></span><span style="display:flex;"><span>		<span style="color:#f92672">}</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">}</span>
</span></span></code></pre></div><p>Save and Exit from folder.</p>
<h3 id="step-3-create-file-docker-composeyml">Step 3: Create file <code>docker-compose.yml</code></h3>
<p>Create file <code>docker-compose.yml</code> in <code>LEMP</code> folder &amp; paste the below lines:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-sh" data-lang="sh"><span style="display:flex;"><span>version: <span style="color:#e6db74">&#39;3&#39;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>services:
</span></span><span style="display:flex;"><span> web:
</span></span><span style="display:flex;"><span>  image: nginx
</span></span><span style="display:flex;"><span>  ports:
</span></span><span style="display:flex;"><span>   - <span style="color:#e6db74">&#34;80:80&#34;</span> 
</span></span><span style="display:flex;"><span>  volumes:
</span></span><span style="display:flex;"><span>   - ./public_html:/public_html
</span></span><span style="display:flex;"><span>   - ./nginx_conf:/etc/nginx/conf.d
</span></span><span style="display:flex;"><span>   - /tmp/nginx_logs:/var/log/nginx 
</span></span><span style="display:flex;"><span>  networks:
</span></span><span style="display:flex;"><span>   - nginx-php
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span> php:
</span></span><span style="display:flex;"><span>  image: php:7.2-fpm
</span></span><span style="display:flex;"><span>  volumes:
</span></span><span style="display:flex;"><span>  - ./public_html:/public_html
</span></span><span style="display:flex;"><span>  networks:
</span></span><span style="display:flex;"><span>  - nginx-php
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>networks:
</span></span><span style="display:flex;"><span> nginx-php:
</span></span></code></pre></div><p>Here we have created the network with the name <code>nginx-php</code>. You can check more about the networks <a href="https://docs.docker.com/compose/networking/#specify-custom-networks">here</a>.</p>
<p>You can set <code>volumes</code> path according to your requirement.</p>
<h3 id="step-4-lets-up-the-containers">Step 4: Let&rsquo;s Up the containers</h3>
<p>Create container:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-sh" data-lang="sh"><span style="display:flex;"><span>cd LEMP/
</span></span><span style="display:flex;"><span>docker-compose up
</span></span></code></pre></div><p><strong>Note:</strong> Make sure no other service is running on port 80.</p>
<p>Output:</p>
<p><img loading="lazy" src="/img/lemp-docker/docker-compose-up.png" alt="docker compose up"  />
</p>
<p>You can hit the <code>localhost</code> on your browser.</p>
<p>Once you hit the <code>localhost</code> check nginx logs:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-sh" data-lang="sh"><span style="display:flex;"><span>cat /tmp/nginx_logs/access.log 
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>172.25.0.1 - - <span style="color:#f92672">[</span>16/May/2020:12:51:04 +0000<span style="color:#f92672">]</span> <span style="color:#e6db74">&#34;GET / HTTP/1.1&#34;</span> <span style="color:#ae81ff">404</span> <span style="color:#ae81ff">556</span> <span style="color:#e6db74">&#34;-&#34;</span> <span style="color:#e6db74">&#34;Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.163 Safari/537.36&#34;</span> <span style="color:#e6db74">&#34;-&#34;</span>
</span></span></code></pre></div><p>Now stop the containers by simply hitting <code>Ctrl + c</code>. Now run docker-compose in detached mode. It will run in the background.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-sh" data-lang="sh"><span style="display:flex;"><span>docker-compose up -d
</span></span></code></pre></div><p>Lists containers</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-sh" data-lang="sh"><span style="display:flex;"><span>docker-compose ps
</span></span></code></pre></div><h3 id="step-5-lets-create-testphp">Step 5: Let&rsquo;s Create <code>test.php</code></h3>
<p>Go to <code>LEMP/public_html</code></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-sh" data-lang="sh"><span style="display:flex;"><span>cd LEMP/public_html
</span></span></code></pre></div><p>Create file <code>test.php</code> &amp; paste below code:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-php" data-lang="php"><span style="display:flex;"><span><span style="color:#f92672">&lt;?</span><span style="color:#a6e22e">php</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">echo</span> <span style="color:#a6e22e">phpinfo</span>();
</span></span></code></pre></div><h3 id="step-6-run-testphp">Step 6: Run <code>test.php</code></h3>
<p>Visit <a href="http://localhost/test.php">http://localhost/test.php</a>.</p>
<p>Output:</p>
<p><img loading="lazy" src="/img/lemp-docker/phpinfo.png" alt="phpinfo"  />
</p>
<h3 id="at-the-end">At the End</h3>
<p>We have successfully created Nginx and PHP Environment. In Part 2, We will check, How we can add MySQL to this environment.</p>
]]></content:encoded>
    </item>
  </channel>
</rss>
