[{"content":"In this blog, we will see how you can quickly setup ELK (Elasticsearch, Logstash, Kibana) stack to receive the HTTP webhook. Mostly ELK stack is known for logging purposes. But Elastic stacks are much more beyond the logging use case. Elastic provides Search, Observability \u0026amp; Security you can check more on this with official documentation.\nWhat is Webhook ? Webhook enables the two programs to communicate or transfer the data with the help of callback functions / hooks. Now in the modern tech world it is also known as Reverse API, Push API etc. Mostly it is used to send small amounts of data from source to destination. It is a one way data transfer procedure. It works over the HTTP protocol using REST API. It is simple like client and server communication. Most of the saas allow you to integrate their product with your system with the help of APIs and Webhook only. E.g. Slack and discord allows you to push messages with the help of webhooks. To accept the webhook event, You need to expose one HTTP endpoint lets say\nPOST /message In the above example, your system will accept messages from a third party system. Here you can setup authentication, Method, required parameters etc. and convey to users who are going to use this webhook.\nSo I have to use the POST method, and send data on the above endpoint. You can hit this Endpoint using curl, postman or any programming language.\nWhat is ELK ? Well from elastic.co -\nE - Elasticsearch - Elasticsearch is a distributed, free and open search and analytics engine for all types of data, including textual, numerical, geospatial, structured, and unstructured. Check more here.\nL - logstash - Logstash is a free and open server-side data processing pipeline that ingests data from a multitude of sources, transforms it, and then sends it to your favorite \u0026ldquo;stash.\u0026rdquo; check more here.\nK - Kibana - Kibana is an free and open frontend application that sits on top of the Elastic Stack, providing search and data visualization capabilities for data indexed in Elasticsearch. Check more here.\nThe Flow Events -\u0026gt; Logstash -\u0026gt; Elasticsearch \u0026lt;- Kibana\nLogstash works with three plugins - input, filter, output.\nYou can choose any source as a input. For example you can use jdbc input plugin to read data from mysql.\nTransform your data with the help of filter plugin.\nPush the data on destination which you have specified in output plugin.\nWe will use below plugins to process the webhooks requests.\nInput plugin -\u0026gt; HTTP Output plugin -\u0026gt; Elasticsearch Logstash will process the event and push to the Elasticsearch. Visualise data from kibana.\nImplementation Installation You can simply follow the official documentation for installation.\nElasticsearch - https://www.elastic.co/guide/en/elasticsearch/reference/current/install-elasticsearch.html Kibana - https://www.elastic.co/guide/en/logstash/current/installing-logstash.html Logstash - https://www.elastic.co/guide/en/logstash/current/installing-logstash.html I am spinning ELK instances using docker. You can choose any method mentioned on above links.\nVerify Lets verify everything is up and running properly.\nElasticsearch Hit below curl command:\ncurl --cacert path/to/ca.crt -u elastic:pass@123 https://localhost:9200 Response\n{ \u0026#34;name\u0026#34; : \u0026#34;es01\u0026#34;, \u0026#34;cluster_name\u0026#34; : \u0026#34;docker-cluster\u0026#34;, \u0026#34;cluster_uuid\u0026#34; : \u0026#34;_CDp3XgbQUKTuQxZWVLh6A\u0026#34;, \u0026#34;version\u0026#34; : { \u0026#34;number\u0026#34; : \u0026#34;8.6.0\u0026#34;, \u0026#34;build_flavor\u0026#34; : \u0026#34;default\u0026#34;, \u0026#34;build_type\u0026#34; : \u0026#34;docker\u0026#34;, \u0026#34;build_hash\u0026#34; : \u0026#34;f67ef2df40237445caa70e2fef79471cc608d70d\u0026#34;, \u0026#34;build_date\u0026#34; : \u0026#34;2023-01-04T09:35:21.782467981Z\u0026#34;, \u0026#34;build_snapshot\u0026#34; : false, \u0026#34;lucene_version\u0026#34; : \u0026#34;9.4.2\u0026#34;, \u0026#34;minimum_wire_compatibility_version\u0026#34; : \u0026#34;7.17.0\u0026#34;, \u0026#34;minimum_index_compatibility_version\u0026#34; : \u0026#34;7.0.0\u0026#34; }, \u0026#34;tagline\u0026#34; : \u0026#34;You Know, for Search\u0026#34; } Logstash Simply check the log file if there is any error.\ntail -f logs/logstash-plain.log Kibana Simply visit https://localhost:5601. Try to login in kibana with credentials.\nNow the ELK stack is up and running. Lets create the logstash configuration file to receive the webhook request and push to elasticsearch.\nLogstash pipeline Create file webhook.conf on the path which you have specified at path.config settings. You can set this setting at config/logstash.yml or config/pipeline.yml.\nwebhook.conf input { http { port =\u0026gt; 4000 } } filter { json { source =\u0026gt; \u0026#34;message\u0026#34; } } output { elasticsearch { hosts =\u0026gt; [\u0026#34;https://es01:9200\u0026#34;] cacert =\u0026gt; \u0026#39;/usr/share/logstash/pipeline/certs/ca.crt\u0026#39; user =\u0026gt; \u0026#39;elastic\u0026#39; password =\u0026gt; \u0026#39;pass@123\u0026#39; index =\u0026gt; \u0026#39;webhook\u0026#39; } } We are configuring HTTP endpoint on port 4000. So whenever anyone calls the webhook endpoint, they need to specify port like http://mydomain.com:4000\nHere i am only defining the index but you can configure data stream as well. Check here for more options.\nTest configuration Run pipeline with below command:\n./bin/logstash -f webhook-receiver.conf Check if any errors are there.\nIf everything seems fine, Let\u0026rsquo;s start the logstash service.\nTest Push sample data\ncurl -XPOST -H \u0026#39;Content-type:applicaton/json\u0026#39; http://localhost:4000 -d \u0026#39;{\u0026#34;test_key1\u0026#34;:\u0026#34;test_value1\u0026#34;,\u0026#34;info\u0026#34;:{\u0026#34;name\u0026#34;:\u0026#34;Ashish\u0026#34;,\u0026#34;last_name\u0026#34;:\u0026#34;Tiwari\u0026#34;},\u0026#34;my_list\u0026#34;:[\u0026#34;el1\u0026#34;,\u0026#34;el2\u0026#34;]}\u0026#39; Response\nok% Verify data in elasticsearch\nLogin to kibana by visiting localhost:5601.\nNavigate to Menu -\u0026gt; Management -\u0026gt; Dev Tools\nLets see if Index is created or not.\nGET _cat/indices?v Response\nhealth status index uuid pri rep docs.count docs.deleted store.size pri.store.size yellow open webhook LiXxWLy5QvKkpYpmLLbYsw 1 1 1 0 12.5kb 12.5kb I can see the index webhook has been created. Also the docs.count is 1 which means data has been inserted.\nLets see the data.\nGET webhook/_search Response\n{ \u0026#34;took\u0026#34;: 0, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 1, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 1, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;webhook\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;-ok63YUBrNdmvLyIE5Ue\u0026#34;, \u0026#34;_score\u0026#34;: 1, \u0026#34;_source\u0026#34;: { \u0026#34;host\u0026#34;: { \u0026#34;ip\u0026#34;: \u0026#34;192.168.192.1\u0026#34; }, \u0026#34;my_list\u0026#34;: [ \u0026#34;el1\u0026#34;, \u0026#34;el2\u0026#34; ], \u0026#34;message\u0026#34;: \u0026#34;\u0026#34;\u0026#34;{\u0026#34;test_key1\u0026#34;:\u0026#34;test_value1\u0026#34;,\u0026#34;info\u0026#34;:{\u0026#34;name\u0026#34;:\u0026#34;Ashish\u0026#34;,\u0026#34;last_name\u0026#34;:\u0026#34;Tiwari\u0026#34;},\u0026#34;my_list\u0026#34;:[\u0026#34;el1\u0026#34;,\u0026#34;el2\u0026#34;]}\u0026#34;\u0026#34;\u0026#34;, \u0026#34;event\u0026#34;: { \u0026#34;original\u0026#34;: \u0026#34;\u0026#34;\u0026#34;{\u0026#34;test_key1\u0026#34;:\u0026#34;test_value1\u0026#34;,\u0026#34;info\u0026#34;:{\u0026#34;name\u0026#34;:\u0026#34;Ashish\u0026#34;,\u0026#34;last_name\u0026#34;:\u0026#34;Tiwari\u0026#34;},\u0026#34;my_list\u0026#34;:[\u0026#34;el1\u0026#34;,\u0026#34;el2\u0026#34;]}\u0026#34;\u0026#34;\u0026#34; }, \u0026#34;@version\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;@timestamp\u0026#34;: \u0026#34;2023-01-23T06:04:08.664398594Z\u0026#34;, \u0026#34;http\u0026#34;: { \u0026#34;version\u0026#34;: \u0026#34;HTTP/1.1\u0026#34;, \u0026#34;method\u0026#34;: \u0026#34;POST\u0026#34;, \u0026#34;request\u0026#34;: { \u0026#34;mime_type\u0026#34;: \u0026#34;applicaton/json\u0026#34;, \u0026#34;body\u0026#34;: { \u0026#34;bytes\u0026#34;: \u0026#34;97\u0026#34; } } }, \u0026#34;info\u0026#34;: { \u0026#34;last_name\u0026#34;: \u0026#34;Tiwari\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Ashish\u0026#34; }, \u0026#34;user_agent\u0026#34;: { \u0026#34;original\u0026#34;: \u0026#34;curl/7.79.1\u0026#34; }, \u0026#34;url\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;/\u0026#34;, \u0026#34;domain\u0026#34;: \u0026#34;localhost\u0026#34;, \u0026#34;port\u0026#34;: 4000 }, \u0026#34;test_key1\u0026#34;: \u0026#34;test_value1\u0026#34; } } ] } } If you are able to see data like above, Your webhook receiver is all set to accept requests.\nData Visualisation Simply navigate to Menu -\u0026gt; Analytics -\u0026gt; Discover\nCreate a Data View on the index webhook.\nTo create the dashboard, go to Menu -\u0026gt; Analytics -\u0026gt; Dashboard. You can create a dashboard according to your requirement.\nConclusion We have successfully set up the webhook receiver with the help of ELK stack. Though ELK use cases are very vast. There are various input and output plugins available in logstash for data pipelines.\n","permalink":"https://ashish.one/blogs/elastic/receive-webhook-requests-using-elk/","summary":"In this blog, we will see how you can quickly setup ELK (Elasticsearch, Logstash, Kibana) stack to receive the HTTP webhook. Mostly ELK stack is known for logging purposes. But Elastic stacks are much more beyond the logging use case. Elastic provides Search, Observability \u0026amp; Security you can check more on this with official documentation.\nWhat is Webhook ? Webhook enables the two programs to communicate or transfer the data with the help of callback functions / hooks.","title":"Receive Webhook Requests Using ELK"},{"content":"1. unknown service runtime.v1alpha2.ImageService Error: pulling image: rpc error: code = Unimplemented desc = unknown service runtime.v1alpha2.ImageService\nSystem configuration centos 9 / 2GB RAM / 2CPU Master Node Same issue on master node.\nCommand [root@kube-master-1 ~]# kubeadm config images pull failed to pull image \u0026#34;registry.k8s.io/kube-apiserver:v1.26.0\u0026#34;: output: E0107 14:52:09.997544 4134 remote_image.go:222] \u0026#34;PullImage from image service failed\u0026#34; err=\u0026#34;rpc error: code = Unimplemented desc = unknown service runtime.v1alpha2.ImageService\u0026#34; image=\u0026#34;registry.k8s.io/kube-apiserver:v1.26.0\u0026#34; time=\u0026#34;2023-01-07T14:52:09Z\u0026#34; level=fatal msg=\u0026#34;pulling image: rpc error: code = Unimplemented desc = unknown service runtime.v1alpha2.ImageService\u0026#34; , error: exit status 1 To see the stack trace of this error execute with --v=5 or higher ‚úÖ Solved Remove below file:\nrm /etc/containerd/config.toml Try again.\nWorker Node Same issue on worker node while joining to master.\nCommand [root@kube-worker-2 ~]# kubeadm join x.x.x.x:6443 --token ga8bqg.01azxe9avjx2n6jr --discovery-token-ca-cert-hash sha256:d57699d74721094e5f921d48a0f9f895a0d7def7e1977e95ce0027a03e7f7d39 [preflight] Running pre-flight checks error execution phase preflight: [preflight] Some fatal errors occurred: [ERROR CRI]: container runtime is not running: output: E0107 17:46:12.269694 11160 remote_runtime.go:948] \u0026#34;Status from runtime service failed\u0026#34; err=\u0026#34;rpc error: code = Unimplemented desc = unknown service runtime.v1alpha2.RuntimeService\u0026#34; time=\u0026#34;2023-01-07T17:46:12Z\u0026#34; level=fatal msg=\u0026#34;getting status of runtime: rpc error: code = Unimplemented desc = unknown service runtime.v1alpha2.RuntimeService\u0026#34; , error: exit status 1 [preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...` To see the stack trace of this error execute with --v=5 or higher ‚úÖ Solved Same remove the config.toml file and restart the containerd service.\nrm /etc/containerd/config.toml systemctl restart containerd ","permalink":"https://ashish.one/blogs/k8s/k8s-generic-errors/","summary":"1. unknown service runtime.v1alpha2.ImageService Error: pulling image: rpc error: code = Unimplemented desc = unknown service runtime.v1alpha2.ImageService\nSystem configuration centos 9 / 2GB RAM / 2CPU Master Node Same issue on master node.\nCommand [root@kube-master-1 ~]# kubeadm config images pull failed to pull image \u0026#34;registry.k8s.io/kube-apiserver:v1.26.0\u0026#34;: output: E0107 14:52:09.997544 4134 remote_image.go:222] \u0026#34;PullImage from image service failed\u0026#34; err=\u0026#34;rpc error: code = Unimplemented desc = unknown service runtime.v1alpha2.ImageService\u0026#34; image=\u0026#34;registry.k8s.io/kube-apiserver:v1.26.0\u0026#34; time=\u0026#34;2023-01-07T14:52:09Z\u0026#34; level=fatal msg=\u0026#34;pulling image: rpc error: code = Unimplemented desc = unknown service runtime.","title":"Kubernetes generic errors"},{"content":"Sample Queries for Elasticsearch Workshop CRUD # Insert POST meetup/_doc/ { \u0026#34;name\u0026#34;:\u0026#34;Ashish Tiwari\u0026#34; } # Insert with id POST meetup/_doc/1 { \u0026#34;name\u0026#34;:\u0026#34;Ashish Tiwari\u0026#34; } # Search GET meetup/_search # Update POST meetup/_doc/1 { \u0026#34;name\u0026#34;:\u0026#34;Ashish\u0026#34;, \u0026#34;company\u0026#34;:\u0026#34;elastic\u0026#34;, \u0026#34;address\u0026#34;:\u0026#34;Navi Mumbai kharghar\u0026#34;, \u0026#34;skills\u0026#34;:{ \u0026#34;language\u0026#34;:[\u0026#34;php\u0026#34;,\u0026#34;java\u0026#34;,\u0026#34;node\u0026#34;], \u0026#34;database\u0026#34;:[\u0026#34;mysql\u0026#34;,\u0026#34;mongodb\u0026#34;], \u0026#34;search\u0026#34;:\u0026#34;elasticsearch\u0026#34; } } # search with query GET meetup/_search { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;address\u0026#34;: \u0026#34;navi\u0026#34; } } } # delete DELETE meetup BULK POST _bulk {\u0026#34;index\u0026#34;:{\u0026#34;_index\u0026#34;:\u0026#34;meetup\u0026#34;}} {\u0026#34;user_id\u0026#34;:1,\u0026#34;first_name\u0026#34;:\u0026#34;Yvonne\u0026#34;,\u0026#34;last_name\u0026#34;:\u0026#34;Willmott\u0026#34;,\u0026#34;email\u0026#34;:\u0026#34;ywillmott0@live.com\u0026#34;,\u0026#34;gender\u0026#34;:\u0026#34;Female\u0026#34;,\u0026#34;street_address\u0026#34;:\u0026#34;38 Helena Avenue\u0026#34;,\u0026#34;ip_address\u0026#34;:\u0026#34;104.221.25.110\u0026#34;,\u0026#34;company\u0026#34;:\u0026#34;Flashset\u0026#34;} {\u0026#34;index\u0026#34;:{\u0026#34;_index\u0026#34;:\u0026#34;meetup\u0026#34;}} {\u0026#34;user_id\u0026#34;:2,\u0026#34;first_name\u0026#34;:\u0026#34;Immanuel\u0026#34;,\u0026#34;last_name\u0026#34;:\u0026#34;Philbrick\u0026#34;,\u0026#34;email\u0026#34;:\u0026#34;iphilbrick1@wunderground.com\u0026#34;,\u0026#34;gender\u0026#34;:\u0026#34;Male\u0026#34;,\u0026#34;street_address\u0026#34;:\u0026#34;01 Bunting Pass\u0026#34;,\u0026#34;ip_address\u0026#34;:\u0026#34;9.20.164.27\u0026#34;,\u0026#34;company\u0026#34;:\u0026#34;Babblestorm\u0026#34;} {\u0026#34;index\u0026#34;:{\u0026#34;_index\u0026#34;:\u0026#34;meetup\u0026#34;}} {\u0026#34;user_id\u0026#34;:3,\u0026#34;first_name\u0026#34;:\u0026#34;Clotilda\u0026#34;,\u0026#34;last_name\u0026#34;:\u0026#34;Danelut\u0026#34;,\u0026#34;email\u0026#34;:\u0026#34;cdanelut2@deliciousdays.com\u0026#34;,\u0026#34;gender\u0026#34;:\u0026#34;Agender\u0026#34;,\u0026#34;street_address\u0026#34;:\u0026#34;0 Crowley Trail\u0026#34;,\u0026#34;ip_address\u0026#34;:\u0026#34;158.94.144.140\u0026#34;,\u0026#34;company\u0026#34;:\u0026#34;Riffpedia\u0026#34;} {\u0026#34;index\u0026#34;:{\u0026#34;_index\u0026#34;:\u0026#34;meetup\u0026#34;}} {\u0026#34;user_id\u0026#34;:4,\u0026#34;first_name\u0026#34;:\u0026#34;Nahum\u0026#34;,\u0026#34;last_name\u0026#34;:\u0026#34;Attfield\u0026#34;,\u0026#34;email\u0026#34;:\u0026#34;nattfield3@blog.com\u0026#34;,\u0026#34;gender\u0026#34;:\u0026#34;Male\u0026#34;,\u0026#34;street_address\u0026#34;:\u0026#34;7 Garrison Court\u0026#34;,\u0026#34;ip_address\u0026#34;:\u0026#34;225.144.148.44\u0026#34;,\u0026#34;company\u0026#34;:\u0026#34;Chatterpoint\u0026#34;} {\u0026#34;index\u0026#34;:{\u0026#34;_index\u0026#34;:\u0026#34;meetup\u0026#34;}} {\u0026#34;user_id\u0026#34;:5,\u0026#34;first_name\u0026#34;:\u0026#34;Vaughan\u0026#34;,\u0026#34;last_name\u0026#34;:\u0026#34;Middis\u0026#34;,\u0026#34;email\u0026#34;:\u0026#34;vmiddis4@ted.com\u0026#34;,\u0026#34;gender\u0026#34;:\u0026#34;Male\u0026#34;,\u0026#34;street_address\u0026#34;:\u0026#34;7 Cody Way\u0026#34;,\u0026#34;ip_address\u0026#34;:\u0026#34;66.198.31.108\u0026#34;,\u0026#34;company\u0026#34;:\u0026#34;Mynte\u0026#34;} {\u0026#34;index\u0026#34;:{\u0026#34;_index\u0026#34;:\u0026#34;meetup\u0026#34;}} {\u0026#34;user_id\u0026#34;:6,\u0026#34;first_name\u0026#34;:\u0026#34;Nolie\u0026#34;,\u0026#34;last_name\u0026#34;:\u0026#34;Alessandrucci\u0026#34;,\u0026#34;email\u0026#34;:\u0026#34;nalessandrucci5@networksolutions.com\u0026#34;,\u0026#34;gender\u0026#34;:\u0026#34;Male\u0026#34;,\u0026#34;street_address\u0026#34;:\u0026#34;826 Brown Hill\u0026#34;,\u0026#34;ip_address\u0026#34;:\u0026#34;96.77.221.95\u0026#34;,\u0026#34;company\u0026#34;:\u0026#34;Feedfish\u0026#34;} {\u0026#34;index\u0026#34;:{\u0026#34;_index\u0026#34;:\u0026#34;meetup\u0026#34;}} {\u0026#34;user_id\u0026#34;:7,\u0026#34;first_name\u0026#34;:\u0026#34;Beverlie\u0026#34;,\u0026#34;last_name\u0026#34;:\u0026#34;Ovitts\u0026#34;,\u0026#34;email\u0026#34;:\u0026#34;bovitts6@tripod.com\u0026#34;,\u0026#34;gender\u0026#34;:\u0026#34;Male\u0026#34;,\u0026#34;street_address\u0026#34;:\u0026#34;6 Sycamore Pass\u0026#34;,\u0026#34;ip_address\u0026#34;:\u0026#34;102.24.117.107\u0026#34;,\u0026#34;company\u0026#34;:\u0026#34;Zazio\u0026#34;} {\u0026#34;index\u0026#34;:{\u0026#34;_index\u0026#34;:\u0026#34;meetup\u0026#34;}} {\u0026#34;user_id\u0026#34;:8,\u0026#34;first_name\u0026#34;:\u0026#34;Graeme\u0026#34;,\u0026#34;last_name\u0026#34;:\u0026#34;Dopson\u0026#34;,\u0026#34;email\u0026#34;:\u0026#34;gdopson7@free.fr\u0026#34;,\u0026#34;gender\u0026#34;:\u0026#34;Female\u0026#34;,\u0026#34;street_address\u0026#34;:\u0026#34;26 Dunning Avenue\u0026#34;,\u0026#34;ip_address\u0026#34;:\u0026#34;198.33.215.93\u0026#34;,\u0026#34;company\u0026#34;:\u0026#34;Flashset\u0026#34;} {\u0026#34;index\u0026#34;:{\u0026#34;_index\u0026#34;:\u0026#34;meetup\u0026#34;}} {\u0026#34;user_id\u0026#34;:9,\u0026#34;first_name\u0026#34;:\u0026#34;Mellisa\u0026#34;,\u0026#34;last_name\u0026#34;:\u0026#34;Hurich\u0026#34;,\u0026#34;email\u0026#34;:\u0026#34;mhurich8@nbcnews.com\u0026#34;,\u0026#34;gender\u0026#34;:\u0026#34;Male\u0026#34;,\u0026#34;street_address\u0026#34;:\u0026#34;6371 Browning Way\u0026#34;,\u0026#34;ip_address\u0026#34;:\u0026#34;66.0.3.199\u0026#34;,\u0026#34;company\u0026#34;:\u0026#34;Divanoodle\u0026#34;} {\u0026#34;index\u0026#34;:{\u0026#34;_index\u0026#34;:\u0026#34;meetup\u0026#34;}} {\u0026#34;user_id\u0026#34;:10,\u0026#34;first_name\u0026#34;:\u0026#34;Dyan\u0026#34;,\u0026#34;last_name\u0026#34;:\u0026#34;Loude\u0026#34;,\u0026#34;email\u0026#34;:\u0026#34;dloude9@berkeley.edu\u0026#34;,\u0026#34;gender\u0026#34;:\u0026#34;Female\u0026#34;,\u0026#34;street_address\u0026#34;:\u0026#34;9818 Reindahl Road\u0026#34;,\u0026#34;ip_address\u0026#34;:\u0026#34;16.56.137.54\u0026#34;,\u0026#34;company\u0026#34;:\u0026#34;Agivu\u0026#34;} {\u0026#34;index\u0026#34;:{\u0026#34;_index\u0026#34;:\u0026#34;meetup\u0026#34;}} {\u0026#34;user_id\u0026#34;:11,\u0026#34;first_name\u0026#34;:\u0026#34;Becky\u0026#34;,\u0026#34;last_name\u0026#34;:\u0026#34;Shank\u0026#34;,\u0026#34;email\u0026#34;:\u0026#34;bshanka@tinypic.com\u0026#34;,\u0026#34;gender\u0026#34;:\u0026#34;Male\u0026#34;,\u0026#34;street_address\u0026#34;:\u0026#34;1206 Warrior Terrace\u0026#34;,\u0026#34;ip_address\u0026#34;:\u0026#34;90.63.35.111\u0026#34;,\u0026#34;company\u0026#34;:\u0026#34;Izio\u0026#34;} {\u0026#34;index\u0026#34;:{\u0026#34;_index\u0026#34;:\u0026#34;meetup\u0026#34;}} {\u0026#34;user_id\u0026#34;:12,\u0026#34;first_name\u0026#34;:\u0026#34;Bar\u0026#34;,\u0026#34;last_name\u0026#34;:\u0026#34;Bedburrow\u0026#34;,\u0026#34;email\u0026#34;:\u0026#34;bbedburrowb@vistaprint.com\u0026#34;,\u0026#34;gender\u0026#34;:\u0026#34;Male\u0026#34;,\u0026#34;street_address\u0026#34;:\u0026#34;75 Onsgard Crossing\u0026#34;,\u0026#34;ip_address\u0026#34;:\u0026#34;85.122.33.250\u0026#34;,\u0026#34;company\u0026#34;:\u0026#34;Zoombox\u0026#34;} {\u0026#34;index\u0026#34;:{\u0026#34;_index\u0026#34;:\u0026#34;meetup\u0026#34;}} {\u0026#34;user_id\u0026#34;:13,\u0026#34;first_name\u0026#34;:\u0026#34;Dorey\u0026#34;,\u0026#34;last_name\u0026#34;:\u0026#34;Isenor\u0026#34;,\u0026#34;email\u0026#34;:\u0026#34;disenorc@privacy.gov.au\u0026#34;,\u0026#34;gender\u0026#34;:\u0026#34;Female\u0026#34;,\u0026#34;street_address\u0026#34;:\u0026#34;53682 Parkside Crossing\u0026#34;,\u0026#34;ip_address\u0026#34;:\u0026#34;150.158.150.213\u0026#34;,\u0026#34;company\u0026#34;:\u0026#34;Rhyzio\u0026#34;} {\u0026#34;index\u0026#34;:{\u0026#34;_index\u0026#34;:\u0026#34;meetup\u0026#34;}} {\u0026#34;user_id\u0026#34;:14,\u0026#34;first_name\u0026#34;:\u0026#34;Torrin\u0026#34;,\u0026#34;last_name\u0026#34;:\u0026#34;Rangall\u0026#34;,\u0026#34;email\u0026#34;:\u0026#34;trangalld@buzzfeed.com\u0026#34;,\u0026#34;gender\u0026#34;:\u0026#34;Male\u0026#34;,\u0026#34;street_address\u0026#34;:\u0026#34;24247 Old Shore Plaza\u0026#34;,\u0026#34;ip_address\u0026#34;:\u0026#34;40.151.17.2\u0026#34;,\u0026#34;company\u0026#34;:\u0026#34;Devpoint\u0026#34;} {\u0026#34;index\u0026#34;:{\u0026#34;_index\u0026#34;:\u0026#34;meetup\u0026#34;}} {\u0026#34;user_id\u0026#34;:15,\u0026#34;first_name\u0026#34;:\u0026#34;Genvieve\u0026#34;,\u0026#34;last_name\u0026#34;:\u0026#34;Beslier\u0026#34;,\u0026#34;email\u0026#34;:\u0026#34;gbesliere@yolasite.com\u0026#34;,\u0026#34;gender\u0026#34;:\u0026#34;Male\u0026#34;,\u0026#34;street_address\u0026#34;:\u0026#34;96214 Miller Trail\u0026#34;,\u0026#34;ip_address\u0026#34;:\u0026#34;115.143.68.208\u0026#34;,\u0026#34;company\u0026#34;:\u0026#34;Oyonder\u0026#34;} {\u0026#34;index\u0026#34;:{\u0026#34;_index\u0026#34;:\u0026#34;meetup\u0026#34;}} {\u0026#34;user_id\u0026#34;:16,\u0026#34;first_name\u0026#34;:\u0026#34;Arden\u0026#34;,\u0026#34;last_name\u0026#34;:\u0026#34;Ramas\u0026#34;,\u0026#34;email\u0026#34;:\u0026#34;aramasf@whitehouse.gov\u0026#34;,\u0026#34;gender\u0026#34;:\u0026#34;Polygender\u0026#34;,\u0026#34;street_address\u0026#34;:\u0026#34;390 Gulseth Alley\u0026#34;,\u0026#34;ip_address\u0026#34;:\u0026#34;36.83.126.154\u0026#34;,\u0026#34;company\u0026#34;:\u0026#34;Youbridge\u0026#34;} {\u0026#34;index\u0026#34;:{\u0026#34;_index\u0026#34;:\u0026#34;meetup\u0026#34;}} {\u0026#34;user_id\u0026#34;:17,\u0026#34;first_name\u0026#34;:\u0026#34;Alyosha\u0026#34;,\u0026#34;last_name\u0026#34;:\u0026#34;Domm\u0026#34;,\u0026#34;email\u0026#34;:\u0026#34;adommg@washingtonpost.com\u0026#34;,\u0026#34;gender\u0026#34;:\u0026#34;Female\u0026#34;,\u0026#34;street_address\u0026#34;:\u0026#34;32 Oxford Way\u0026#34;,\u0026#34;ip_address\u0026#34;:\u0026#34;174.71.176.45\u0026#34;,\u0026#34;company\u0026#34;:\u0026#34;Wikizz\u0026#34;} Upload sample json data from kibana Download movies.json and insert into elasticsearch by using below command:\nOpen Kibana -\u0026gt; Menu -\u0026gt; Home -\u0026gt; Upload a file\nCreate Data view in Kibana Open Kibana -\u0026gt; Menu -\u0026gt; Stack Management -\u0026gt; Data Views (Kibana)\nCreate Dashboard Open Kibana -\u0026gt; Menu -\u0026gt; Analytics -\u0026gt; Dashboard\nCreate mapping PUT /devfest-raipur { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;age\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;integer\u0026#34; }, \u0026#34;email\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34; }, \u0026#34;name\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34; } } } } Analyze # analyze GET /_analyze?pretty { \u0026#34;text\u0026#34; : \u0026#34;Quick Brown Foxes!\u0026#34; } # Whitespace GET _analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;whitespace\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;The 2 QUICK Brown-Foxes jumped over the lazy dog\\u0027s bone.\u0026#34; } What is an analyzer? An analyzer is made of character filters, tokenizer and token filters.\nLet\u0026rsquo;s build one\nPOST _analyze { \u0026#34;char_filter\u0026#34;: [], \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [], \u0026#34;text\u0026#34;: [ \u0026#34;I like when the \u0026lt;strong\u0026gt;quick\u0026lt;/strong\u0026gt; foxes jumps over lazy DOGS!\u0026#34;, \u0026#34;and \u0026lt;strong\u0026gt;fast\u0026lt;/strong\u0026gt;\u0026#34; ] } Let\u0026rsquo;s remove the html code.\nPOST _analyze { \u0026#34;char_filter\u0026#34;: [\u0026#34;html_strip\u0026#34;], \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [], \u0026#34;text\u0026#34;: [ \u0026#34;I like when the \u0026lt;strong\u0026gt;quick\u0026lt;/strong\u0026gt; foxes jumps over lazy DOGS!\u0026#34;, \u0026#34;and \u0026lt;strong\u0026gt;fast\u0026lt;/strong\u0026gt;\u0026#34; ] } Some words don\u0026rsquo;t bring us any value. Let\u0026rsquo;s skip them.\nPOST _analyze { \u0026#34;char_filter\u0026#34;: [\u0026#34;html_strip\u0026#34;], \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;stop\u0026#34;, \u0026#34;stopwords\u0026#34;: [ \u0026#34;_english_\u0026#34;] } ], \u0026#34;text\u0026#34;: [ \u0026#34;I like when the \u0026lt;strong\u0026gt;quick\u0026lt;/strong\u0026gt; foxes jumps over lazy DOGS!\u0026#34;, \u0026#34;and \u0026lt;strong\u0026gt;fast\u0026lt;/strong\u0026gt;\u0026#34; ] } We can also remove \u0026ldquo;I\u0026rdquo;, \u0026ldquo;when\u0026rdquo; and \u0026ldquo;over\u0026rdquo;.\nPOST _analyze { \u0026#34;char_filter\u0026#34;: [\u0026#34;html_strip\u0026#34;], \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;stop\u0026#34;, \u0026#34;ignore_case\u0026#34;:true, \u0026#34;stopwords\u0026#34;: [ \u0026#34;_english_\u0026#34;, \u0026#34;I\u0026#34;, \u0026#34;when\u0026#34;, \u0026#34;over\u0026#34;] } ], \u0026#34;text\u0026#34;: [ \u0026#34;I like when the \u0026lt;strong\u0026gt;quick\u0026lt;/strong\u0026gt; foxes jumps over lazy DOGS!\u0026#34;, \u0026#34;and \u0026lt;strong\u0026gt;fast\u0026lt;/strong\u0026gt;\u0026#34; ] } DOGS and dogs should match.\nPOST _analyze { \u0026#34;char_filter\u0026#34;: [\u0026#34;html_strip\u0026#34;], \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;stop\u0026#34;, \u0026#34;ignore_case\u0026#34;:true, \u0026#34;stopwords\u0026#34;: [ \u0026#34;_english_\u0026#34;, \u0026#34;I\u0026#34;, \u0026#34;when\u0026#34;, \u0026#34;over\u0026#34;] }, \u0026#34;lowercase\u0026#34; ], \u0026#34;text\u0026#34;: [ \u0026#34;I like when the \u0026lt;strong\u0026gt;quick\u0026lt;/strong\u0026gt; foxes jumps over lazy DOGS!\u0026#34;, \u0026#34;and \u0026lt;strong\u0026gt;fast\u0026lt;/strong\u0026gt;\u0026#34; ] } dog, dogs and fox, foxes and jump, jumps, jumping, jumped should match. Let\u0026rsquo;s use a stemmer.\nPOST _analyze { \u0026#34;char_filter\u0026#34;: [\u0026#34;html_strip\u0026#34;], \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;stop\u0026#34;, \u0026#34;ignore_case\u0026#34;:true, \u0026#34;stopwords\u0026#34;: [ \u0026#34;_english_\u0026#34;, \u0026#34;I\u0026#34;, \u0026#34;when\u0026#34;, \u0026#34;over\u0026#34;] }, \u0026#34;lowercase\u0026#34;, { \u0026#34;type\u0026#34;: \u0026#34;stemmer\u0026#34;, \u0026#34;language\u0026#34;: \u0026#34;english\u0026#34; } ], \u0026#34;text\u0026#34;: [ \u0026#34;jumping jumps jump jumped\u0026#34;, \u0026#34;I like when the \u0026lt;strong\u0026gt;quick\u0026lt;/strong\u0026gt; foxes jumps over lazy DOGS!\u0026#34;, \u0026#34;and \u0026lt;strong\u0026gt;fast\u0026lt;/strong\u0026gt;\u0026#34; ] } Language analyzer GET /_analyze?pretty { \u0026#34;analyzer\u0026#34;: \u0026#34;hindi\u0026#34;, \u0026#34;text\u0026#34; : \u0026#34;‡§ö‡§æ‡§£‡§ï‡•ç‡§Ø ‡§®‡•á ‡§ö‡§Ç‡§¶‡•ç‡§∞‡§ó‡•Å‡§™‡•ç‡§§ ‡§î‡§∞ ‡§¨‡§ø‡§Ç‡§¶‡•Ç‡§∏‡§æ‡§∞ ‡§¶‡•ã‡§®‡•ã‡§Ç ‡§ï‡•á ‡§≤‡§ø‡§è ‡§™‡•ç‡§∞‡§ß‡§æ‡§®‡§Æ‡§Ç‡§§‡•ç‡§∞‡•Ä ‡§î‡§∞ ‡§∞‡§æ‡§ú‡§®‡§Ø‡§ø‡§ï ‡§∏‡§≤‡§æ‡§π‡§ï‡§æ‡§∞ ‡§ï‡•á ‡§∞‡•Ç‡§™ ‡§Æ‡•á‡§Ç ‡§ï‡§æ‡§Æ ‡§ï‡§ø‡§Ø‡§æ‡•§\u0026#34; } Let\u0026rsquo;s create Hindi search engine (‡§ó‡•Ä‡§§‡§Ø‡§Ç‡§§‡•ç‡§∞) Create mapping PUT geetyantra { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;analyzer\u0026#34;: { \u0026#34;geetyantra-analyzer\u0026#34;:{ \u0026#34;type\u0026#34;:\u0026#34;hindi\u0026#34; } } } }, \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;adhyay\u0026#34;: {\u0026#34;type\u0026#34;: \u0026#34;integer\u0026#34;}, \u0026#34;updesh\u0026#34;: {\u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;,\u0026#34;analyzer\u0026#34;: \u0026#34;geetyantra-analyzer\u0026#34;} } } } Insert data Taking data from üôè‡§µ‡§ø‡§ï‡§ø‡§™‡•Ä‡§°‡§ø‡§Ø‡§æ-‡§∂‡•ç‡§∞‡•Ä‡§Æ‡§¶‡•ç‡§≠‡§ó‡§µ‡§¶‡•ç‡§ó‡•Ä‡§§‡§æ\nLets insert some data like below:\nPOST geetyantra/_doc { \u0026#34;adhyay\u0026#34;:1, \u0026#34;updesh\u0026#34;:\u0026#34;‡§ï‡•É‡§∑‡•ç‡§£ ‡§®‡•á ‡§Ö‡§∞‡•ç‡§ú‡•Å‡§® ‡§ï‡•Ä ‡§µ‡§π ‡§∏‡•ç‡§•‡§ø‡§§‡§ø ‡§¶‡•á‡§ñ‡§ï‡§∞ ‡§ú‡§æ‡§® ‡§≤‡§ø‡§Ø‡§æ ‡§ï‡§ø ‡§Ö‡§∞‡•ç‡§ú‡•Å‡§® ‡§ï‡§æ ‡§∂‡§∞‡•Ä‡§∞ ‡§†‡•Ä‡§ï ‡§π‡•à ‡§ï‡§ø‡§Ç‡§§‡•Å ‡§Ø‡•Å‡§¶‡•ç‡§ß ‡§Ü‡§∞‡§Ç‡§≠ ‡§π‡•ã‡§®‡•á ‡§∏‡•á ‡§™‡§π‡§≤‡•á ‡§π‡•Ä ‡§â‡§∏ ‡§Ö‡§¶‡•ç‡§≠‡•Å‡§§ ‡§ï‡•ç‡§∑‡§§‡•ç‡§∞‡§ø‡§Ø ‡§ï‡§æ ‡§Æ‡§®‡•ã‡§¨‡§≤ ‡§ü‡•Ç‡§ü ‡§ö‡•Å‡§ï‡§æ ‡§π‡•à‡•§ ‡§¨‡§ø‡§®‡§æ ‡§Æ‡§® ‡§ï‡•á ‡§Ø‡§π ‡§∂‡§∞‡•Ä‡§∞ ‡§ñ‡§°‡§º‡§æ ‡§®‡§π‡•Ä‡§Ç ‡§∞‡§π ‡§∏‡•§\u0026#34; } POST geetyantra/_doc { \u0026#34;adhyay\u0026#34;:2, \u0026#34;updesh\u0026#34;:\u0026#34;‡§¶‡•Ç‡§∏‡§∞‡•á ‡§Ö‡§ß‡•ç‡§Ø‡§æ‡§Ø ‡§ï‡§æ ‡§®‡§æ‡§Æ ‡§∏‡§æ‡§Ç‡§ñ‡•ç‡§Ø‡§Ø‡•ã‡§ó ‡§π‡•à‡•§ ‡§á‡§∏‡§Æ‡•á‡§Ç ‡§ú‡•Ä‡§µ‡§® ‡§ï‡•Ä ‡§¶‡•ã ‡§™‡•ç‡§∞‡§æ‡§ö‡•Ä‡§® ‡§∏‡§Ç‡§Æ‡§æ‡§®‡§ø‡§§ ‡§™‡§∞‡§Ç‡§™‡§∞‡§æ‡§ì‡§Ç ‡§ï‡§æ ‡§§‡§∞‡•ç‡§ï‡•ã‡§Ç ‡§¶‡•ç‡§µ‡§æ‡§∞‡§æ ‡§µ‡§∞‡•ç‡§£‡§® ‡§Ü‡§Ø‡§æ ‡§π‡•à‡•§ ‡§Ö‡§∞‡•ç‡§ú‡•Å‡§® ‡§ï‡•ã ‡§â‡§∏ ‡§ï‡•É‡§™‡§£ ‡§∏‡•ç‡§•‡§ø‡§§‡§ø ‡§Æ‡•á‡§Ç ‡§∞‡•ã‡§§‡•á ‡§¶‡•á‡§ñ‡§ï‡§∞ ‡§ï‡•É‡§∑‡•ç‡§£ ‡§®‡•á ‡§â‡§®‡§ï‡§æ ‡§ß‡•ç‡§Ø‡§æ‡§® ‡§¶‡§ø‡§≤‡§æ‡§Ø‡§æ ‡§π‡•à ‡§ï‡§ø ‡§á‡§∏ ‡§™‡•ç‡§∞‡§ï‡§æ‡§∞ ‡§ï‡§æ ‡§ï‡•ç‡§≤‡•à‡§µ‡•ç‡§Ø ‡§î‡§∞ ‡§π‡•É‡§¶‡§Ø ‡§ï‡•Ä ‡§ï‡•ç‡§∑‡•Å‡§¶‡•ç‡§∞ ‡§¶‡•Å‡§∞‡•ç‡§¨‡§≤‡§§‡§æ ‡§Ö‡§∞‡•ç‡§ú‡•Å‡§® ‡§ú‡•à‡§∏‡•á ‡§µ‡•Ä‡§∞ ‡§ï‡•á ‡§≤‡§ø‡§è ‡§â‡§ö‡§ø‡§§ ‡§®‡§π‡•Ä‡§Ç‡•§\u0026#34; } POST geetyantra/_doc { \u0026#34;adhyay\u0026#34;:3, \u0026#34;updesh\u0026#34;:\u0026#34;‡§®‡§ø‡§§‡•ç‡§Ø ‡§ï‡§∞‡•ç‡§Æ ‡§ï‡§∞‡§®‡•á ‡§µ‡§æ‡§≤‡•á ‡§ï‡•Ä ‡§∂‡•ç‡§∞‡•á‡§∑‡•ç‡§†‡§§‡§æ\u0026#34; } POST geetyantra/_doc { \u0026#34;adhyay\u0026#34;:4, \u0026#34;updesh\u0026#34;:\u0026#34;‡§ö‡•å‡§•‡•á ‡§Ö‡§ß‡•ç‡§Ø‡§æ‡§Ø ‡§Æ‡•á‡§Ç, ‡§ú‡§ø‡§∏‡§ï‡§æ ‡§®‡§æ‡§Æ ‡§ú‡•ç‡§û‡§æ‡§®-‡§ï‡§∞‡•ç‡§Æ-‡§∏‡§Ç‡§®‡•ç‡§Ø‡§æ‡§∏-‡§Ø‡•ã‡§ó ‡§π‡•à, ‡§Ø‡§π ‡§¨‡§æ‡§§‡§æ‡§Ø‡§æ ‡§ó‡§Ø‡§æ ‡§π‡•à ‡§ï‡§ø ‡§ú‡•ç‡§û‡§æ‡§® ‡§™‡•ç‡§∞‡§æ‡§™‡•ç‡§§ ‡§ï‡§∞‡§ï‡•á ‡§ï‡§∞‡•ç‡§Æ ‡§ï‡§∞‡§§‡•á ‡§π‡•Å‡§è ‡§≠‡•Ä ‡§ï‡§∞‡•ç‡§Æ‡§∏‡§Ç‡§®‡•ç‡§Ø‡§æ‡§∏ ‡§ï‡§æ ‡§´‡§≤ ‡§ï‡§ø‡§∏ ‡§â‡§™‡§æ‡§Ø ‡§∏‡•á ‡§™‡•ç‡§∞‡§æ‡§™‡•ç‡§§ ‡§ï‡§ø‡§Ø‡§æ ‡§ú‡§æ ‡§∏‡§ï‡§§‡§æ ‡§π‡•à‡•§\u0026#34;, \u0026#34;shloka\u0026#34;:\u0026#34;‡§Ø‡§¶‡§æ ‡§Ø‡§¶‡§æ ‡§π‡§ø ‡§ß‡§∞‡•ç‡§Æ‡§∏‡•ç‡§Ø ‡§ó‡•ç‡§≤‡§æ‡§®‡§ø‡§∞‡•ç‡§≠‡§µ‡§§‡§ø ‡§≠‡§æ‡§∞‡§§,‡§Ö‡§≠‡•ç‡§Ø‡•Å‡§§‡•ç‡§•‡§æ‡§®‡§Æ‡§ß‡§∞‡•ç‡§Æ‡§∏‡•ç‡§Ø ‡§§‡§¶‡§æ‡§§‡•ç‡§Æ‡§æ‡§®‡§Ç ‡§∏‡•É‡§ú‡§æ‡§Æ‡•ç‡§Ø‡§π‡§Æ‡•ç\u0026#34; } POST geetyantra/_doc { \u0026#34;adhyay\u0026#34;:5, \u0026#34;updesh\u0026#34;:\u0026#34;‡§ú‡•ç‡§û‡§æ‡§®‡•Ä ‡§Æ‡§π‡§æ‡§™‡•Å‡§∞‡•Å‡§∑ ‡§µ‡§ø‡§¶‡•ç‡§Ø‡§æ-‡§µ‡§ø‡§®‡§Ø‡§Ø‡•Å‡§ï‡•ç‡§§ ‡§¨‡•ç‡§∞‡§æ‡§π‡•ç‡§Æ‡§£ ‡§Æ‡•á‡§Ç ‡§î‡§∞ ‡§ö‡§æ‡§£‡•ç‡§°‡§æ‡§≤ ‡§Æ‡•á‡§Ç ‡§§‡§•‡§æ ‡§ó‡§æ‡§Ø, ‡§π‡§æ‡§•‡•Ä ‡§è‡§µ‡§Ç ‡§ï‡•Å‡§§‡•ç‡§§‡•á ‡§Æ‡•á‡§Ç ‡§≠‡•Ä ‡§∏‡§Æ‡§∞‡•Ç‡§™ ‡§™‡§∞‡§Æ‡§æ‡§§‡•ç‡§Æ‡§æ ‡§ï‡•ã ‡§¶‡•á‡§ñ‡§®‡•á ‡§µ‡§æ‡§≤‡•á ‡§π‡•ã‡§§‡•á ‡§π‡•à‡§Ç‡•§\u0026#34;, \u0026#34;shloka\u0026#34;:\u0026#34;‡§µ‡§ø‡§¶‡•ç‡§Ø‡§æ‡§µ‡§ø‡§®‡§Ø‡§∏‡§Ç‡§™‡§®‡•ç‡§®‡•á ‡§¨‡•ç‡§∞‡§æ‡§π‡•ç‡§Æ‡§£‡•á ‡§ó‡§µ‡§ø ‡§π‡§∏‡•ç‡§§‡§ø‡§®‡§ø,‡§∂‡•Å‡§®‡§ø ‡§ö‡•à‡§µ ‡§∂‡•ç‡§µ‡§™‡§æ‡§ï‡•á ‡§ö ‡§™‡§Ç‡§°‡§ø‡§§‡§æ: ‡§∏‡§Æ‡§¶‡§∞‡•ç‡§∂‡§ø‡§®\u0026#34; } POST geetyantra/_doc { \u0026#34;adhyay\u0026#34;:6, \u0026#34;updesh\u0026#34;:\u0026#34;‡§õ‡§†‡§æ ‡§Ö‡§ß‡•ç‡§Ø‡§æ‡§Ø ‡§Ü‡§§‡•ç‡§Æ‡§∏‡§Ç‡§Ø‡§Æ ‡§Ø‡•ã‡§ó ‡§π‡•à ‡§ú‡§ø‡§∏‡§ï‡§æ ‡§µ‡§ø‡§∑‡§Ø ‡§®‡§æ‡§Æ ‡§∏‡•á ‡§π‡•Ä ‡§™‡•ç‡§∞‡§ï‡§ü ‡§π‡•à‡•§ ‡§ú‡§ø‡§§‡§®‡•á ‡§µ‡§ø‡§∑‡§Ø ‡§π‡•à‡§Ç ‡§â‡§® ‡§∏‡§¨‡§∏‡•á ‡§á‡§Ç‡§¶‡•ç‡§∞‡§ø‡§Ø‡•ã‡§Ç ‡§ï‡§æ ‡§∏‡§Ç‡§Ø‡§Æ-‡§Ø‡§π‡•Ä ‡§ï‡§∞‡•ç‡§Æ ‡§î‡§∞ ‡§ú‡•ç‡§û‡§æ‡§® ‡§ï‡§æ ‡§®‡§ø‡§ö‡•ã‡§°‡§º ‡§π‡•à‡•§ ‡§∏‡•Å‡§ñ ‡§Æ‡•á‡§Ç ‡§î‡§∞ ‡§¶‡•Å‡§ñ ‡§Æ‡•á‡§Ç ‡§Æ‡§® ‡§ï‡•Ä ‡§∏‡§Æ‡§æ‡§® ‡§∏‡•ç‡§•‡§ø‡§§‡§ø, ‡§á‡§∏‡•á ‡§π‡•Ä ‡§Ø‡•ã‡§ó ‡§ï‡§π‡§§‡•á ‡§π‡•à‡§Ç‡•§\u0026#34; } POST geetyantra/_doc { \u0026#34;adhyay\u0026#34;:7, \u0026#34;updesh\u0026#34;:\u0026#34;‡§™‡§Ç‡§ö‡§§‡§§‡•ç‡§µ, ‡§Æ‡§®, ‡§¨‡•Å‡§¶‡•ç‡§ß‡§ø ‡§≠‡•Ä ‡§Æ‡•à‡§Ç ‡§π‡•Ç‡§Å| ‡§Æ‡•à‡§Ç ‡§π‡•Ä ‡§∏‡§Ç‡§∏‡§æ‡§∞ ‡§ï‡•Ä ‡§â‡§§‡•ç‡§™‡§§‡•ç‡§§‡§ø ‡§ï‡§∞‡§§‡§æ ‡§π‡•Ç‡§Å ‡§î‡§∞ ‡§µ‡§ø‡§®‡§æ‡§∂ ‡§≠‡•Ä ‡§Æ‡•à‡§Ç ‡§π‡•Ä ‡§ï‡§∞‡§§‡§æ ‡§π‡•Ç‡§Å‡•§ ‡§Æ‡•á‡§∞‡•á ‡§≠‡§ï‡•ç‡§§ ‡§ö‡§æ‡§π‡•á ‡§ú‡§ø‡§∏ ‡§™‡•ç‡§∞‡§ï‡§æ‡§∞ ‡§≠‡§ú‡•á‡§Ç ‡§™‡§∞‡§®‡•ç‡§§‡•Å ‡§Ö‡§Ç‡§§‡§§‡§É ‡§Æ‡•Å‡§ù‡•á ‡§π‡•Ä ‡§™‡•ç‡§∞‡§æ‡§™‡•ç‡§§ ‡§π‡•ã‡§§‡•á ‡§π‡•à‡§Ç‡•§ ‡§Æ‡•à‡§Ç ‡§Ø‡•ã‡§ó‡§Æ‡§æ‡§Ø‡§æ ‡§∏‡•á ‡§Ö‡§™‡•ç‡§∞‡§ï‡§ü ‡§∞‡§π‡§§‡§æ ‡§π‡•Ç‡§Å ‡§î‡§∞ ‡§Æ‡•Å‡§∞‡•ç‡§ñ ‡§Æ‡•Å‡§ù‡•á ‡§ï‡•á‡§µ‡§≤ ‡§∏‡§æ‡§ß‡§æ‡§∞‡§£ ‡§Æ‡§®‡•Å‡§∑‡•ç‡§Ø ‡§π‡•Ä ‡§∏‡§Æ‡§ù‡§§‡•á ‡§π‡•à‡§Ç‡•§\u0026#34;, \u0026#34;shloka\u0026#34;:\u0026#34;‡§Ø‡•ã ‡§Ø‡•ã ‡§Ø‡§æ‡§Ç ‡§Ø‡§æ‡§Ç ‡§§‡§®‡•Å‡§Ç ‡§≠‡§ï‡•ç‡§§‡§É ‡§∂‡•ç‡§∞‡§¶‡•ç‡§ß‡§Ø‡§æ‡§∞‡•ç‡§ö‡§ø‡§§‡•Å‡§Æ‡§ø‡§ö‡•ç‡§õ‡§§‡§ø,‡§§‡§∏‡•ç‡§Ø ‡§§‡§∏‡•ç‡§Ø‡§æ‡§ö‡§≤‡§æ‡§Ç ‡§∂‡•ç‡§∞‡§¶‡•ç‡§ß‡§æ‡§Ç ‡§§‡§æ‡§Æ‡•á‡§µ ‡§µ‡§ø‡§¶‡§ß‡§æ‡§Æ‡•ç‡§Ø‡§π‡§Æ‡•ç\u0026#34; } POST geetyantra/_doc { \u0026#34;adhyay\u0026#34;:8, \u0026#34;updesh\u0026#34;:\u0026#34;‡§ï‡•Ä ‡§∏‡§Ç‡§ú‡•ç‡§û‡§æ ‡§Ö‡§ï‡•ç‡§∑‡§∞ ‡§¨‡•ç‡§∞‡§π‡•ç‡§Æ‡§Ø‡•ã‡§ó ‡§π‡•à‡•§ ‡§â‡§™‡§®‡§ø‡§∑‡§¶‡•ã‡§Ç ‡§Æ‡•á‡§Ç ‡§Ö‡§ï‡•ç‡§∑‡§∞ ‡§µ‡§ø‡§¶‡•ç‡§Ø‡§æ ‡§ï‡§æ ‡§µ‡§ø‡§∏‡•ç‡§§‡§æ‡§∞ ‡§π‡•Å‡§Ü‡•§ ‡§ó‡•Ä‡§§‡§æ ‡§Æ‡•á‡§Ç ‡§â‡§∏ ‡§Ö‡§ï‡•ç‡§∑‡§∞‡§µ‡§ø‡§¶‡•ç‡§Ø‡§æ ‡§ï‡§æ ‡§∏‡§æ‡§∞ ‡§ï‡§π ‡§¶‡§ø‡§Ø‡§æ ‡§ó‡§Ø‡§æ ‡§π‡•à-‡§Ö‡§ï‡•ç‡§∑‡§∞ ‡§¨‡•ç‡§∞‡§π‡•ç‡§Æ ‡§™‡§∞‡§Æ‡§Ç, ‡§Ö‡§∞‡•ç‡§•‡§æ‡§§‡•ç ‡§™‡§∞‡§¨‡•ç‡§∞‡§π‡•ç‡§Æ ‡§ï‡•Ä ‡§∏‡§Ç‡§ú‡•ç‡§û‡§æ ‡§Ö‡§ï‡•ç‡§∑‡§∞ ‡§π‡•à‡•§ ‡§Æ‡§®‡•Å‡§∑‡•ç‡§Ø, ‡§Ö‡§∞‡•ç‡§•‡§æ‡§§‡•ç ‡§ú‡•Ä‡§µ ‡§î‡§∞ ‡§∂‡§∞‡•Ä‡§∞ ‡§ï‡•Ä ‡§∏‡§Ç‡§Ø‡•Å‡§ï‡•ç‡§§ ‡§∞‡§ö‡§®‡§æ ‡§ï‡§æ ‡§π‡•Ä ‡§®‡§æ‡§Æ ‡§Ö‡§ß‡•ç‡§Ø‡§æ‡§§‡•ç‡§Æ ‡§π‡•à‡•§\u0026#34; } POST geetyantra/_doc { \u0026#34;adhyay\u0026#34;:9, \u0026#34;updesh\u0026#34;:\u0026#34;‡§ï‡•ã ‡§∞‡§æ‡§ú‡§ó‡•Å‡§π‡•ç‡§Ø‡§Ø‡•ã‡§ó ‡§ï‡§π‡§æ ‡§ó‡§Ø‡§æ ‡§π‡•à, ‡§Ö‡§∞‡•ç‡§•‡§æ‡§§‡•ç ‡§Ø‡§π ‡§Ö‡§ß‡•ç‡§Ø‡§æ‡§§‡•ç‡§Æ ‡§µ‡§ø‡§¶‡•ç‡§Ø‡§æ ‡§µ‡§ø‡§¶‡•ç‡§Ø‡§æ‡§∞‡§æ‡§ú‡•ç‡§û‡•Ä ‡§π‡•à ‡§î‡§∞ ‡§Ø‡§π ‡§ó‡•Å‡§π‡•ç‡§Ø ‡§ú‡•ç‡§û‡§æ‡§® ‡§∏‡§¨‡§Æ‡•á‡§Ç ‡§∂‡•ç‡§∞‡•á‡§∑‡•ç‡§† ‡§π‡•à‡•§ ‡§∞‡§æ‡§ú‡§æ ‡§∂‡§¨‡•ç‡§¶‡§ï‡§æ ‡§è‡§ï ‡§Ö‡§∞‡•ç‡§• ‡§Æ‡§® ‡§≠‡•Ä ‡§•‡§æ‡•§ ‡§Ö‡§§‡§è‡§µ ‡§Æ‡§® ‡§ï‡•Ä ‡§¶‡§ø‡§µ‡•ç‡§Ø ‡§∂‡§ï‡•ç‡§§‡§ø‡§Æ‡§Ø‡•ã‡§Ç ‡§ï‡•ã ‡§ï‡§ø‡§∏ ‡§™‡•ç‡§∞‡§ï‡§æ‡§∞ ‡§¨‡•ç‡§∞‡§π‡•ç‡§Æ‡§Æ‡§Ø ‡§¨‡§®‡§æ‡§Ø‡§æ ‡§ú‡§æ‡§Ø, ‡§á‡§∏‡§ï‡•Ä ‡§Ø‡•Å‡§ï‡•ç‡§§‡§ø ‡§π‡•Ä ‡§∞‡§æ‡§ú‡§µ‡§ø‡§¶‡•ç‡§Ø‡§æ ‡§π‡•à‡•§\u0026#34; } POST geetyantra/_doc { \u0026#34;adhyay\u0026#34;:10, \u0026#34;updesh\u0026#34;:\u0026#34;‡§¶‡§∏‡§µ‡•á‡§Ç ‡§Ö‡§ß‡•ç‡§Ø‡§æ‡§Ø ‡§ï‡§æ ‡§®‡§æ‡§Æ ‡§µ‡§ø‡§≠‡•Ç‡§§‡§ø‡§Ø‡•ã‡§ó ‡§π‡•à‡•§ ‡§á‡§∏‡§ï‡§æ ‡§∏‡§æ‡§∞ ‡§Ø‡§π ‡§π‡•à ‡§ï‡§ø ‡§≤‡•ã‡§ï ‡§Æ‡•á‡§Ç ‡§ú‡§ø‡§§‡§®‡•á ‡§¶‡•á‡§µ‡§§‡§æ ‡§π‡•à‡§Ç, ‡§∏‡§¨ ‡§è‡§ï ‡§π‡•Ä ‡§≠‡§ó‡§µ‡§æ‡§®, ‡§ï‡•Ä ‡§µ‡§ø‡§≠‡•Ç‡§§‡§ø‡§Ø‡§æ‡§Å ‡§π‡•à‡§Ç, ‡§Æ‡§®‡•Å‡§∑‡•ç‡§Ø ‡§ï‡•á ‡§∏‡§Æ‡§∏‡•ç‡§§ ‡§ó‡•Å‡§£ ‡§î‡§∞ ‡§Ö‡§µ‡§ó‡•Å‡§£ ‡§≠‡§ó‡§µ‡§æ‡§® ‡§ï‡•Ä ‡§∂‡§ï‡•ç‡§§‡§ø ‡§ï‡•á ‡§π‡•Ä ‡§∞‡•Ç‡§™ ‡§π‡•à‡§Ç‡•§\u0026#34; } POST geetyantra/_doc { \u0026#34;adhyay\u0026#34;:11, \u0026#34;updesh\u0026#34;:\u0026#34;‡§ï‡§æ ‡§®‡§æ‡§Æ ‡§µ‡§ø‡§∂‡•ç‡§µ‡§∞‡•Ç‡§™‡§¶‡§∞‡•ç‡§∂‡§® ‡§Ø‡•ã‡§ó ‡§π‡•à‡•§ ‡§á‡§∏‡§Æ‡•á‡§Ç ‡§Ö‡§∞‡•ç‡§ú‡•Å‡§® ‡§®‡•á ‡§≠‡§ó‡§µ‡§æ‡§® ‡§ï‡§æ ‡§µ‡§ø‡§∂‡•ç‡§µ‡§∞‡•Ç‡§™ ‡§¶‡•á‡§ñ‡§æ‡•§ ‡§µ‡§ø‡§∞‡§æ‡§ü ‡§∞‡•Ç‡§™ ‡§ï‡§æ ‡§Ö‡§∞‡•ç‡§• ‡§π‡•à ‡§Æ‡§æ‡§®‡§µ‡•Ä‡§Ø ‡§ß‡§∞‡§æ‡§§‡§≤ ‡§î‡§∞ ‡§™‡§∞‡§ø‡§ß‡§ø ‡§ï‡•á ‡§ä‡§™‡§∞ ‡§ú‡•ã ‡§Ö‡§®‡§Ç‡§§ ‡§µ‡§ø‡§∂‡•ç‡§µ ‡§ï‡§æ ‡§™‡•ç‡§∞‡§æ‡§£‡§µ‡§Ç‡§§ ‡§∞‡§ö‡§®‡§æ‡§µ‡§ø‡§ß‡§æ‡§® ‡§π‡•à, ‡§â‡§∏‡§ï‡§æ ‡§∏‡§æ‡§ï‡•ç‡§∑‡§æ‡§§ ‡§¶‡§∞‡•ç‡§∂‡§®‡•§ ‡§µ‡§ø‡§∑‡•ç‡§£‡•Å ‡§ï‡§æ ‡§ú‡•ã ‡§ö‡§§‡•Å‡§∞‡•ç‡§≠‡•Å‡§ú ‡§∞‡•Ç‡§™ ‡§π‡•à, ‡§µ‡§π ‡§Æ‡§æ‡§®‡§µ‡•Ä‡§Ø ‡§ß‡§∞‡§æ‡§§‡§≤ ‡§™‡§∞ ‡§∏‡•å‡§Æ‡•ç‡§Ø‡§∞‡•Ç‡§™ ‡§π‡•à‡•§\u0026#34; } POST geetyantra/_doc { \u0026#34;adhyay\u0026#34;:12, \u0026#34;updesh\u0026#34;:\u0026#34;‡§ï‡§æ ‡§®‡§æ‡§Æ ‡§≠‡§ï‡•ç‡§§‡§ø ‡§Ø‡•ã‡§ó ‡§π‡•à‡•§ ‡§ú‡•ã ‡§ú‡§æ‡§®‡§®‡•á ‡§Ø‡•ã‡§ó‡•ç‡§Ø ‡§π‡•à‡•§ ‡§ú‡§ø‡§∏‡§ï‡•ã ‡§ú‡§æ‡§®‡§ï‡§∞ ‡§Æ‡§®‡•Å‡§∑‡•ç‡§Ø ‡§™‡§∞‡§Æ‡§æ‡§®‡§®‡•ç‡§¶ ‡§ï‡•ã ‡§™‡•ç‡§∞‡§æ‡§™‡•ç‡§§ ‡§π‡•ã ‡§ú‡§æ‡§§‡§æ ‡§π‡•à ‡§Ö‡§∞‡•ç‡§•‡§æ‡§§ ‡§µ‡•ã ‡§™‡§∞‡§Æ‡§æ‡§§‡•ç‡§Æ‡§æ ‡§π‡•Ä ‡§∏‡§§‡•ç‡§Ø ‡§π‡•à ‡•§‡•§\u0026#34; } POST geetyantra/_doc { \u0026#34;adhyay\u0026#34;:13, \u0026#34;updesh\u0026#34;:\u0026#34;‡§Æ‡•á‡§Ç ‡§è‡§ï ‡§∏‡•Ä‡§ß‡§æ ‡§µ‡§ø‡§∑‡§Ø ‡§ï‡•ç‡§∑‡•á‡§§‡•ç‡§∞ ‡§î‡§∞ ‡§ï‡•ç‡§∑‡•á‡§§‡•ç‡§∞‡§ú‡•ç‡§û ‡§ï‡§æ ‡§µ‡§ø‡§ö‡§æ‡§∞ ‡§π‡•à‡•§ ‡§Ø‡§π ‡§∂‡§∞‡•Ä‡§∞ ‡§ï‡•ç‡§∑‡•á‡§§‡•ç‡§∞ ‡§π‡•à, ‡§â‡§∏‡§ï‡§æ ‡§ú‡§æ‡§®‡§®‡•á‡§µ‡§æ‡§≤‡§æ ‡§ú‡•Ä‡§µ‡§æ‡§§‡•ç‡§Æ‡§æ ‡§ï‡•ç‡§∑‡•á‡§§‡•ç‡§∞‡§ú‡•ç‡§û ‡§π‡•à‡•§\u0026#34; } POST geetyantra/_doc { \u0026#34;adhyay\u0026#34;:14, \u0026#34;updesh\u0026#34;:\u0026#34;‡§ï‡§æ ‡§®‡§æ‡§Æ ‡§ó‡•Å‡§£‡§§‡•ç‡§∞‡§Ø ‡§µ‡§ø‡§≠‡§æ‡§ó ‡§Ø‡•ã‡§ó ‡§π‡•à‡•§ ‡§Ø‡§π ‡§µ‡§ø‡§∑‡§Ø ‡§∏‡§Æ‡§∏‡•ç‡§§ ‡§µ‡•à‡§¶‡§ø‡§ï, ‡§¶‡§æ‡§∞‡•ç‡§∂‡§®‡§ø‡§ï ‡§î‡§∞ ‡§™‡•å‡§∞‡§æ‡§£‡§ø‡§ï ‡§§‡§§‡•ç‡§µ‡§ö‡§ø‡§Ç‡§§‡§® ‡§ï‡§æ ‡§®‡§ø‡§ö‡•ã‡§°‡§º ‡§π‡•à-‡§∏‡§§‡•ç‡§µ, ‡§∞‡§ú, ‡§§‡§Æ ‡§®‡§æ‡§Æ‡§ï ‡§§‡•Ä‡§® ‡§ó‡•Å‡§£-‡§§‡•ç‡§∞‡§ø‡§ï‡•ã ‡§ï‡•Ä ‡§Ö‡§®‡•á‡§ï ‡§µ‡•ç‡§Ø‡§æ‡§ñ‡•ç‡§Ø‡§æ‡§è‡§Å ‡§π‡•à‡§Ç‡•§\u0026#34; } POST geetyantra/_doc { \u0026#34;adhyay\u0026#34;:15, \u0026#34;updesh\u0026#34;:\u0026#34;‡§ï‡§æ ‡§®‡§æ‡§Æ ‡§™‡•Å‡§∞‡•Å‡§∑‡•ã‡§§‡•ç‡§§‡§Æ‡§Ø‡•ã‡§ó ‡§π‡•à‡•§ ‡§á‡§∏‡§Æ‡•á‡§Ç ‡§µ‡§ø‡§∂‡•ç‡§µ ‡§ï‡§æ ‡§Ö‡§∂‡•ç‡§µ‡§§‡•ç‡§• ‡§ï‡•á ‡§∞‡•Ç‡§™ ‡§Æ‡•á‡§Ç ‡§µ‡§∞‡•ç‡§£‡§® ‡§ï‡§ø‡§Ø‡§æ ‡§ó‡§Ø‡§æ ‡§π‡•à‡•§ ‡§Ø‡§π ‡§Ö‡§∂‡•ç‡§µ‡§§‡•ç‡§• ‡§∞‡•Ç‡§™‡•Ä ‡§∏‡§Ç‡§∏‡§æ‡§∞ ‡§Æ‡§π‡§æ‡§® ‡§µ‡§ø‡§∏‡•ç‡§§‡§æ‡§∞‡§µ‡§æ‡§≤‡§æ ‡§π‡•à‡•§ ‡§¶‡•á‡§∂ ‡§î‡§∞ ‡§ï‡§æ‡§≤ ‡§Æ‡•á‡§Ç ‡§á‡§∏‡§ï‡§æ ‡§ï‡•ã‡§à ‡§Ö‡§Ç‡§§ ‡§®‡§π‡•Ä‡§Ç ‡§π‡•à‡•§\u0026#34; } POST geetyantra/_doc { \u0026#34;adhyay\u0026#34;:16, \u0026#34;updesh\u0026#34;:\u0026#34;‡§Æ‡•á‡§Ç ‡§¶‡•á‡§µ‡§æ‡§∏‡•Å‡§∞ ‡§∏‡§Ç‡§™‡§§‡•ç‡§§‡§ø ‡§ï‡§æ ‡§µ‡§ø‡§≠‡§æ‡§ó ‡§¨‡§§‡§æ‡§Ø‡§æ ‡§ó‡§Ø‡§æ ‡§π‡•à‡•§ ‡§Ü‡§∞‡§Ç‡§≠ ‡§∏‡•á ‡§π‡•Ä ‡§ã‡§ó‡•ç‡§¶‡•á‡§µ ‡§Æ‡•á‡§Ç ‡§∏‡•É‡§∑‡•ç‡§ü‡§ø ‡§ï‡•Ä ‡§ï‡§≤‡•ç‡§™‡§®‡§æ ‡§¶‡•à‡§µ‡•Ä ‡§î‡§∞ ‡§Ü‡§∏‡•Å‡§∞‡•Ä ‡§∂‡§ï‡•ç‡§§‡§ø‡§Ø‡•ã‡§Ç ‡§ï‡•á ‡§∞‡•Ç‡§™ ‡§Æ‡•á‡§Ç ‡§ï‡•Ä ‡§ó‡§à ‡§π‡•à‡•§ \u0026#34; } POST geetyantra/_doc { \u0026#34;adhyay\u0026#34;:17, \u0026#34;updesh\u0026#34;:\u0026#34;‡§ï‡•Ä ‡§∏‡§Ç‡§ú‡•ç‡§û‡§æ ‡§∂‡•ç‡§∞‡§¶‡•ç‡§ß‡§æ‡§§‡•ç‡§∞‡§Ø ‡§µ‡§ø‡§≠‡§æ‡§ó ‡§Ø‡•ã‡§ó ‡§π‡•à‡•§ ‡§á‡§∏‡§ï‡§æ ‡§∏‡§Ç‡§¨‡§Ç‡§ß ‡§∏‡§§, ‡§∞‡§ú ‡§î‡§∞ ‡§§‡§Æ, ‡§á‡§® ‡§§‡•Ä‡§® ‡§ó‡•Å‡§£‡•ã‡§Ç ‡§∏‡•á ‡§π‡•Ä ‡§π‡•à, ‡§Ö‡§∞‡•ç‡§•‡§æ‡§§‡•ç ‡§ú‡§ø‡§∏‡§Æ‡•á‡§Ç ‡§ú‡§ø‡§∏ ‡§ó‡•Å‡§£ ‡§ï‡§æ ‡§™‡•ç‡§∞‡§æ‡§¶‡•Å‡§∞‡•ç‡§≠‡§æ‡§µ ‡§π‡•ã‡§§‡§æ ‡§π‡•à, ‡§â‡§∏‡§ï‡•Ä ‡§∂‡•ç‡§∞‡§¶‡•ç‡§ß‡§æ ‡§Ø‡§æ ‡§ú‡•Ä‡§µ‡§® ‡§ï‡•Ä ‡§®‡§ø‡§∑‡•ç‡§†‡§æ ‡§µ‡•à‡§∏‡•Ä ‡§π‡•Ä ‡§¨‡§® ‡§ú‡§æ‡§§‡•Ä ‡§π‡•à‡•§\u0026#34; } POST geetyantra/_doc { \u0026#34;adhyay\u0026#34;:18, \u0026#34;updesh\u0026#34;:\u0026#34;‡§ï‡•Ä ‡§∏‡§Ç‡§ú‡•ç‡§û‡§æ ‡§Æ‡•ã‡§ï‡•ç‡§∑‡§∏‡§Ç‡§®‡•ç‡§Ø‡§æ‡§∏ ‡§Ø‡•ã‡§ó ‡§π‡•à‡•§ ‡§á‡§∏‡§Æ‡•á‡§Ç ‡§ó‡•Ä‡§§‡§æ ‡§ï‡•á ‡§∏‡§Æ‡§∏‡•ç‡§§ ‡§â‡§™‡§¶‡•á‡§∂‡•ã‡§Ç ‡§ï‡§æ ‡§∏‡§æ‡§∞ ‡§è‡§µ‡§Ç ‡§â‡§™‡§∏‡§Ç‡§π‡§æ‡§∞ ‡§π‡•à‡•§ ‡§Ø‡§π‡§æ‡§Å ‡§™‡•Å‡§®: ‡§¨‡§≤‡§™‡•Ç‡§∞‡•ç‡§µ‡§ï ‡§Æ‡§æ‡§®‡§µ ‡§ú‡•Ä‡§µ‡§® ‡§ï‡•á ‡§≤‡§ø‡§è ‡§§‡•Ä‡§® ‡§ó‡•Å‡§£‡•ã‡§Ç ‡§ï‡§æ ‡§Æ‡§π‡§§‡•ç‡§µ ‡§ï‡§π‡§æ ‡§ó‡§Ø‡§æ ‡§π‡•à‡•§ ‡§™‡•É‡§•‡•ç‡§µ‡•Ä ‡§ï‡•á ‡§Æ‡§æ‡§®‡§µ‡•ã‡§Ç ‡§Æ‡•á‡§Ç ‡§î‡§∞ ‡§∏‡•ç‡§µ‡§∞‡•ç‡§ó ‡§ï‡•á ‡§¶‡•á‡§µ‡§§‡§æ‡§ì‡§Ç ‡§Æ‡•á‡§Ç ‡§ï‡•ã‡§à ‡§≠‡•Ä ‡§ê‡§∏‡§æ ‡§®‡§π‡•Ä‡§Ç ‡§ú‡•ã ‡§™‡•ç‡§∞‡§ï‡•É‡§§‡§ø ‡§ï‡•á ‡§ö‡§≤‡§æ‡§è ‡§π‡•Å‡§è ‡§á‡§® ‡§§‡•Ä‡§® ‡§ó‡•Å‡§£‡•ã‡§Ç ‡§∏‡•á ‡§¨‡§ö‡§æ ‡§π‡•ã‡•§\u0026#34; } Search GET geetyantra/_search?size=20 GET geetyantra/_search { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;updesh\u0026#34;: \u0026#34;‡§Ö‡§∞‡•ç‡§ú‡•Å‡§®\u0026#34; } } } GET geetyantra/_search { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;shloka\u0026#34;: \u0026#34;‡§ß‡§∞‡•ç‡§Æ‡§∏‡•ç‡§Ø\u0026#34; } } } GET geetyantra/_search { \u0026#34;query\u0026#34;: { \u0026#34;multi_match\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;‡§∂‡•ç‡§∞‡§¶‡•ç‡§ß‡§æ\u0026#34;, \u0026#34;fields\u0026#34;: [\u0026#34;shloka\u0026#34;,\u0026#34;updesh\u0026#34;] } } } GET geetyantra/_search { \u0026#34;query\u0026#34;: { \u0026#34;match_phrase_prefix\u0026#34;: { \u0026#34;updesh\u0026#34;: \u0026#34;‡§ö‡•å‡§•‡•á ‡§Ö‡§ß‡•ç‡§Ø‡§æ‡§Ø ‡§Æ‡•á‡§Ç\u0026#34; } } } Search as you type Refer article Search as you type.\n","permalink":"https://ashish.one/talks/ws-es/","summary":"Sample Queries for Elasticsearch Workshop CRUD # Insert POST meetup/_doc/ { \u0026#34;name\u0026#34;:\u0026#34;Ashish Tiwari\u0026#34; } # Insert with id POST meetup/_doc/1 { \u0026#34;name\u0026#34;:\u0026#34;Ashish Tiwari\u0026#34; } # Search GET meetup/_search # Update POST meetup/_doc/1 { \u0026#34;name\u0026#34;:\u0026#34;Ashish\u0026#34;, \u0026#34;company\u0026#34;:\u0026#34;elastic\u0026#34;, \u0026#34;address\u0026#34;:\u0026#34;Navi Mumbai kharghar\u0026#34;, \u0026#34;skills\u0026#34;:{ \u0026#34;language\u0026#34;:[\u0026#34;php\u0026#34;,\u0026#34;java\u0026#34;,\u0026#34;node\u0026#34;], \u0026#34;database\u0026#34;:[\u0026#34;mysql\u0026#34;,\u0026#34;mongodb\u0026#34;], \u0026#34;search\u0026#34;:\u0026#34;elasticsearch\u0026#34; } } # search with query GET meetup/_search { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;address\u0026#34;: \u0026#34;navi\u0026#34; } } } # delete DELETE meetup BULK POST _bulk {\u0026#34;index\u0026#34;:{\u0026#34;_index\u0026#34;:\u0026#34;meetup\u0026#34;}} {\u0026#34;user_id\u0026#34;:1,\u0026#34;first_name\u0026#34;:\u0026#34;Yvonne\u0026#34;,\u0026#34;last_name\u0026#34;:\u0026#34;Willmott\u0026#34;,\u0026#34;email\u0026#34;:\u0026#34;ywillmott0@live.com\u0026#34;,\u0026#34;gender\u0026#34;:\u0026#34;Female\u0026#34;,\u0026#34;street_address\u0026#34;:\u0026#34;38 Helena Avenue\u0026#34;,\u0026#34;ip_address\u0026#34;:\u0026#34;104.","title":"Getting started with Elasticsearch"},{"content":"Just a thought üí≠ \u0026hellip; As a Developer, we keep learning \u0026amp; developing stuff. Sometimes we also find the solutions. Our stuff is distributed the same as our System architecture. We maintain different platforms like GitHub for projects, Medium for blogs, LinkedIn for profile, etc.\nAll this stuff we want to share on a single platform but as a tech, I am lazy if you ask me to install some CMS and maintain all stuff over there. it is hard to switch from a black terminal üíª window to some UI üòÖ\nAs a developer I was always thinking can I write my blogs, article or notes in Vim? What if we can publish the blog the same as we do releases by hitting some commands.\nIn this gist, I am quickly going to give you some suggestions for tools that will help you to build your platform.\nThe Requirement üìù Write a blog in any editor (For me its Vim) Easy to publish Developer Friendly Lightweight and speed Cost effective Let\u0026rsquo;s Build üîß I was evaluating multiple tools while searching for a platform and found Hugo which is easy, fast, and handy as a developer. Hugo is an opensource static site generator which means you can write everything in Markdown and Hugo will generate the site accordingly. You need to know the basic Markdown syntax and you are good to go.\nYou can refer official hugo document to get started.\nBelow is some tool that helped me to set up my platform.\nTheme Lots of themes are present which you can configure with your Hugo site. There are some common features across the themes and some themes provide the special features also. You can explore all themes here themes.gohugo.io\nI was looking for a simple theme which has a simple layout with menus, a dark theme, and tech friendly.\nPaperMod I have build my website in PaperMod. Few pointers why i choose:\n1. Search üîç PaperMod uses Fuse.js Basic for search functionality.\n2. Post Cover Image üóº It gives an easy option to add a cover image to your post.\n3. Edit link for post ‚úèÔ∏è Suggest changes option to ask viewers to contribute or Raise PR.\nYou can check more details about all features here\nhyde-hyde hyde-hyde I found a simple and easy theme if you want to get started with a simple menu and post. I chooses this because of its simplicity and then migrated to PaperMod.\nComments üí¨ Once your audience starts reading your article, they would like to give feedback, suggestion and sometime it could be a discussion. You need someplace like comments where viewers can add their points to a particular article. The theme does not come with comments, for that you need to integrate the comments tool. Below are some suggestions you can explore:\n1. giscus Giscus comments system powered by GitHub Discussions. Let visitors leave comments and reactions on your website via GitHub! As soon as your viewers comment on your article, it will create the discussion thread on Github Discussion. You can explore more about giscus on the official site.\n2. Utterances utterances lightweight comments system built powered by GitHub issues. It will create an issue per article once anyone comments. All comments will be associated with a particular Github issue.\n3. DISQUS Disqus is a blog comment hosting paid service for websites and online communities that use a networked platform. It also comes with social integration, social networks, user profiles, profile notifications, etc.\nI migrated my comment system from utterances to giscus. As Github discussion is a proper tool for commenting, discussion, etc. Both are lightweight and you can choose accordingly.\nShortcodes While developing the site with Hugo, Shortcodes are your friends. Always check if shortcodes are available for popular tools e.g. youtube, github gist, etc.\nHosting ‚òÅÔ∏è After the site generation, you need to host your website somewhere.\nFound Github pages best place to host your static site. It allows you to manage your website the same way you manage your projects on github. Get started by creating a simple repository and pushing your source directory.\nCheck out all steps here\nOnce you set up end to end, You can publish your blog or changes by just basic git commands i.e. git add, git commit, git push.\nIt will give you the same feeling as you are releasing some features for your project.\nYou can check more options about hosting and deployment\nI have hosted current website on github.com/ashishtiwari1993.\nDomain By default, Github pages assign the domain like username.github.io and you can access your site by visiting that subdomain. You can also setup the custom domain.\nSpeed üöÄ Almost 100% Page speed.\nPagespeed insights\ntl;dr ‚úÖ Platform - Hugo\n‚úÖ Theme - PaperMod\n‚úÖ Comment - giscus\n‚úÖ Hosting - Github Pages\n‚úÖ PageSpeed - 100%\n‚úÖ Cost - 100% Free\n","permalink":"https://ashish.one/blogs/build-developer-profile/","summary":"Just a thought üí≠ \u0026hellip; As a Developer, we keep learning \u0026amp; developing stuff. Sometimes we also find the solutions. Our stuff is distributed the same as our System architecture. We maintain different platforms like GitHub for projects, Medium for blogs, LinkedIn for profile, etc.\nAll this stuff we want to share on a single platform but as a tech, I am lazy if you ask me to install some CMS and maintain all stuff over there.","title":"How to build a Developer Profile - HugoConf2022"},{"content":"Introduction In this gist, we will quickly try to spin Elastic stacks with Docker containers. We are going to use docker-compose. You can learn more about Docker \u0026amp; Docker Compose, Which will help you to understand the flow.\nPrerequisite Tested on the below configuration.\ndocker:Docker version 20.10.16, build aa7e414 docker-compose:Docker version 20.10.16, build aa7e414 Cluster This setup will include\nElasticsearch Kibana Logstash APM Setup Clone repo:\ngit clone https://github.com/ashishtiwari1993/elastic-docker.git cd elastic-docker Make changes in .env file.\nStart the cluster Start docker-compose up -d Just visit to localhost:5601. You should see a kibana login page.\nStop docker-compose down Stop with deleting network, containers and volumes docker-compose down -v Access stacks Elasticsearch Access via curl from host machine Copy ca.crt file docker cp elastic-docker_es01_1:/usr/share/elasticsearch/config/certs/ca/ca.crt /tmp/ Curl command curl --cacert /tmp/ca.crt -u elastic:pass@123 https://localhost:9200 Logstash You need to have pipeline configuration files on LOGSTASH_PIPELINE_PATH location. If there will be no file, Logstash will throw an error and get exit. NOTE You can simply comment other stacks which is not needed. For example if you want to just run Elasticsearch \u0026amp; Kibana, Just comment the APM or other stack specification.\n","permalink":"https://ashish.one/blogs/elastic-docker-compose/","summary":"Introduction In this gist, we will quickly try to spin Elastic stacks with Docker containers. We are going to use docker-compose. You can learn more about Docker \u0026amp; Docker Compose, Which will help you to understand the flow.\nPrerequisite Tested on the below configuration.\ndocker:Docker version 20.10.16, build aa7e414 docker-compose:Docker version 20.10.16, build aa7e414 Cluster This setup will include\nElasticsearch Kibana Logstash APM Setup Clone repo:\ngit clone https://github.com/ashishtiwari1993/elastic-docker.git cd elastic-docker Make changes in .","title":"Start a single node elastic cluster with Docker Compose"},{"content":"Introduction As a developer, you need to log everything it may be info, error or debug logs, etc. There are multiple types of log formats like Common log, JSON log, etc. and there are already solutions available in an elastic stack like filebeat to read JSON logs and push them to elasticsearch.\nThere can be cases where you need to log the data according to your convenience which will not be any standard log format. Or sometimes you just need to dump the log in an unstructured way but you need to have it in a structured format if you want to analyze those.\nIn this article, we are going to see how we can parse custom logs of any format into Elasticsearch. Elasticsearch provides the ingest pipeline with grok processor which will be able to match any unstructured log.\nOverview Ingest Pipeline Elasticsearch gives ingest pipeline where it let you perform different types of operations or data transformations before indexing the data. The pipeline consists of a series of various processors.\nEach processor in the pipeline takes input and produces the output. That output goes as input to the next processor.\nLike below, After converting into lowercase letters by \u0026ldquo;lowercase processor\u0026rdquo; it sends the same string as input to the \u0026ldquo;split processor\u0026rdquo;.\nExample\n{\u0026#34;date\u0026#34;:\u0026#34;HELLO - WORLD\u0026#34;} Input ‚Üí | lowercase processor ({\u0026#34;date\u0026#34;:\u0026#34;hello - world\u0026#34;}) ‚Üí split processor ({\u0026#34;date\u0026#34;:\u0026#34;hello world\u0026#34;}) | ‚Üí Index You can check more about ingest pipeline and processors.\nGrok Processor Grok Processor allows you to extract structured data from the ingested value on the specific field. Grok Processor works with grok patterns.\nLet\u0026rsquo;s suppose you have the following document\n{ \u0026#34;Ip\u0026#34;:\u0026#34;1.2.3.4\u0026#34;, \u0026#34;country\u0026#34;:\u0026#34;India\u0026#34;, \u0026#34;log\u0026#34;:\u0026#34;LEVEL:ERROR,method:fetchUser(),message:user not found\u0026#34; } You can mention the field name (Ex. log) where you want to apply a grok processor. The Grok processor uses the Grok pattern.\nGrok pattern Grok pattern is the regular expression which is the alias of some of the predefined expressions which can be reused.\nBelow is some sample grok patterns:\nHOSTNAME \\b(?:[0-9A-Za-z][0-9A-Za-z-]{0,62})(?:\\.(?:[0-9A-Za-z][0-9A-Za-z-]{0,62}))*(\\.?|\\b) USERNAME [a-zA-Z0-9._-]+ USER %{USERNAME} SPACE \\s* Here HOSTNAME, USERNAME, USER \u0026amp; SPACE are Grok pattern names.\nI can directly use SPACE label to define regex for space instead of a regular pattern like \\s*. Also, the Grok pattern can be reused as the same as USERNAME.\nPattern list You can find all pattern list on the below link:\nhttps://github.com/elastic/elasticsearch/tree/8.1/libs/grok/src/main/resources/patterns\nYou can select any folder and explore all the patterns.\nGrok pattern: https://github.com/elastic/elasticsearch/blob/master/libs/grok/src/main/resources/patterns/ecs-v1/grok-patterns\nThe flow Create \u0026amp; Test Ingest Pipeline Index custom data Lets create the pipeline with a grok processor which is going to parse the message field with the document below.\n{ \u0026#34;Ip\u0026#34;:\u0026#34;1.2.3.4\u0026#34;, \u0026#34;country\u0026#34;:\u0026#34;India\u0026#34;, \u0026#34;message\u0026#34;:\u0026#34;LEVEL:ERROR,method:fetchUser(),message:user not found,code:123\u0026#34; } I would like to extract the below data from message field and index into elasticsearch,\n\u0026#34;method_name\u0026#34; : \u0026#34;fetchUser\u0026#34;, \u0026#34;log_level\u0026#34; : \u0026#34;ERROR\u0026#34;, \u0026#34;response_code\u0026#34; : \u0026#34;123\u0026#34;, \u0026#34;message\u0026#34; : \u0026#34;user not found\u0026#34; Create \u0026amp; Test Ingest Pipeline Syntax %{SYNTAX:ID} SYNTAX: It is the pattern name.\nID: It is the key name of the document.\nData need to parse\nLEVEL:ERROR,method:fetchUser(),message:user not found Let\u0026rsquo;s match some of the grok patterns from the below file https://github.com/elastic/elasticsearch/blob/master/libs/grok/src/main/resources/patterns/ecs-v1/grok-patterns\nERROR = %{LOGLEVEL:log_level} fetchUser = %{WORD:method_name} user not found = %{GREEDYDATA:message} 123 = code:(?\u0026lt;response_code\u0026gt;(?:[+-]?(?:[0-9]+))) Data\nLEVEL:ERROR,method:fetchUser(),message:user not found Final Pattern\n\u0026#34;LEVEL:%{LOGLEVEL:log_level},method:%{WORD:method_name}\\\\(\\\\),message:%{GREEDYDATA:message},code:(?\u0026lt;response_code\u0026gt;(?:[+-]?(?:[0-9]+)))\u0026#34; Note: You can add your own regex as well. For example, I have added regex for code.\nTest pipeline For testing above pattern or pipeline we can use Simulate Pipeline API like below:\n_simulate POST /_ingest/pipeline/_simulate?pretty { \u0026#34;pipeline\u0026#34;: { \u0026#34;description\u0026#34; : \u0026#34;testing grok processor\u0026#34;, \u0026#34;processors\u0026#34;: [ { \u0026#34;grok\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;log\u0026#34;, \u0026#34;patterns\u0026#34;: [\u0026#34;LEVEL:%{LOGLEVEL:log_level},method:%{WORD:method_name}\\\\(\\\\),message:%{GREEDYDATA:message},code:(?\u0026lt;response_code\u0026gt;(?:[+-]?(?:[0-9]+)))\u0026#34;] } } ] }, \u0026#34;docs\u0026#34;:[ { \u0026#34;_source\u0026#34;: { \u0026#34;log\u0026#34;: \u0026#34;LEVEL:ERROR,method:fetchUser(),message:user not found,code:123\u0026#34; } } ] } Response { \u0026#34;docs\u0026#34; : [ { \u0026#34;doc\u0026#34; : { \u0026#34;_index\u0026#34; : \u0026#34;_index\u0026#34;, \u0026#34;_id\u0026#34; : \u0026#34;_id\u0026#34;, \u0026#34;_source\u0026#34; : { \u0026#34;response_code\u0026#34; : \u0026#34;123\u0026#34;, \u0026#34;log\u0026#34; : \u0026#34;LEVEL:ERROR,method:fetchUser(),message:user not found,code:123\u0026#34;, \u0026#34;method_name\u0026#34; : \u0026#34;fetchUser\u0026#34;, \u0026#34;log_level\u0026#34; : \u0026#34;ERROR\u0026#34;, \u0026#34;message\u0026#34; : \u0026#34;user not found\u0026#34; }, \u0026#34;_ingest\u0026#34; : { \u0026#34;timestamp\u0026#34; : \u0026#34;2022-04-29T14:20:59.454111422Z\u0026#34; } } } ] } As you can see we got the desired output. Now let\u0026rsquo;s save the pipeline.\nSave pipeline Pipeline can be saved using _ingest/pipeline API.\nPUT /_ingest/pipeline/custom_log?pretty { \u0026#34;description\u0026#34; : \u0026#34;testing grok processor\u0026#34;, \u0026#34;processors\u0026#34;: [ { \u0026#34;grok\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;log\u0026#34;, \u0026#34;patterns\u0026#34;: [\u0026#34;LEVEL:%{LOGLEVEL:log_level},method:%{WORD:method_name}\\\\(\\\\),message:%{GREEDYDATA:message},code:(?\u0026lt;response_code\u0026gt;(?:[+-]?(?:[0-9]+)))\u0026#34;] } } ] } Custom_log: It is the pipeline name. We need to specify the pipeline name while indexing the data.\nIndex custom data Now let\u0026rsquo;s index some data with the same pipeline which we created.\nPOST data-stream/_doc?pipeline=custom_log { \u0026#34;log\u0026#34; : \u0026#34;LEVEL:INFO,method:addUser(),message:user added successfully.,code:200\u0026#34; } POST data-stream/_doc?pipeline=custom_log { \u0026#34;log\u0026#34; : \u0026#34;LEVEL:DEBUG,method:deleteUser(),message:user_id notfound.,code:433\u0026#34; } POST data-stream/_doc?pipeline=custom_log { \u0026#34;log\u0026#34; : \u0026#34;LEVEL:ERROR,method:fetchUser(),message:Database connection timeout,code:567\u0026#34; } Check all data\nGET data-stream/_search?pretty { \u0026#34;took\u0026#34; : 898, \u0026#34;timed_out\u0026#34; : false, \u0026#34;_shards\u0026#34; : { \u0026#34;total\u0026#34; : 1, \u0026#34;successful\u0026#34; : 1, \u0026#34;skipped\u0026#34; : 0, \u0026#34;failed\u0026#34; : 0 }, \u0026#34;hits\u0026#34; : { \u0026#34;total\u0026#34; : { \u0026#34;value\u0026#34; : 3, \u0026#34;relation\u0026#34; : \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34; : 1.0, \u0026#34;hits\u0026#34; : [ { \u0026#34;_index\u0026#34; : \u0026#34;data-stream\u0026#34;, \u0026#34;_id\u0026#34; : \u0026#34;3460dYABYeDE95hG23XJ\u0026#34;, \u0026#34;_score\u0026#34; : 1.0, \u0026#34;_source\u0026#34; : { \u0026#34;response_code\u0026#34; : \u0026#34;200\u0026#34;, \u0026#34;log\u0026#34; : \u0026#34;LEVEL:INFO,method:addUser(),message:user added successfully.,code:200\u0026#34;, \u0026#34;method_name\u0026#34; : \u0026#34;addUser\u0026#34;, \u0026#34;log_level\u0026#34; : \u0026#34;INFO\u0026#34;, \u0026#34;message\u0026#34; : \u0026#34;user added successfully.\u0026#34; } }, { \u0026#34;_index\u0026#34; : \u0026#34;data-stream\u0026#34;, \u0026#34;_id\u0026#34; : \u0026#34;Kwi2dYABbuPmoOiCSUjL\u0026#34;, \u0026#34;_score\u0026#34; : 1.0, \u0026#34;_source\u0026#34; : { \u0026#34;response_code\u0026#34; : \u0026#34;433\u0026#34;, \u0026#34;log\u0026#34; : \u0026#34;LEVEL:DEBUG,method:deleteUser(),message:user_id notfound.,code:433\u0026#34;, \u0026#34;method_name\u0026#34; : \u0026#34;deleteUser\u0026#34;, \u0026#34;log_level\u0026#34; : \u0026#34;DEBUG\u0026#34;, \u0026#34;message\u0026#34; : \u0026#34;user_id notfound.\u0026#34; } }, { \u0026#34;_index\u0026#34; : \u0026#34;data-stream\u0026#34;, \u0026#34;_id\u0026#34; : \u0026#34;LAi4dYABbuPmoOiCmUgb\u0026#34;, \u0026#34;_score\u0026#34; : 1.0, \u0026#34;_source\u0026#34; : { \u0026#34;response_code\u0026#34; : \u0026#34;567\u0026#34;, \u0026#34;log\u0026#34; : \u0026#34;LEVEL:ERROR,method:fetchUser(),message:Database connection timeout,code:567\u0026#34;, \u0026#34;method_name\u0026#34; : \u0026#34;fetchUser\u0026#34;, \u0026#34;log_level\u0026#34; : \u0026#34;ERROR\u0026#34;, \u0026#34;message\u0026#34; : \u0026#34;Database connection timeout\u0026#34; } } ] } } Parsing common log format Common log format is standard format for logging which is used by webservers and system log generators etc.\nLet\u0026rsquo;s say we having below log:\n97.92.139.153 1254 ashishtiwari [04/Mar/2022:15:18:55 +0530] \u0026#34;GET /niches HTTP/1.1\u0026#34; 201 2322 Grok Pattern Final Grok Pattern %{IPORHOST:ip} %{USER:user_id} %{USERNAME:username} \\\\[%{HTTPDATE:date}\\\\] \\\u0026#34;%{WORD:request.method} %{URIPATH:request.path} %{URIPROTO:request.proto}/%{NUMBER:request.http_version}\\\u0026#34; %{NUMBER:request.response} %{NUMBER:request.size_bytes} _simulate Lets simulate the pipeline with above grok pattern\nPOST /_ingest/pipeline/_simulate?pretty { \u0026#34;pipeline\u0026#34;: { \u0026#34;description\u0026#34; : \u0026#34;testing grok processor\u0026#34;, \u0026#34;processors\u0026#34;: [ { \u0026#34;grok\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;log\u0026#34;, \u0026#34;patterns\u0026#34;: [\u0026#34;%{IPORHOST:ip} %{USER:user_id} %{USERNAME:username} \\\\[%{HTTPDATE:date}\\\\] \\\u0026#34;%{WORD:request.method} %{URIPATH:request.path} %{URIPROTO:request.proto}/%{NUMBER:request.http_version}\\\u0026#34; %{NUMBER:request.response} %{NUMBER:request.size_bytes}\u0026#34;] } } ] }, \u0026#34;docs\u0026#34;:[ { \u0026#34;_source\u0026#34;: { \u0026#34;log\u0026#34;: \u0026#34;97.92.139.153 1254 ashishtiwari [04/Mar/2022:15:18:55 +0530] \\\u0026#34;GET /niches HTTP/1.1\\\u0026#34; 201 2322\u0026#34; } } ] } Response { \u0026#34;docs\u0026#34; : [ { \u0026#34;doc\u0026#34; : { \u0026#34;_index\u0026#34; : \u0026#34;_index\u0026#34;, \u0026#34;_id\u0026#34; : \u0026#34;_id\u0026#34;, \u0026#34;_source\u0026#34; : { \u0026#34;date\u0026#34; : \u0026#34;04/Mar/2022:15:18:55 +0530\u0026#34;, \u0026#34;request\u0026#34; : { \u0026#34;path\u0026#34; : \u0026#34;/niches\u0026#34;, \u0026#34;size_bytes\u0026#34; : \u0026#34;2322\u0026#34;, \u0026#34;method\u0026#34; : \u0026#34;GET\u0026#34;, \u0026#34;response\u0026#34; : \u0026#34;201\u0026#34;, \u0026#34;proto\u0026#34; : \u0026#34;HTTP\u0026#34;, \u0026#34;http_version\u0026#34; : \u0026#34;1.1\u0026#34; }, \u0026#34;log\u0026#34; : \u0026#34;\u0026#34;\u0026#34;97.92.139.153 1254 ashishtiwari [04/Mar/2022:15:18:55 +0530] \u0026#34;GET /niches HTTP/1.1\u0026#34; 201 2322\u0026#34;\u0026#34;\u0026#34;, \u0026#34;user_id\u0026#34; : \u0026#34;1254\u0026#34;, \u0026#34;ip\u0026#34; : \u0026#34;97.92.139.153\u0026#34;, \u0026#34;username\u0026#34; : \u0026#34;ashishtiwari\u0026#34; }, \u0026#34;_ingest\u0026#34; : { \u0026#34;timestamp\u0026#34; : \u0026#34;2022-04-29T14:42:51.765427484Z\u0026#34; } } } ] } As we can see it successfully parsed the data. You can save the pipeline and index the data as shown on the above steps.\nSimilarly you can parse any log format with Grok pattern.\nDemo You can also check the demo on the below link where I presented how you can parse common log format using ingest pipeline and grok processor.\nhttps://ashish.one/talks/devops-conf-2022/\nConclusion We have seen how we can parse any types of log format into elasticsearch with the help of ingest pipeline \u0026amp; grok processor. You can also use this ingest pipeline with filebeat or logstash. Where you just need to specify the pipeline name.\nFeel free to put in comments if you have any doubts.\n","permalink":"https://ashish.one/blogs/parsing-custom-log-format-to-the-elasticsearch/","summary":"Introduction As a developer, you need to log everything it may be info, error or debug logs, etc. There are multiple types of log formats like Common log, JSON log, etc. and there are already solutions available in an elastic stack like filebeat to read JSON logs and push them to elasticsearch.\nThere can be cases where you need to log the data according to your convenience which will not be any standard log format.","title":"Parsing Custom log format to the Elasticsearch"},{"content":"Introduction In this blog, we will try to understand how ‚ÄúSearch as you type‚Äù works and Quickly setup one demo using some sample data. You must have seen various websites like eCommerce, food apps, etc. where you just start typing \u0026amp; simultaneously relevant options start displaying as suggestions and autocomplete. We will try to achieve somewhat the same feature. Search as you type Elasticsearch gives this specific mapping type which you can simply set to a specific field where you want to perform this kind of search.\nWhy search_as_you_type? No need to think about what kind of functionality like analyzer, tokenizer, etc. you have to apply to achieve this. It automatically handles everything in the backend by producing necessary terms on which you can query efficiently.\nYou can simply create mapping like the below example:\nCreate Index PUT products { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;description\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;search_as_you_type\u0026#34; } } } } Insert sample data POST products/_doc/ { \u0026#34;description\u0026#34;: \u0026#34;best jogging shoes for men\u0026#34; } How is data indexed ? search_as_you_type mapping creates 4 types of fields in the backend.\nField 1: description It will produce the terms according to the default analyzer if no analyzer is defined i.e. standard analyzer.\n[ \u0026#34;best\u0026#34;, \u0026#34;jogging\u0026#34;, \u0026#34;shoes\u0026#34;, \u0026#34;for\u0026#34;, \u0026#34;men\u0026#34; ] Field 2: description._2gram This will use a shingle token filter and produce the terms with shingle size 2. This means a shingle token filter produces the token by concatenating the adjacent token. You can find more here.\nThis operation will perform on all the terms which are created on the description field and it will produce the below terms.\n[ \u0026#34;best jogging\u0026#34;, \u0026#34;jogging shoes\u0026#34;, \u0026#34;shoes for\u0026#34;, \u0026#34;for men\u0026#34; ] Field 3: description._3gram This will also use a shingle token filter and produce the terms with shingle size 3. This means it will concatenate 3 adjacent tokens like below.\n[ \u0026#34;best jogging shoes\u0026#34;, \u0026#34;jogging shoes for\u0026#34;, \u0026#34;shoes for men\u0026#34; ] Field 4: description._index_prefix This will apply an edge n gram token filter on the field description._3gram which means it will split terms (words) of description._3gram to a small substring that will start from the edge. You can have a look at the terms below.\n[ \u0026#34;b\u0026#34;, \u0026#34;be\u0026#34;, \u0026#34;bes\u0026#34;, \u0026#34;best\u0026#34;, \u0026#34;best \u0026#34;, \u0026#34;best j\u0026#34;, \u0026#34;best jo\u0026#34;, \u0026#34;best jog\u0026#34;, \u0026#34;best jogg\u0026#34;, \u0026#34;best joggi\u0026#34;, \u0026#34;best joggin\u0026#34;, \u0026#34;best jogging\u0026#34;, \u0026#34;best jogging \u0026#34;, \u0026#34;best jogging s\u0026#34;, \u0026#34;best jogging sh\u0026#34;, \u0026#34;best jogging sho\u0026#34;, \u0026#34;best jogging shoe\u0026#34;, \u0026#34;best jogging shoes\u0026#34;, \u0026#34;j\u0026#34;, \u0026#34;jo\u0026#34;, \u0026#34;jog\u0026#34;, \u0026#34;jogg\u0026#34;, \u0026#34;joggi\u0026#34;, \u0026#34;joggin\u0026#34;, \u0026#34;jogging\u0026#34;, \u0026#34;jogging \u0026#34;, \u0026#34;jogging s\u0026#34;, \u0026#34;jogging sh\u0026#34;, \u0026#34;jogging sho\u0026#34;, \u0026#34;jogging shoe\u0026#34;, \u0026#34;jogging shoes\u0026#34;, \u0026#34;jogging shoes \u0026#34;, \u0026#34;jogging shoes f\u0026#34;, \u0026#34;jogging shoes fo\u0026#34;, \u0026#34;jogging shoes for\u0026#34;, \u0026#34;s\u0026#34;, \u0026#34;sh\u0026#34;, \u0026#34;sho\u0026#34;, \u0026#34;shoe\u0026#34;, \u0026#34;shoes\u0026#34;, \u0026#34;shoes \u0026#34;, \u0026#34;shoes f\u0026#34;, \u0026#34;shoes fo\u0026#34;, \u0026#34;shoes for\u0026#34;, \u0026#34;shoes for \u0026#34;, \u0026#34;shoes for m\u0026#34;, \u0026#34;shoes for me\u0026#34;, \u0026#34;shoes for men\u0026#34;, \u0026#34;f\u0026#34;, \u0026#34;fo\u0026#34;, \u0026#34;for\u0026#34;, \u0026#34;for \u0026#34;, \u0026#34;for m\u0026#34;, \u0026#34;for me\u0026#34;, \u0026#34;for men\u0026#34;, \u0026#34;for men \u0026#34;, \u0026#34;m\u0026#34;, \u0026#34;me\u0026#34;, \u0026#34;men\u0026#34;, \u0026#34;men \u0026#34;, \u0026#34;men \u0026#34; ] As you noticed the token limit is up to 3 words only because description._3gram has generated 3 word tokens only.\nSearch Query multi_match We will use a multi_match query here. Because we want to look up on each subfield for a perfect match.\nGET products/_search { \u0026#34;query\u0026#34;: { \u0026#34;multi_match\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;jogging\u0026#34;, \u0026#34;fields\u0026#34;: [ \u0026#34;description\u0026#34;, \u0026#34;description._2gram\u0026#34;, \u0026#34;description._3gram\u0026#34; ] } } } The above query is going to search the term \u0026quot;jogging\u0026quot; on all 3 subfields which are specified in fields[].\n{ \u0026#34;took\u0026#34; : 2, \u0026#34;timed_out\u0026#34; : false, \u0026#34;_shards\u0026#34; : { \u0026#34;total\u0026#34; : 1, \u0026#34;successful\u0026#34; : 1, \u0026#34;skipped\u0026#34; : 0, \u0026#34;failed\u0026#34; : 0 }, \u0026#34;hits\u0026#34; : { \u0026#34;total\u0026#34; : { \u0026#34;value\u0026#34; : 1, \u0026#34;relation\u0026#34; : \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34; : 0.2876821, \u0026#34;hits\u0026#34; : [ { \u0026#34;_index\u0026#34; : \u0026#34;products\u0026#34;, \u0026#34;_id\u0026#34; : \u0026#34;TSNNiX8BYh0NLleBiu4u\u0026#34;, \u0026#34;_score\u0026#34; : 0.2876821, \u0026#34;_source\u0026#34; : { \u0026#34;description\u0026#34; : \u0026#34;best jogging shoes for men\u0026#34; } } ] } } Lets try with any substring (jog).\nGET products/_search { \u0026#34;query\u0026#34;: { \u0026#34;multi_match\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;jog\u0026#34;, \u0026#34;fields\u0026#34;: [ \u0026#34;description\u0026#34;, \u0026#34;description._2gram\u0026#34;, \u0026#34;description._3gram\u0026#34; ] } } } { \u0026#34;took\u0026#34; : 0, \u0026#34;timed_out\u0026#34; : false, \u0026#34;_shards\u0026#34; : { \u0026#34;total\u0026#34; : 1, \u0026#34;successful\u0026#34; : 1, \u0026#34;skipped\u0026#34; : 0, \u0026#34;failed\u0026#34; : 0 }, \u0026#34;hits\u0026#34; : { \u0026#34;total\u0026#34; : { \u0026#34;value\u0026#34; : 0, \u0026#34;relation\u0026#34; : \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34; : null, \u0026#34;hits\u0026#34; : [ ] } } The result is empty. Because there is no term created with the name jog if you closely look at the above generated tokens on respective fields.\nTo solve this we need to use the bool_prefix query.\nbool_prefix analyze the input and constructs the bool query from the terms. But it puts the last term in the prefix query. For example, input is given as men jogging s, So it will produce terms like [\u0026quot;men\u0026quot;,\u0026quot;jogging\u0026quot;,\u0026quot;s\u0026quot;] but it will always perform a prefix query on the last term which is \u0026quot;s\u0026quot;. So documents will return where terms will match with \u0026quot;men\u0026quot; or \u0026quot;jogging\u0026quot; or any term which is starting with \u0026quot;s\u0026quot;.\nBelow is the query which will give you the desired output.\nGET products/_search { \u0026#34;query\u0026#34;: { \u0026#34;multi_match\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;jog\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;bool_prefix\u0026#34;, \u0026#34;fields\u0026#34;: [ \u0026#34;description\u0026#34;, \u0026#34;description._2gram\u0026#34;, \u0026#34;description._3gram\u0026#34; ] } } } { \u0026#34;took\u0026#34; : 4, \u0026#34;timed_out\u0026#34; : false, \u0026#34;_shards\u0026#34; : { \u0026#34;total\u0026#34; : 1, \u0026#34;successful\u0026#34; : 1, \u0026#34;skipped\u0026#34; : 0, \u0026#34;failed\u0026#34; : 0 }, \u0026#34;hits\u0026#34; : { \u0026#34;total\u0026#34; : { \u0026#34;value\u0026#34; : 1, \u0026#34;relation\u0026#34; : \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34; : 1.0, \u0026#34;hits\u0026#34; : [ { \u0026#34;_index\u0026#34; : \u0026#34;products\u0026#34;, \u0026#34;_id\u0026#34; : \u0026#34;TSNNiX8BYh0NLleBiu4u\u0026#34;, \u0026#34;_score\u0026#34; : 1.0, \u0026#34;_source\u0026#34; : { \u0026#34;description\u0026#34; : \u0026#34;best jogging shoes for men\u0026#34; } } ] } } Here when we make a prefix query on the root field (description) or any subfields, It will rewrite the query as a term query on description._index_prefix field.\nThis matches more efficiently because prefixes up to 3 words are already created as the terms as shown in the above.\nNote: This query will search for terms irrespective of order. For example, if we search for jogging men, This will also give the result because it will search for both the terms jogging or men. In most of the cases this query (multi_match + bool_prefix) is recommended because the end user can search for any string like shoes or shoes for men or jogging shoes etc.\nWhat if you want to search with strict prefix order? You can use match_phrase_prefix, It will strictly match input from prefix in the same order only. So input like ‚Äúmen best‚Äù won‚Äôt return anything. Whereas you will get results with the previous one.\nGET products/_search { \u0026#34;query\u0026#34;: { \u0026#34;match_phrase_prefix\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;best jogging s\u0026#34; } } } It will return documents where the term\u0026rsquo;s prefix will be matched with ‚Äúbest jogging s‚Äù. Sometimes it can provide confusing results. You can check more about match_phrase_prefix.\nDemo Let‚Äôs take a practical experience of how it is going to work.\nSearch As You Type (Elasticsearch) Demo code and sample employees data to implement the \u0026ldquo;Search as you type\u0026rdquo; feature on elasticsearch.\nWritten the middleware API in python using flask. Used JQuery for javascript operations.\nInstallation Assuming you have successfully installed Elasticsearch and Kibana on your machine and it is working perfectly. Kindly refer respective installation document.\nOR\nYou can run Elasticsearch on the cloud with a few clicks.\nInstall Python3 \u0026amp; pip3 Refer Document to install python3 \u0026amp; pip3 on your system. Install flask pip3 install flask Install elasticsearch package pip3 install elasticsearch git Clone git clone https://github.com/ashishtiwari1993/search_as_you_type.git cd search_as_you_type Create Index and load data Make sure Elasticsearch and kibana are up and running fine on your machine.\nCreate Index PUT /sayt?pretty { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;first_name\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;search_as_you_type\u0026#34; }, \u0026#34;last_name\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;search_as_you_type\u0026#34; }, \u0026#34;street_address\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;search_as_you_type\u0026#34; }, \u0026#34;company\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;search_as_you_type\u0026#34; }, \u0026#34;email\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;search_as_you_type\u0026#34; } } } } Load sample data Sample data.json file is given which need to load with the help of bulk API.\ncurl -s -H \u0026#34;Content-Type: application/x-ndjson\u0026#34; -XPOST \u0026#34;localhost:9200/_bulk\u0026#34; --data-binary \u0026#34;@data.json\u0026#34; Do not forget to change the elasticsearch\u0026rsquo;s endpoint.\nRun api.py \u0026amp; test Open api.py and change elasticsearch endpoint accordingly.\nes = Elasticsearch(\u0026#34;http://localhost:9200\u0026#34;) Start API Server python3 api.py This will start the API service on port 5001.\nOpen index.html on your browser.\n","permalink":"https://ashish.one/blogs/search-as-you-type/","summary":"Introduction In this blog, we will try to understand how ‚ÄúSearch as you type‚Äù works and Quickly setup one demo using some sample data. You must have seen various websites like eCommerce, food apps, etc. where you just start typing \u0026amp; simultaneously relevant options start displaying as suggestions and autocomplete. We will try to achieve somewhat the same feature. Search as you type Elasticsearch gives this specific mapping type which you can simply set to a specific field where you want to perform this kind of search.","title":"[Part -1] Search as you type"},{"content":"Introduction In the talk, Quickly showed how you can use the metricbeat to check system health. With a few clicks, you can start monitoring your infra.\nAnother demo shows, How you can parse your custom unstructured logs to Elasticsearch. There is some utility already available for predefined logs format like json, apache, nginx and system logs etc.\nBut if you have different requirement where you need to parse a different types of log format , You can parse using Grok processor.\nAlso explained how you can read custom logs from log files in real time and ingest to Elasticsearch.\nSlides Talk Video Feel free to comment below, If you have any doubts or suggestion about this talk. ","permalink":"https://ashish.one/talks/devops-conf-2022/","summary":"Introduction In the talk, Quickly showed how you can use the metricbeat to check system health. With a few clicks, you can start monitoring your infra.\nAnother demo shows, How you can parse your custom unstructured logs to Elasticsearch. There is some utility already available for predefined logs format like json, apache, nginx and system logs etc.\nBut if you have different requirement where you need to parse a different types of log format , You can parse using Grok processor.","title":"Parse custom logs in Elasticsearch using grok_pattern"},{"content":"Introduction What this talk is all about ? The purpose of the talk is to give a short overview of Elastic solutions \u0026amp; Elastic stacks. In the demo shown, how you can deploy elasticsearch instance on Microsoft Azure.\nAlso, it gives an idea to use the elastic cloud to manage the elasticsearch instance which deployed on the Azure cloud. You can also create deployment on elastic cloud (cloud.elastic.co).\nIn the demo, Successfully shipped the metric data of the local system (my MacBook) to the newly deployed elasticsearch instance and explored the dashboard on kibana.\nSlides Talk Video Feel free to comment below, If you have any doubts or suggestion about this talk. ","permalink":"https://ashish.one/talks/deploy_elastic_on_azure/","summary":"Introduction What this talk is all about ? The purpose of the talk is to give a short overview of Elastic solutions \u0026amp; Elastic stacks. In the demo shown, how you can deploy elasticsearch instance on Microsoft Azure.\nAlso, it gives an idea to use the elastic cloud to manage the elasticsearch instance which deployed on the Azure cloud. You can also create deployment on elastic cloud (cloud.elastic.co).\nIn the demo, Successfully shipped the metric data of the local system (my MacBook) to the newly deployed elasticsearch instance and explored the dashboard on kibana.","title":"Deploy Elasicsearch on Azure cloud"},{"content":" Introduction This article is not containing the detailed procedure for installation but listed some challenges which you can face while installing the arch Linux. Arch Linux is one of the Linux distributions, Which gives you full control over your application and OS. You have complete freedom to install or what to keep. Unlike the other distribution like Ubuntu, Centos, etc. It doesn\u0026rsquo;t come with pre-loaded applications or software.\nThough Arch wiki has a detailed explanation of the installation process I faced some challenges whose solution was not easily available. I am listing a few here and will keep updating this post as I move forward.\nIn this post I am not going to cover the installation steps, You will get a better explanation on the arch wiki.\nMachine Configuration Hardware RAM: 2GB Disk: 500 BIOS configuration Secure boot: disabled\nLegacy boot: enabled\nBoot Mode: UEFI\nChallenges 1. What should be my disk partition ? 1.1 Check Boot mode You need to check installation guide, Refer step Verify the boot mode.\nIn a live environment, you need to check first what is your boot mode.\nls /sys/firmware/efi/efivars If the command shows the directory without error, then the system is booted in UEFI mode. If the directory does not exist, the system may be booted in BIOS (or CSM) mode.\nSo my machine\u0026rsquo;s boot mode is UEFI.\n1.2 How to create partitions? I would recommend using cfdisk command to create the partition. Which gives you good visualization.\nMy SCSI Disk is dev/sda.\n1.3 Types of Partition You need to create 3 types of partition as shown below:\nMount Point Partition Partition Type Size /mnt/boot /dev/sda1 EFI System 1G [SWAP] /dev/sda2 LINUX Swap 4G /mnt /dev/sda3 LINUX Filesystem 460G Here on my machine i have created 3 partition name dev/sda1, dev/sda2, dev/sda3. This name can be different.\nThis is how you need to create the partition with the help of the cfdisk command.\n1.4 Mount the partition Format the partition mkfs.ext4 /dev/sda3 mkswap /dev/sda2 mkfs.fat -F 32 /dev/sda1 Mount the partition mount /dev/sda3 /mnt mount /dev/sda1 /mnt/boot swapon /dev/sda2 2. Install Bootloader I have decided to install a grub bootloader on my machine. You can decide on this wiki page.\n2.1 Install GRUB pacman -S grub 2.2 Generate Grub config grub-install --target=x86_64-efi --efi-directory=/boot --bootloader-id=GRUB 2.3 Reboot the system exit umount /dev/sda3 reboot 3. How to login again with Live bootable Arch device (USB/CD etc.) Sometimes we need to install some package with a live environment only. In such a case, you can prefer these steps.\nAttach the live bootable device and choose the specific device in the boot order. Once you are logged in, Just perform the below commands.\nmount /dev/sda3 /mnt mount /dev/sda1 /mnt/boot swapon /dev/sda2 arch-chroot /mnt Once you are in Than you can perform any operation on your installed Arch OS.\n4. Install NetworkManager Please refer to challenge #3 to log in with the live environment. Once you successfully log in, Perform the below commands:\npacman -S NetworkManager systemctl start NetworkManager systemclt enable NetworkManager Simply check with ping if you connected with a LAN connection.\nping google.com Note: Please start and enable the Network Manager after reboot. It will not start on first time login.\n5. Install Broadcom Wireless pacman -S linux-headers pacman -S broadcom-wl-dkms Reboot the system.\n6. Desktop Environment Arch To run any desktop environment, Need to install Display Manager. First, we will install LXDM (display manager).\n6.1 LXDM pacman -S lxdm systemctl start lxdm systemctl enable lxdm 6.2 LXDE LXDE is a lightweight desktop. As my system has 2GB RAM, I have tried this.\npacman -S lxde Reboot the system.\n6.3 XFCE4 Xfce is a lightweight and modular desktop environment currently based on GTK 3. To provide a complete user experience, it includes a window manager, a file manager, desktop and panel.\npacman -S xfce4 xfce4-goodies Edit lxdm.conf\nEdit below line\nsession=/usr/bin/startlxfce4 Reboot the system.\n7. Connect to WIFI You need to install Network Manager and wireless driver first. You can refer step 4 \u0026amp; 5.\nOnce done with proper installation you can connect to wifi using:\nnmtui 8. Stuck on GRUB window while booting on arch linux It seems my bootloader was not configured properly. Re Attached the live bootable device and perform steps 3 and then 2.\nReboot the system.\n9. Error: [ TIME ] Timed out waiting for device /dev/disk/by-uuid/ Please perform step 3.\nCheck UUID of partitions\nlsblk -l Cross-check whether the above UUIDs and mentioned UUIDs in /etc/fstab should be the same.\nvim /etc/fstab Update UUID accordingly on the above file. Save and exit.\nReboot the system.\n","permalink":"https://ashish.one/blogs/arch-linux-installation-challenges/","summary":"Introduction This article is not containing the detailed procedure for installation but listed some challenges which you can face while installing the arch Linux. Arch Linux is one of the Linux distributions, Which gives you full control over your application and OS. You have complete freedom to install or what to keep. Unlike the other distribution like Ubuntu, Centos, etc. It doesn\u0026rsquo;t come with pre-loaded applications or software.\nThough Arch wiki has a detailed explanation of the installation process I faced some challenges whose solution was not easily available.","title":"Arch Linux Installation Challenges"},{"content":"I have always been in confusion about how to get started with security or pentest or somehow with a bug bounty. There are tons of resources available on the internet.\nThe Fact The fact is there is no hard and fast rule or there is no standard course by following which you will get the tag of a security expert.\nThere is no defined way to become a security researcher.\nHere at the end, I am also going to give you some resources about the get started but to be honest it will not help until you start with a practice or use it in your day-to-day life for learning purposes.\nSo what missing? Resources will get the directions for learning. Let‚Äôs say you got one blog where the blogger explains everything steps by step and You decided to complete it in 7 days. Your approach SHOULD NOT be that once you finish with the whole tutorial you should start with researching it won‚Äôt help or probably you must have forgotten the things which you learned on day 1.\nThe Approach Start performing practical from the first day. Whatever your learning start implementing those. Try to find the smallest loopholes. And trust me you won‚Äôt get it on Day 1. But with the practice, you will become to know how to find or where to find.\nWhere to do practice? You can use any Bugbounty side where lots of products, apps, websites get listed to find bugs, vulnerabilities, etc. You can also use CTF (Capture the flag). I will not go in deep with these terminologies because you can find better explanations on google. You will also get a better guide on how to start with these platforms.\nDon‚Äôt Forget your practice will give you a real security expert badge. Keep practicing.\nSome resources to get start https://github.com/nahamsec/Resources-for-Beginner-Bug-Bounty-Hunters\nhttps://whoami.securitybreached.org/2019/06/03/guide-getting-started-in-bug-bounty-hunting/\nhttps://infosecwriteups.com/guide-to-basic-recon-bug-bounties-recon-728c5242a115\nPentesting for n00bs is ELI5 hacking, designed for beginners who have never walked through a hack before. All 10 episodes:\nEp 1 - Legacy: https://youtu.be/JZN3JhoAdWo\nEp 2 - Lame: https://youtu.be/ntBkyid_u8Y\nEp 3 - Blue: https://youtu.be/xLI7OialKk4\nEp 4 - Devel: https://youtu.be/ODUDau7BPSY\nEp. 5 - Jerry: https://youtu.be/nF14K2VAVtw\nEp. 6 - Nibbles: https://youtu.be/8ulnQVFHcOE\nEp. 7 - Optimum: https://youtu.be/bTxnobhJ_b8\nEp. 8 - Bashed: https://youtu.be/5406MfOfXBc\nEp. 9 - Grandpa: https://youtu.be/3aASluoJ-iM\nEp. 10 - Netmon: https://youtu.be/8k-8aVwS0fk\nPlease feel free to post your doubt or suggestions below üôÇ\nThanks\n","permalink":"https://ashish.one/blogs/get-start-with-bugbounty-pentest-security-researcher/","summary":"I have always been in confusion about how to get started with security or pentest or somehow with a bug bounty. There are tons of resources available on the internet.\nThe Fact The fact is there is no hard and fast rule or there is no standard course by following which you will get the tag of a security expert.\nThere is no defined way to become a security researcher.","title":"Get start with BugBounty, Pentest and Security Researcher"},{"content":"In this gist, We will check how we can extract SPF records in Go.\nPrerequisite Go version $ go version go version go1.13 linux/amd64 Dependency DNS Library(https://github.com/miekg/dns)\nInstall dependency $ go get github.com/miekg/dns spfLookup.go Here you can change nameserver according to your requirement. I have specified here google\u0026rsquo;s name server (8.8.8.8). You can also use cloudflare\u0026rsquo;s nameserver (1.1.1.1)\nConclusion You can make any DNS query with miekg/dns library. In the above script, we have looked up TXT Records and then we have searched for a string containing v=spf1.\n","permalink":"https://ashish.one/blogs/spf-lookup-in-go/","summary":"In this gist, We will check how we can extract SPF records in Go.\nPrerequisite Go version $ go version go version go1.13 linux/amd64 Dependency DNS Library(https://github.com/miekg/dns)\nInstall dependency $ go get github.com/miekg/dns spfLookup.go Here you can change nameserver according to your requirement. I have specified here google\u0026rsquo;s name server (8.8.8.8). You can also use cloudflare\u0026rsquo;s nameserver (1.1.1.1)\nConclusion You can make any DNS query with miekg/dns library. In the above script, we have looked up TXT Records and then we have searched for a string containing v=spf1.","title":"SPF Lookup in Go"},{"content":" Goal of this blog In this blog, I am going to show you how easily we can write logs to the files in Golang. As well as we are going to store all logs on elasticsearch with EKB (Elasticsearch, Kibana, Beats).\nWhy ELKB stack ? Logs are very important for debugging, reporting, insights etc. In today\u0026rsquo;s tech world, We uses multiple cloud servers, private servers etc. Which consist of lots of different applications, scripts, programs, daemons, services and they generate their logs too. It is very difficult to go to each server and check all log files in case of debugging or to generate any insights or reporting.\nIn my case I used to go to every server and perform grep on log files.\nELKB Gives you an easy setup to move all yours logs into one central Place which is Elasticsearch \u0026amp; With a beautiful kibana interface, You can visualize all your logs. You can also make various types of dashboards.\nThis is a very small use case I am going to convert but you can find lots of big problem solving use cases on the internet about ELKB.\nI will not go much deeper about ELKB stack. You can find a wonderful explanation on elastic.co.\nTwo Parts: Write JSON logs to the file in Go Shift All JSON logs on Elasticsearch My Configuration OS: ubuntu 18.04 Go version: go version go1.13 linux/amd64 Elasticsearch version: 7.7.1 Filebeat version: filebeat version 7.7.1 (amd64) Kibana version: 7.7.1 1. Write JSON logs to the file in Go I am assuming you have already installed Go on your machine. If not then you can refer to the installation guide here.\nMy Golang version: go version go1.13 linux/amd64 Logrus This is a wonderful package available to write logs. Below is the short example of using logrus package:\n1. Install Logrus package go install github.com/sirupsen/logru 2. Create log.go and paste the below code package main import ( \u0026#34;os\u0026#34; log \u0026#34;github.com/sirupsen/logrus\u0026#34; ) func init() { f, _ := os.OpenFile(\u0026#34;/tmp/go_logs/example.log\u0026#34;, os.O_RDWR|os.O_CREATE|os.O_APPEND, 0666) log.SetFormatter(\u0026amp;log.JSONFormatter{}) log.SetOutput(f) } func main() { log.WithFields(log.Fields{ \u0026#34;event\u0026#34;: \u0026#34;create_profile\u0026#34;, \u0026#34;user_id\u0026#34;: 10, }).Info(\u0026#34;This is an info message.\u0026#34;) log.WithFields(log.Fields{ \u0026#34;event\u0026#34;: \u0026#34;delete_profile\u0026#34;, \u0026#34;user_id\u0026#34;: 11, }).Warn(\u0026#34;This is a warning message.\u0026#34;) log.WithFields(log.Fields{ \u0026#34;event\u0026#34; : \u0026#34;edit_profile\u0026#34;, \u0026#34;user_id\u0026#34;: 13, \u0026#34;package\u0026#34; : \u0026#34;main\u0026#34;, }).Fatal(\u0026#34;This is a critical message.\u0026#34;) } Here I have specified the log file /tmp/go_logs/example.log. You can specify according to your need. We also specified the log format JSON.\n3. Lets run the log.go $ go run log.go 4. Check log file: $ cat /tmp/go_logs/example.log Output\n{\u0026#34;event\u0026#34;:\u0026#34;create_profile\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;info\u0026#34;,\u0026#34;msg\u0026#34;:\u0026#34;This is an info message.\u0026#34;,\u0026#34;time\u0026#34;:\u0026#34;2020-06-06T22:51:30+05:30\u0026#34;,\u0026#34;user_id\u0026#34;:10} {\u0026#34;event\u0026#34;:\u0026#34;delete_profile\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;warning\u0026#34;,\u0026#34;msg\u0026#34;:\u0026#34;This is a warning message.\u0026#34;,\u0026#34;time\u0026#34;:\u0026#34;2020-06-06T22:51:30+05:30\u0026#34;,\u0026#34;user_id\u0026#34;:11} {\u0026#34;event\u0026#34;:\u0026#34;edit_profile\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;fatal\u0026#34;,\u0026#34;msg\u0026#34;:\u0026#34;This is a critical message.\u0026#34;,\u0026#34;package\u0026#34;:\u0026#34;main\u0026#34;,\u0026#34;time\u0026#34;:\u0026#34;2020-06-06T22:51:30+05:30\u0026#34;,\u0026#34;user_id\u0026#34;:13} Here I have used the log format JSON. Every log will be written in JSON format on the newline. You can check more features about logrus here.\n2. Shift All JSON logs on Elasticsearch This part has no dependency on the above part. You can use any JSON log file irrespective of any language.\nBefore start I am assuming you have installed Elasticsearch, Filebeat \u0026amp; Kibana on your machine. If not then refer below link:\nInstall Elasticsearch Install Filebeat Install Kibana Note: In this part we are not going to use Logstash.\n1. Start Elasticsearch Service $ service elasticsearch start It will run on port 9200. You can verify with the below command or just hit localhost:9200 on your browser.\nashish@ashish-laptop:~$ curl localhost:9200 { \u0026#34;name\u0026#34; : \u0026#34;753853fa62d1\u0026#34;, \u0026#34;cluster_name\u0026#34; : \u0026#34;docker-cluster\u0026#34;, \u0026#34;cluster_uuid\u0026#34; : \u0026#34;JLlH0Z0pQqWjGEHO8MQgZQ\u0026#34;, \u0026#34;version\u0026#34; : { \u0026#34;number\u0026#34; : \u0026#34;7.7.1\u0026#34;, \u0026#34;build_flavor\u0026#34; : \u0026#34;default\u0026#34;, \u0026#34;build_type\u0026#34; : \u0026#34;docker\u0026#34;, \u0026#34;build_hash\u0026#34; : \u0026#34;ad56dce891c901a492bb1ee393f12dfff473a423\u0026#34;, \u0026#34;build_date\u0026#34; : \u0026#34;2020-05-28T16:30:01.040088Z\u0026#34;, \u0026#34;build_snapshot\u0026#34; : false, \u0026#34;lucene_version\u0026#34; : \u0026#34;8.5.1\u0026#34;, \u0026#34;minimum_wire_compatibility_version\u0026#34; : \u0026#34;6.8.0\u0026#34;, \u0026#34;minimum_index_compatibility_version\u0026#34; : \u0026#34;6.0.0-beta1\u0026#34; }, \u0026#34;tagline\u0026#34; : \u0026#34;You Know, for Search\u0026#34; } 2. Start Kibana service $ service kibana start It will run on port 5601. You can verify by visiting localhost:5601 from your browser. You should see the kibana dashboard.\n3. Edit filebeat.yml Open filebeat.yml. Add below snippet in filebeat.inputs:\nfilebeat.inputs: - type: log enabled: true paths: - /tmp/go_logs/*.log json.add_error_key: true output.elasticsearch: # Array of hosts to connect to. hosts: [\u0026#34;localhost:9200\u0026#34;] You can check on below link to know more about filebeat log input \u0026amp; JSON decoding\nhttps://www.elastic.co/guide/en/beats/filebeat/current/filebeat-input-log.html\nhttps://www.elastic.co/guide/en/beats/filebeat/current/filebeat-input-log.html#filebeat-input-log-config-json\n4. Restart Filebeat $ service filebeat restart Verify If Data Indexed on Elasticsearch Check Logs $ tail -f /var/log/filebeat/filebeat Check Indices: $ curl localhost:9200/_cat/indices?v Or you can simply visit localhost:9200/_cat/indices?v on your browser.\nOutput:\nhealth status index uuid pri rep docs.count docs.deleted store.size pri.store.size green open .apm-custom-link xp0mitnBQtijaZ9tEgan_g 1 0 0 0 208b 208b green open .kibana_task_manager_1 7Q4mMTYxRhCB6sfnQ2ibmA 1 0 5 0 34kb 34kb green open .apm-agent-configuration 3piA79spTbGWAVItYL3PlQ 1 0 0 0 208b 208b yellow open filebeat-7.7.1-2020.06.06-000001 nsFk7mOuTguIfaPSbeM3PA 1 1 19 0 74.9kb 74.9kb green open .kibana_1 LBmzoJspR8a8HAcs9WGr8g 1 0 54 0 171.6kb 171.6kb As you can see a new index is created by filebeat with the name filebeat-7.7.1-2020.06.06-000001. In your case index name can be different but it will start with filebeat*.\nIndex lifecycle will be handled by filebeat. You don‚Äôt need to worry about that.\nCheck Documents $ curl localhost:9200/filebeat-7.7.1-2020.06.06-000001/_search?pretty Sample Output:\n{ \u0026#34;_index\u0026#34; : \u0026#34;filebeat-7.7.1-2020.06.06-000001\u0026#34;, \u0026#34;_type\u0026#34; : \u0026#34;_doc\u0026#34;, \u0026#34;_id\u0026#34; : \u0026#34;4qW3inIBVJJF9hMQm_yi\u0026#34;, \u0026#34;_score\u0026#34; : 1.0, \u0026#34;_source\u0026#34; : { \u0026#34;@timestamp\u0026#34; : \u0026#34;2020-06-06T17:39:47.208Z\u0026#34;, \u0026#34;log\u0026#34; : { \u0026#34;offset\u0026#34; : 0, \u0026#34;file\u0026#34; : { \u0026#34;path\u0026#34; : \u0026#34;/tmp/go_logs/ashish.log\u0026#34; } }, \u0026#34;json\u0026#34; : { \u0026#34;msg\u0026#34; : \u0026#34;This is an info message.\u0026#34;, \u0026#34;time\u0026#34; : \u0026#34;2020-06-06T23:09:26+05:30\u0026#34;, \u0026#34;user_id\u0026#34; : 10, \u0026#34;event\u0026#34; : \u0026#34;create_profile\u0026#34;, \u0026#34;level\u0026#34; : \u0026#34;info\u0026#34; }, \u0026#34;input\u0026#34; : { \u0026#34;type\u0026#34; : \u0026#34;log\u0026#34; }, \u0026#34;host\u0026#34; : { \u0026#34;name\u0026#34; : \u0026#34;ashish-laptop\u0026#34; }, \u0026#34;agent\u0026#34; : { \u0026#34;version\u0026#34; : \u0026#34;7.7.1\u0026#34;, \u0026#34;type\u0026#34; : \u0026#34;filebeat\u0026#34;, \u0026#34;ephemeral_id\u0026#34; : \u0026#34;3f53f9c1-66a0-4e93-85fa-1532221c9670\u0026#34;, \u0026#34;hostname\u0026#34; : \u0026#34;ashish-laptop\u0026#34;, \u0026#34;id\u0026#34; : \u0026#34;72976284-d927-49d1-abcb-1d2a5be15176\u0026#34; }, \u0026#34;ecs\u0026#34; : { \u0026#34;version\u0026#34; : \u0026#34;1.5.0\u0026#34; } } } Your data will be saved on the json key. Another is metadata which is added by filebeat. Filebeat provides lots of options, features \u0026amp; settings. You can use it according to your requirements.\nLogs on Kibana Visit on localhost:5601 from your browser. Create index pattern with the pattern filebeat*. Check here for more details.\nOnce you are done with defining the index pattern, Go to Discover section, Here you will see all your logs.\nYou can query on your logs. You can apply filters \u0026amp; create various types of dashboard to get better insights from your logs. You can find a wonderful explanation on the official site.\nIn the end We have successfully shipped our logs on Elasticsearch. This is only a small use case of ELKB Stack. It provides much more than this. You can explore more on the internet.\nPut your comments if you have any doubts.\n","permalink":"https://ashish.one/blogs/shipping-golang-logs-with-elkb-stack/","summary":"Goal of this blog In this blog, I am going to show you how easily we can write logs to the files in Golang. As well as we are going to store all logs on elasticsearch with EKB (Elasticsearch, Kibana, Beats).\nWhy ELKB stack ? Logs are very important for debugging, reporting, insights etc. In today\u0026rsquo;s tech world, We uses multiple cloud servers, private servers etc. Which consist of lots of different applications, scripts, programs, daemons, services and they generate their logs too.","title":"Shipping Golang logs with ELKB stack"},{"content":"Hi guys, In this series, we are going to setup LEMP Stack (Linux, Nginx, MySQL, PHP). Mainly it is used by web developers. I am assuming you have a basic idea about Docker \u0026amp; How it works.\nIn this blog, We are going to setup PHP and Nginx.\nWhy Docker? I will not go too much deep, You can find more resources over the internet about the docker.\nDocker makes the installation process very smooth and it gives your isolated environment as the container.\nWith the docker, There will be no more excuses like:\n‚ÄùIt\u0026rsquo;s working on my machine.‚Äù :D\nBecause your docker environment remains the same across the platforms, So above excuses will going to turn with:\n‚ÄùIt\u0026rsquo;s working on every machine‚Äù ;)\nIn Short, Docker makes tech person\u0026rsquo;s life easy like the developer, tester, etc.\nMy Machine configuration: Docker Version: 18.09.7 Docker Compose Version: 1.24.1 RAM: 8GB OS: Ubuntu 18.04 Make sure docker and docker-compose is installed on your machine.\nStep 1: Create folders: mkdir LEMP cd LEMP mkdir {public_html,nginx_conf} public_html: It will contains your PHP code.\nnginx_conf: It will contains your Nginx configuration file.\nStep 2: Create Nginx conf file Go to nginx_conf:\ncd nginx_conf Create file lemp-docker.conf (You can give any name according to your requirement) \u0026amp; Paste below configuration block.\nserver { listen 80; server_name _; root /public_html; location / { index index.php index.html; } location ~* \\.php$ { fastcgi_pass php:9000; fastcgi_index index.php; include fastcgi_params; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; fastcgi_param PATH_INFO $fastcgi_path_info; } } Save and Exit from folder.\nStep 3: Create file docker-compose.yml Create file docker-compose.yml in LEMP folder \u0026amp; paste the below lines:\nversion: \u0026#39;3\u0026#39; services: web: image: nginx ports: - \u0026#34;80:80\u0026#34; volumes: - ./public_html:/public_html - ./nginx_conf:/etc/nginx/conf.d - /tmp/nginx_logs:/var/log/nginx networks: - nginx-php php: image: php:7.2-fpm volumes: - ./public_html:/public_html networks: - nginx-php networks: nginx-php: Here we have created the network with the name nginx-php. You can check more about the networks here.\nYou can set volumes path according to your requirement.\nStep 4: Let\u0026rsquo;s Up the containers Create container:\ncd LEMP/ docker-compose up Note: Make sure no other service is running on port 80.\nOutput:\nYou can hit the localhost on your browser.\nOnce you hit the localhost check nginx logs:\ncat /tmp/nginx_logs/access.log 172.25.0.1 - - [16/May/2020:12:51:04 +0000] \u0026#34;GET / HTTP/1.1\u0026#34; 404 556 \u0026#34;-\u0026#34; \u0026#34;Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.163 Safari/537.36\u0026#34; \u0026#34;-\u0026#34; Now stop the containers by simply hitting Ctrl + c. Now run docker-compose in detached mode. It will run in the background.\ndocker-compose up -d Lists containers\ndocker-compose ps Step 5: Let\u0026rsquo;s Create test.php Go to LEMP/public_html\ncd LEMP/public_html Create file test.php \u0026amp; paste below code:\n\u0026lt;?php echo phpinfo(); Step 6: Run test.php Visit http://localhost/test.php.\nOutput:\nAt the End We have successfully created Nginx and PHP Environment. In Part 2, We will check, How we can add MySQL to this environment.\n","permalink":"https://ashish.one/blogs/part-1-setup-lemp-environment-with-docker-setup-nginx-and-php/","summary":"Hi guys, In this series, we are going to setup LEMP Stack (Linux, Nginx, MySQL, PHP). Mainly it is used by web developers. I am assuming you have a basic idea about Docker \u0026amp; How it works.\nIn this blog, We are going to setup PHP and Nginx.\nWhy Docker? I will not go too much deep, You can find more resources over the internet about the docker.\nDocker makes the installation process very smooth and it gives your isolated environment as the container.","title":"[Part 1] Setup LEMP environment with Docker - Setup Nginx and PHP"},{"content":"Steps to add Responsive google slides iframe with Hugo:\nHugo version:\n$ hugo version Hugo Static Site Generator v0.69.0-4205844B linux/amd64 BuildDate: 2020-04-10T09:12:34Z Step 1: Create Shortcode Create gslides.html file vim layouts/shortcodes/gslides.html Step 2: Add below code: \u0026lt;div id=\u0026#34;Container\u0026#34; style=\u0026#34;padding-bottom:56.25%; position:relative; display:block; width: 100%\u0026#34;\u0026gt; \u0026lt;iframe id=\u0026#34;googleSlideIframe\u0026#34; width=\u0026#34;100%\u0026#34; height=\u0026#34;100%\u0026#34; src=\u0026#34;{{ .Get \u0026#34;src\u0026#34; }}\u0026#34; frameborder=\u0026#34;0\u0026#34; allowfullscreen=\u0026#34;\u0026#34; style=\u0026#34;position:absolute; top:0; left: 0\u0026#34;\u0026gt;\u0026lt;/iframe\u0026gt; \u0026lt;/div\u0026gt; Step 3: Use shortcode \u0026lsquo;gslides\u0026rsquo; in your Blog/Post Markdown file Simply place below snippet in your markdown file.\nReplace src value with your Google slide URL.\n{{\u0026lt; gslides src=\"https://docs.google.com/presentation/d/e/2PACX-1vQExSl-gRPoA9hC6qXuqrjwiQVHAanDieZN_5GpV2Lw9cuxjsVFEN_wkTThqpQwZ36vJz4zwmTvV7cC/embed?start=false\u0026loop=false\u0026delayms=3000\" \u003e}} I have shown for google slides, Similarly, you can create or add any other document type like google sheets, doc, etc. OR you can place any iframe like this.\n","permalink":"https://ashish.one/blogs/add-responsive-google-slides-on-hugo/","summary":"Steps to add Responsive google slides iframe with Hugo:\nHugo version:\n$ hugo version Hugo Static Site Generator v0.69.0-4205844B linux/amd64 BuildDate: 2020-04-10T09:12:34Z Step 1: Create Shortcode Create gslides.html file vim layouts/shortcodes/gslides.html Step 2: Add below code: \u0026lt;div id=\u0026#34;Container\u0026#34; style=\u0026#34;padding-bottom:56.25%; position:relative; display:block; width: 100%\u0026#34;\u0026gt; \u0026lt;iframe id=\u0026#34;googleSlideIframe\u0026#34; width=\u0026#34;100%\u0026#34; height=\u0026#34;100%\u0026#34; src=\u0026#34;{{ .Get \u0026#34;src\u0026#34; }}\u0026#34; frameborder=\u0026#34;0\u0026#34; allowfullscreen=\u0026#34;\u0026#34; style=\u0026#34;position:absolute; top:0; left: 0\u0026#34;\u0026gt;\u0026lt;/iframe\u0026gt; \u0026lt;/div\u0026gt; Step 3: Use shortcode \u0026lsquo;gslides\u0026rsquo; in your Blog/Post Markdown file Simply place below snippet in your markdown file.","title":"Add Responsive Google Slides on Hugo"},{"content":" Successfully built custom #Linux system by referring https://t.co/lyrdKkfXo9 .\nListed my challenges here https://t.co/gU2SoAxly2@nixcraft#linux #lfs #linuxfromscratch pic.twitter.com/XlWchVHy26\n\u0026mdash; Ashish Tiwari üáÆüá≥ (@_ashish_tiwari) April 27, 2020 I have started with www.linuxfromscratch.org, I am facing some challenges and problems which I am going to share with you in this blog. I will keep updating this blog as well as I move forward.\nBefore we get start You need some basic linux command knowledge to use linuxfromscratch guide. I am going to build LFS on my local machine. If your machine has a different OS or different configuration, Then some solution will not work.\nI am going to use the same chapter title as mentioned in linuxfromscratch. I will skip those sections where I haven\u0026rsquo;t faced any difficulties.\nMy Machine Configuration OS: Ubuntu 14.04 RAM: 2GB Disk: 500 GB Going to build LFS version: 9.1 Published: March 1st, 2020 LFS doc:http://www.linuxfromscratch.org/lfs/downloads/9.1/LFS-BOOK-9.1-NOCHUNKS.html Challenges 2.2. Host System Requirements Doc link: http://www.linuxfromscratch.org/lfs/downloads/9.1/LFS-BOOK-9.1-NOCHUNKS.html#ch-partitioning-hostreqs\nVersion check cat \u0026gt; version-check.sh \u0026lt;\u0026lt; \u0026#34;EOF\u0026#34; #!/bin/bash # Simple script to list version numbers of critical development tools export LC_ALL=C bash --version | head -n1 | cut -d\u0026#34; \u0026#34; -f2-4 MYSH=$(readlink -f /bin/sh) echo \u0026#34;/bin/sh -\u0026gt; $MYSH\u0026#34; echo $MYSH | grep -q bash || echo \u0026#34;ERROR: /bin/sh does not point to bash\u0026#34; unset MYSH echo -n \u0026#34;Binutils: \u0026#34;; ld --version | head -n1 | cut -d\u0026#34; \u0026#34; -f3- bison --version | head -n1 if [ -h /usr/bin/yacc ]; then echo \u0026#34;/usr/bin/yacc -\u0026gt; `readlink -f /usr/bin/yacc`\u0026#34;; elif [ -x /usr/bin/yacc ]; then echo yacc is `/usr/bin/yacc --version | head -n1` else echo \u0026#34;yacc not found\u0026#34; fi bzip2 --version 2\u0026gt;\u0026amp;1 \u0026lt; /dev/null | head -n1 | cut -d\u0026#34; \u0026#34; -f1,6- echo -n \u0026#34;Coreutils: \u0026#34;; chown --version | head -n1 | cut -d\u0026#34;)\u0026#34; -f2 diff --version | head -n1 find --version | head -n1 gawk --version | head -n1 if [ -h /usr/bin/awk ]; then echo \u0026#34;/usr/bin/awk -\u0026gt; `readlink -f /usr/bin/awk`\u0026#34;; elif [ -x /usr/bin/awk ]; then echo awk is `/usr/bin/awk --version | head -n1` else echo \u0026#34;awk not found\u0026#34; fi gcc --version | head -n1 g++ --version | head -n1 ldd --version | head -n1 | cut -d\u0026#34; \u0026#34; -f2- # glibc version grep --version | head -n1 gzip --version | head -n1 cat /proc/version m4 --version | head -n1 make --version | head -n1 patch --version | head -n1 echo Perl `perl -V:version` python3 --version sed --version | head -n1 tar --version | head -n1 makeinfo --version | head -n1 # texinfo version xz --version | head -n1 echo \u0026#39;int main(){}\u0026#39; \u0026gt; dummy.c \u0026amp;\u0026amp; g++ -o dummy dummy.c if [ -x dummy ] then echo \u0026#34;g++ compilation OK\u0026#34;; else echo \u0026#34;g++ compilation failed\u0026#34;; fi rm -f dummy.c dummy EOF bash version-check.sh Error 1: /bin/sh does not point to bash\nsudo ln -sf bash /bin/sh Error 2: version-check.sh: line 11: bison: command not found\nsudo apt-get install bison Error 3: version-check.sh: line 25: gawk: command not found\nsudo apt-get install gawk Error 4: version-check.sh: line 36: g++: command not found\nsudo apt-get install g++ Error 5: version-check.sh: line 48: makeinfo: command not found\nsudo apt-get update -y sudo apt-get install -y texinfo Final success output: $ bash version-check.sh bash, version 4.3.11(1)-release /bin/sh -\u0026gt; /bin/bash Binutils: (GNU Binutils for Ubuntu) 2.24 bison (GNU Bison) 3.0.2 /usr/bin/yacc -\u0026gt; /usr/bin/bison.yacc bzip2, Version 1.0.6, 6-Sept-2010. Coreutils: 8.21 diff (GNU diffutils) 3.3 find (GNU findutils) 4.4.2 GNU Awk 4.0.1 /usr/bin/awk -\u0026gt; /usr/bin/gawk gcc (Ubuntu 4.8.4-2ubuntu1~14.04.4) 4.8.4 g++ (Ubuntu 4.8.4-2ubuntu1~14.04.4) 4.8.4 (Ubuntu EGLIBC 2.19-0ubuntu6.15) 2.19 grep (GNU grep) 2.16 gzip 1.6 Linux version 4.4.0-142-generic (buildd@lcy01-amd64-006) (gcc version 4.8.4 (Ubuntu 4.8.4-2ubuntu1~14.04.4) ) #168~14.04.1-Ubuntu SMP Sat Jan 19 11:26:28 UTC 2019 m4 (GNU M4) 1.4.17 GNU Make 3.81 GNU patch 2.7.1 Perl version=\u0026#39;5.18.2\u0026#39;; Python 3.4.3 sed (GNU sed) 4.2.2 tar (GNU tar) 1.27.1 makeinfo (GNU texinfo) 5.2 xz (XZ Utils) 5.1.0alpha g++ compilation OK 2.5. Creating a File System on the Partition Doc link: http://www.linuxfromscratch.org/lfs/downloads/9.1/LFS-BOOK-9.1-NOCHUNKS.html#ch-partitioning-creatingfilesystem\nI had only one partition. I need to create new physical partition for LFS.\nIt is recommended to use Physical partition only not logical.\nI have reinstall my OS with new partition which is long way. You can use GParted to make partitions. Remember you can create only 4 Physical partition. Give 1 physical partition dedicated to LFS. Below is my partition view.\n$ lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT sda 8:0 0 465.8G 0 disk |-sda1 8:1 0 476M 0 part /boot/efi |-sda2 8:2 0 1.9G 0 part [SWAP] |-sda3 8:3 0 190.8G 0 part / `-sda4 8:4 0 272.7G 0 part /lfs Challenge: My Partition wipe out after restart my system After restart I again hit the same command lsblk. /lfs partition was not there.\nIn Ubuntu, You need to make entry in /etc/fstab in order to mount the partition on restart.\nYou have to find first volumeId or UUID to make entry in /etc/fstab. You can use blkid command to get all UUID:\n$ blkid /dev/sda1: UUID=\u0026#34;1FBD-F51A\u0026#34; TYPE=\u0026#34;vfat\u0026#34; /dev/sda2: UUID=\u0026#34;9f41c453-3b5c-4537-81b0-2fb33f19ded8\u0026#34; TYPE=\u0026#34;swap\u0026#34; /dev/sda3: UUID=\u0026#34;1a16afba-cabb-40c7-b704-1129b7004ffb\u0026#34; TYPE=\u0026#34;ext4\u0026#34; /dev/sda4: UUID=\u0026#34;b3b2eb8c-2a43-405d-97da-af8b46c743e2\u0026#34; TYPE=\u0026#34;ext4\u0026#34; Above my Partition is /dev/sda4 which is mounted on /lfs, So my UUID is b3b2eb8c-2a43-405d-97da-af8b46c743e2. Now I have added below line in /etc/fstab.\nUUID=b3b2eb8c-2a43-405d-97da-af8b46c743e2 /lfs ext4 rw,user,exec,errors=remount-ro 0 0 Here\nrw: Read, Write exec: Execute errors=remount-ro: It will mount as Read-Only, If you get any error on mounting. Make sure you have added exec, Otherwise you cannot execute any binaries from chapter 5.4. Binutils-2.34 - Pass 1\n5.4. Binutils-2.34 - Pass 1 Doc link: http://www.linuxfromscratch.org/lfs/downloads/9.1/LFS-BOOK-9.1-NOCHUNKS.html#ch-tools-binutils-pass1\nError 1: bash: ../configure: No such file or directory As I mentioned on above Section, Make sure you have added exec in your /etc/fstab entry OR Make sure your partition /lfs has Read, Write \u0026amp; Execute permission. Below /etc/fstab entry worked for me:\nUUID=b3b2eb8c-2a43-405d-97da-af8b46c743e2 /lfs ext4 rw,user,exec,errors=remount-ro 0 0 This setting can be different for different OS.\nError 2: I had no idea where to create build folder.\nBelow steps worked for me:\n$ cd $LFS/source $ tar xvf binutils-2.34.tar.xz $ cd binutils-2.34 $ mkdir -v build $ cd build $ ../configure --prefix=/tools \\ --with-sysroot=$LFS \\ --with-lib-path=/tools/lib \\ --target=$LFS_TGT \\ --disable-nls \\ --disable-werror 5.5. GCC-9.2.0 - Pass 1 cd $LFS/sources tar -xvf gcc-9.2.0.tar.xz cd gcc-9.2.0 After this you can continue with document flow.\ntar -xf ../mpfr-4.0.2.tar.xz mv -v mpfr-4.0.2 mpfr tar -xf ../gmp-6.2.0.tar.xz mv -v gmp-6.2.0 gmp tar -xf ../mpc-1.1.0.tar.gz mv -v mpc-1.1.0 mpc Error 1: On make command, I got below error:\nerror: \u0026lsquo;for\u0026rsquo; loop initial declarations are only allowed in C99 mode compute_powtab.c: In function \u0026#39;mpn_compute_powtab_mul\u0026#39;: compute_powtab.c:142:3: error: \u0026#39;for\u0026#39; loop initial declarations are only allowed in C99 mode for (long pi = start_idx; pi \u0026gt;= 0; pi--) ^ compute_powtab.c:142:3: note: use option -std=c99 or -std=gnu99 to compile your code compute_powtab.c: In function \u0026#39;mpn_compute_powtab_div\u0026#39;: compute_powtab.c:226:3: error: \u0026#39;for\u0026#39; loop initial declarations are only allowed in C99 mode for (long pi = n_pows - 1; pi \u0026gt;= 0; pi--) ^ compute_powtab.c:274:13: error: redefinition of \u0026#39;pi\u0026#39; for (long pi = n_pows; pi \u0026gt;= 0; pi--) ^ compute_powtab.c:226:13: note: previous definition of \u0026#39;pi\u0026#39; was here for (long pi = n_pows - 1; pi \u0026gt;= 0; pi--) ^ compute_powtab.c:274:3: error: \u0026#39;for\u0026#39; loop initial declarations are only allowed in C99 mode for (long pi = n_pows; pi \u0026gt;= 0; pi--) ^ compute_powtab.c: In function \u0026#39;powtab_decide\u0026#39;: compute_powtab.c:296:3: error: \u0026#39;for\u0026#39; loop initial declarations are only allowed in C99 mode for (size_t pn = (un + 1) \u0026gt;\u0026gt; 1; pn != 1; pn = (pn + 1) \u0026gt;\u0026gt; 1) ^ compute_powtab.c:304:10: error: redefinition of \u0026#39;pn\u0026#39; size_t pn = un - 1; ^ compute_powtab.c:296:15: note: previous definition of \u0026#39;pn\u0026#39; was here for (size_t pn = (un + 1) \u0026gt;\u0026gt; 1; pn != 1; pn = (pn + 1) \u0026gt;\u0026gt; 1) ^ compute_powtab.c:308:3: error: \u0026#39;for\u0026#39; loop initial declarations are only allowed in C99 mode for (long i = n_pows - 2; i \u0026gt;= 0; i--) As per (https://stackoverflow.com/a/29338269)[https://stackoverflow.com/a/29338269],\nThis happens because declaring variables inside a for loop wasn\u0026rsquo;t valid C until C99(which is the standard of C published in 1999).\nHow I sovled this? Open file:\nvim $LFS/sources/gcc-9.2.0/build/gmp/mpn/compute_powtab.c We will solve first error:\ncompute_powtab.c:226:3: error: \u0026#39;for\u0026#39; loop initial declarations are only allowed in C99 mode for (long pi = n_pows - 1; pi \u0026gt;= 0; pi--) Changes on line number 226:\nlong pi; for (pi = n_pows - 1; pi \u0026gt;= 0; pi--) { Just declare long pi; outside the loop. Similarly you can solve above all errors.\nOR\nYou can download my edited version of compute_powtab.c, Place in $LFS/sources/gcc-9.2.0/build/gmp/mpn/.\nHit\nmake Now it should works :)\n5.7. Glibc-2.31 On below command:\n../configure \\ --prefix=/tools \\ --host=$LFS_TGT \\ --build=$(../scripts/config.guess) \\ --enable-kernel=3.2 \\ Error: These critical programs are missing or too old: make configure: error: *** These critical programs are missing or too old: make *** Check the INSTALL file for required versions. I checked my current make version:\n$ make --version GNU Make 3.81 Copyright (C) 2006 Free Software Foundation, Inc. This is free software; see the source for copying conditions. There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. This program built for x86_64-pc-linux-gnu As per (https://stackoverflow.com/a/31915313)[https://stackoverflow.com/a/31915313]\nDue to a long-standing unresolved Debian bug report, GNU Make remained the age-old 3.81 in Debian for a very long time, and as a consequence, in Debian-based distributions such as Ubuntu and Mint.\nSo we need to upgrade our make version. Below steps worked for me.\ncd /tmp wget http://ftp.gnu.org/gnu/make/make-4.1.tar.gz tar xvf make-4.1.tar.gz cd make-4.1/ ./configure make sudo make install sudo mv /usr/local/bin/make /usr/bin/make Now again hit ../configure .. command, Now it should work :)\n6.7. Linux-5.5.3 API Headers Error (lfs chroot) root:/sources/linux-5.5.3# make mrproper /bin/sh: sed: command not found Makefile:653: arch//Makefile: No such file or directory make: *** No rule to make target \u0026#39;arch//Makefile\u0026#39;. Stop. Because of my silly mistake. I had not installed the sed package. I just reinstall the package.\n6.12. Bzip2-1.0.8 Error: collect2: error: ld returned 1 exit status make: *** [Makefile-libbz2_so:38: all] Error 1 Just hit make clean.\nAgain run make -f Makefile-libbz2_so.\n6.18. Binutils-2.34 $ make -k check make[5]: *** [Makefile:2565: check-DEJAGNU] Error 1 make[5]: Leaving directory \u0026#39;/sources/binutils-2.34/build_6.18/ld\u0026#39; make[4]: *** [Makefile:1902: check-am] Error 2 make[4]: Leaving directory \u0026#39;/sources/binutils-2.34/build_6.18/ld\u0026#39; make[3]: *** [Makefile:1771: check-recursive] Error 1 make[3]: Leaving directory \u0026#39;/sources/binutils-2.34/build_6.18/ld\u0026#39; make[2]: *** [Makefile:1904: check] Error 2 make[2]: Leaving directory \u0026#39;/sources/binutils-2.34/build_6.18/ld\u0026#39; make[1]: *** [Makefile:7568: check-ld] Error 2 make[1]: Leaving directory \u0026#39;/sources/binutils-2.34/build_6.18\u0026#39; make: *** [Makefile:2222: do-check] Error 2 As I research these are few common errors But as I have not found any solution So I jump on to the next step as suggested in various places.\nmake tooldir=/usr install\nIf you get any solution of above error, Feel free to comment below.\n6.25. GCC-9.2.0 Error 1: make: *** [Makefile:2295: do-check] Error 2 I ignored this and move to next steps.\nError 2: Got no output with below command: echo \u0026#39;int main(){}\u0026#39; \u0026gt; dummy.c cc dummy.c -v -Wl,--verbose \u0026amp;\u0026gt; dummy.log readelf -l a.out | grep \u0026#39;: /lib\u0026#39; Performed below steps as mentioned on this link https://stackoverflow.com/a/48289145:\ngcc -dumpspecs | sed -e \u0026#39;s@/tools@@g\u0026#39; \u0026gt; `dirname $(gcc --print-libgcc-file-name)`/specs Again perform above command:\necho \u0026#39;int main(){}\u0026#39; \u0026gt; dummy.c $ cc dummy.c -v -Wl,--verbose \u0026amp;\u0026gt; dummy.log readelf -l a.out | grep \u0026#39;: /lib\u0026#39; [Requesting program interpreter: /lib64/ld-linux-x86-64.so.2] 6.41. Perl-5.30.1 Error: erlio.c:(.text+0x91b): undefined reference to `pthread_getspecific\u0026#39; /usr/bin/ld: perlio.o:perlio.c:(.text+0x44e4): more undefined references to `pthread_getspecific\u0026#39; follow collect2: error: ld returned 1 exit status make: *** [makefile:364: lib/buildcustomize.pl] Error 1 Solution which suggested here https://www.linuxquestions.org/questions/linux-from-scratch-13/errors-with-6-40-perl-5-28-0-a-4175642138/ It worked for me.\nit\u0026rsquo;s due to not having deleted the extracted Perl-5.28.0 directory after Chapter 5. I deleted the folder. Again untar the file.\n8.4. Using GRUB to Set Up the Boot Process NOTE: I have not performed this section, Becuase I am going to use my host system\u0026rsquo;s (Ubuntu 14.04) GRUB bootloader. Just go to another terminal of host system and hit below command.\nsudo update-grub2 After this again go to old terminal(LFS) and proceed with next step (The End).\nPlease comment below if you facing any issue.\nThanks\n","permalink":"https://ashish.one/blogs/challenges-in-linuxfromscratch/","summary":"Successfully built custom #Linux system by referring https://t.co/lyrdKkfXo9 .\nListed my challenges here https://t.co/gU2SoAxly2@nixcraft#linux #lfs #linuxfromscratch pic.twitter.com/XlWchVHy26\n\u0026mdash; Ashish Tiwari üáÆüá≥ (@_ashish_tiwari) April 27, 2020 I have started with www.linuxfromscratch.org, I am facing some challenges and problems which I am going to share with you in this blog. I will keep updating this blog as well as I move forward.\nBefore we get start You need some basic linux command knowledge to use linuxfromscratch guide.","title":"Challenges in linuxfromscratch"},{"content":"As you know Prometheus already having UI (localhost:9090). But it is not enough to give you better visualization on one screen. For better visualization and a graphical representation, we are going to use Grafana.\nWhat is Grafana? As grafana.com says\n‚ÄùGrafana is the open-source analytics and monitoring solution for every database.‚Äù\nThis means Grafana is an independent tool for analytics and monitor which gives your various types of Graphs. It is not restricted to Prometheus DB only, You can use mostly any Databases like MySQL, Elasticsearch, etc. So you can visualize different data points from the different databases on one screen. This is the flexibility and power Grafana provides.\nGrafana works on TSDB(Time Series Database) or Your data should be save in time series manner. Check explaination here.\nIt has an alert system. You can configure an alert on Grafana itself for any Metric.\nWhy Grafana? Open-source of course freely Available It is constantly contributed by the community. It is stable and used by many good brands. Good community support and well documented. You do not need any big infrastructure to get started. Lots of pre-build Grafana dashboards already available and build by the community. So it will be a rare case where you have to build your dashboard. My local system configuration: 8GB RAM Ubuntu 18.04 LTS You can check here your system requirement according to your Operating System. Also, They explained different ways to install Grafana for different OS.\nSetup Grafana Installation Grafana has two types of Software:\nEnterprise Release Open-Sources Software(OSS) or Community Release We are going to install OSS Release.\nStep 1: Install apt-transport-https $ sudo apt-get install -y apt-transport-https Step 2: Install wget $ sudo apt-get install -y software-properties-common wget Step 3: Add key $ wget -q -O - https://packages.grafana.com/gpg.key | sudo apt-key add - Step 4: Add apt Repository $ sudo add-apt-repository \u0026#34;deb https://packages.grafana.com/oss/deb stable main\u0026#34; OK Step 5: Update \u0026amp; Install $ sudo apt-get update $ sudo apt-get install grafana Step 6: Start grafana-server $ sudo systemctl daemon-reload $ sudo systemctl start grafana-server $ sudo systemctl status grafana-server Step 7: Visit localhost:3000 Install with Docker Ignore this part if you not using docker.\ndocker run -d -p 3000:3000 grafana/grafana visit localhost:3000.\nWe have successfully installed Grafana. Let\u0026rsquo;s configure with Prometheus.\nConfigure with Prometheus We already setup Prometheus in part 1. Now I am considering your Prometheus and Node exporter are running on port 9090 \u0026amp; 9100 respectively.\nSetup Grafana \u0026lt;\u0026ndash;\u0026gt; Prometheus Step 1: Login on Grafana Visit localhost:3000. 3000 is the default port of Grafana, However, you can run on any port. To change the port see the configuration.\nLogin into Grafana with default Credentials:\nUsername:admin\nPassword:admin\nNow it will ask you to change your password. Once done, Click on save, You will be redirect on the dashboard.\nStep 2: Add Data Source Goto Sidebar \u0026amp; Navigate:\nConfiguration -\u0026gt; Data Sources\nclick on Add data source. Search for Prometheus \u0026amp; click on Select.\nAdd Prometheus endpoint in URL and click on save \u0026amp; Test.\nYou will get the notification for success. Here we have successfully integrated Grafana and Prometheus.\nStep 3: Add Dashboard Here we have two option available:\nCreate your Dashboard Import any pre-build Dashboard We will go with the second option because that is the beauty of community :) People already build a dashboard for various stacks.\nYou can search for Dashboard from https://grafana.com/grafana/dashboards as per your requirement.\nHere is the link for Node exporter dashboard: https://grafana.com/grafana/dashboards/1860\nHere Dashboard ID is 1860. Every dashboard which contributed in Grafana has unique ID. You can just import any dashboard by inserting the ID.\nOn Grafana Goto sidebar \u0026amp; Navigate:\n+(Plus/Add sign) -\u0026gt; Import\nEnter Dashboard Id and click on load. After that\nEnter the Name \u0026amp; select Prometheus on label Prometheus as shown in below image. Click on Import.\nYour Dashboard will be ready.\nHere we have successfully integrated the Grafana with Prometheus. You can explore more features of Grafana like Alerts, Users, etc.\nFor the alerting system, I prefer Prometheus Alertmanager only. You can check more details on the Alertmanager in part 2. It gives you more control and advanced feature.\nBut every tool has use cases and built for special purposes. You can choose according to your requirements.\nIn case of any confusion or issues leave comments below :)\n","permalink":"https://ashish.one/blogs/setup-grafana-with-prometheus/","summary":"As you know Prometheus already having UI (localhost:9090). But it is not enough to give you better visualization on one screen. For better visualization and a graphical representation, we are going to use Grafana.\nWhat is Grafana? As grafana.com says\n‚ÄùGrafana is the open-source analytics and monitoring solution for every database.‚Äù\nThis means Grafana is an independent tool for analytics and monitor which gives your various types of Graphs. It is not restricted to Prometheus DB only, You can use mostly any Databases like MySQL, Elasticsearch, etc.","title":"[Part 4] Setup Grafana With Prometheus"},{"content":"Prerequisite OS: Ubuntu 14.04 LTS\nProcessor: 64 Bit\nRAM: 2 GB\n1. Install python3.6 From source Step 1.1: Compile $ wget https://www.python.org/ftp/python/3.6.3/Python-3.6.3.tgz $ tar -xvf Python-3.6.3.tgz $ cd Python-3.6.3 $ sudo ./configure --enable-optimizations $ sudo make install Step 1.2: Check $ python3.6 --version 2. Install pip3.6 Step 2.1: Download pip $ wget https://bootstrap.pypa.io/get-pip.py Step 2.2: Execute $ sudo python3.6 get-pip.py Step 2.3: If you Got below error Error zlib not available Traceback (most recent call last): File \u0026#34;get-pip.py\u0026#34;, line 22711, in \u0026lt;module\u0026gt; main() File \u0026#34;get-pip.py\u0026#34;, line 198, in main bootstrap(tmpdir=tmpdir) File \u0026#34;get-pip.py\u0026#34;, line 82, in bootstrap from pip._internal.cli.main import main as pip_entry_point zipimport.ZipImportError: can\u0026#39;t decompress data; zlib not available Step 2.3.1: Install zlib: Install with some other dependency $ sudo apt-get install -y make build-essential libssl-dev zlib1g-dev libbz2-dev libreadline-dev libsqlite3-dev wget curl llvm libncurses5-dev xz-utils tk-dev libxml2-dev libxmlsec1-dev OR\nYou can install just zlib1g-dev $ sudo apt-get install zlib1g-dev Chances to get error $ zlib1g-dev is already the newest version. Just upgrade it if not new $ sudo apt-get upgrade zlib1g-dev Step 2.3.2: Now again try $ sudo python3.6 get-pip.py If you get same error again, Then go for below steps: Open Modules/Setup from folder \u0026lsquo;Python-3.6.3\u0026rsquo; which we extracted on Step 1.1\n$ sudo vim Modules/Setup Uncomment the below line or jump on line no. 366 zlib zlibmodule.c -I$(prefix)/include -L$(exec_prefix)/lib -lz Now we need to again perform few operation from steps 1.1 $ sudo ./configure $ sudo make install Now run again \u0026amp; It should be work $ sudo python3.6 get-pip.py Step 2.4: Let\u0026rsquo;s Install pipenv for testing purpose: $ sudo pip3.6 install pipenv ","permalink":"https://ashish.one/blogs/install-python3.6-pip3.6-pipenv-on-ubuntu14.04/","summary":"Prerequisite OS: Ubuntu 14.04 LTS\nProcessor: 64 Bit\nRAM: 2 GB\n1. Install python3.6 From source Step 1.1: Compile $ wget https://www.python.org/ftp/python/3.6.3/Python-3.6.3.tgz $ tar -xvf Python-3.6.3.tgz $ cd Python-3.6.3 $ sudo ./configure --enable-optimizations $ sudo make install Step 1.2: Check $ python3.6 --version 2. Install pip3.6 Step 2.1: Download pip $ wget https://bootstrap.pypa.io/get-pip.py Step 2.2: Execute $ sudo python3.6 get-pip.py Step 2.3: If you Got below error Error zlib not available Traceback (most recent call last): File \u0026#34;get-pip.","title":"Install python3.6, pip3.6, pipenv on  Ubuntu 14.04 LTS"},{"content":"Hi Guys, I am trying to listing all data source \u0026amp; endpoints for COVID19 - India as well as Global. It can contains offical or unofficial APIs. Anyone is working on any COVID19 project for India, Can use these sources.\nAPIs 1. Github: amodm/api-covid19-in (India) Repo: https://github.com/amodm/api-covid19-in APIs available: Statewise Data StateWise StateWise History Medica Hospital Stats Bed Stats Contacts HelpLines Contacts Patient Tracing History Sources The source is both types of official \u0026amp; Unofficial.\nOfficial Post Mar 15, data is from The Ministry of Health \u0026amp; Family Welfare Pre Mar 15, data is sourced from datameet/covid19 Hospital \u0026amp; bed data: https://api.steinhq.com/v1/storages/5e732accb88d3d04ae0815ae/StateWiseHealthCapacity ICMR testing stats API: https://api.steinhq.com/v1/storages/5e6e3e9fb88d3d04ae08158c/ICMRTestData Unofficial The awesome volunteer driven patient tracing data covid19india.org API (NLP): http://coronatravelhistory.pythonanywhere.com/ API (Travel history): https://api.covid19india.org/travel_history.json 2. Github: ashishtiwari1993/india_mohfw.gov.in_scrape_covid19_statewise_status (India) Scraping https://mohfw.gov.in to extract the statewise data.\nRepo: https://github.com/ashishtiwari1993/india_mohfw.gov.in_scrape_covid19_statewise_status APIs available: Statewise status Total confirmed case Counts Deaths Sources Scraping from offical site of The Ministry of Health and Family Welfare.\n3. Github: NovelCOVID/API (Global) Repo: https://github.com/NovelCOVID/API APIs available: It is global data source. You will get all information for any country.\nSources Worldometers 4. Github: CSSEGISandData/COVID-19 (Global) This is the data repository for the 2019 Novel Coronavirus Visual Dashboard operated by the Johns Hopkins University Center for Systems Science and Engineering (JHU CSSE). Also, Supported by ESRI Living Atlas Team and the Johns Hopkins University Applied Physics Lab (JHU APL).\nRepo: https://github.com/CSSEGISandData/COVID-19 Data Available CSVs are available in time series manner. Information is available about all country.\nSources World Health Organization (WHO): https://www.who.int/ DXY.cn. Pneumonia. 2020. http://3g.dxy.cn/newh5/view/pneumonia. BNO News: https://bnonews.com/index.php/2020/02/the-latest-coronavirus-cases/ National Health Commission of the People‚Äôs Republic of China (NHC): http://www.nhc.gov.cn/xcs/yqtb/list_gzbd.shtml China CDC (CCDC): http://weekly.chinacdc.cn/news/TrackingtheEpidemic.htm Hong Kong Department of Health: https://www.chp.gov.hk/en/features/102465.html Macau Government: https://www.ssm.gov.mo/portal/ Taiwan CDC: https://sites.google.com/cdc.gov.tw/2019ncov/taiwan?authuser=0 US CDC: https://www.cdc.gov/coronavirus/2019-ncov/index.html Government of Canada: https://www.canada.ca/en/public-health/services/diseases/coronavirus.html Australia Government Department of Health: https://www.health.gov.au/news/coronavirus-update-at-a-glance European Centre for Disease Prevention and Control (ECDC): https://www.ecdc.europa.eu/en/geographical-distribution-2019-ncov-cases Ministry of Health Singapore (MOH): https://www.moh.gov.sg/covid-19 Italy Ministry of Health: http://www.salute.gov.it/nuovocoronavirus 1Point3Arces: https://coronavirus.1point3acres.com/en WorldoMeters: https://www.worldometers.info/coronavirus/ ","permalink":"https://ashish.one/blogs/covid19-data-source-and-endpoints-india-global/","summary":"Hi Guys, I am trying to listing all data source \u0026amp; endpoints for COVID19 - India as well as Global. It can contains offical or unofficial APIs. Anyone is working on any COVID19 project for India, Can use these sources.\nAPIs 1. Github: amodm/api-covid19-in (India) Repo: https://github.com/amodm/api-covid19-in APIs available: Statewise Data StateWise StateWise History Medica Hospital Stats Bed Stats Contacts HelpLines Contacts Patient Tracing History Sources The source is both types of official \u0026amp; Unofficial.","title":"Covid19 Data Source for India \u0026 Global"},{"content":"What this talk about? I am working on Golang for the last 1 year from the published date. I have shared some basics of Golang.\nAlso, shared What are the pain points developers face when they migrate from any other language (Especially from web language like PHP) to Golang?\nI have explained the Webhook architecture of MimePost And how we sending 100k Request hourly( Though Benchmark proves we can scale up to 500k).\nShown some immature code which given me a better understanding of 100% CPU utilization and How I waste my major time to debug on silly things.\nShared one of our error and it\u0026rsquo;s solutions related to How you can avoid race conditions on \u0026ldquo;map\u0026rdquo; type variables.\nSlides Talk Video Demo - 1 This code will make 100% CPU utilization and forever() function not going to share any single CPU time with anotherGoroutine()\nDemo - 2 Reproduce Golang \u0026ldquo;fatal error: concurrent map writes\u0026rdquo; \u0026amp; Solution. To reproduce comment Mutex related all operation like line no. 12, 30, 32, 44, 46. Mutex is use to prevent race condition which generates this error.\nFind more details on this blog.\nFeel free to comment below, If you have any doubts or suggestion about this talk. ","permalink":"https://ashish.one/talks/golang-basics-and-send-100k-hourly-webhooks-with-golang-mimepost/","summary":"What this talk about? I am working on Golang for the last 1 year from the published date. I have shared some basics of Golang.\nAlso, shared What are the pain points developers face when they migrate from any other language (Especially from web language like PHP) to Golang?\nI have explained the Webhook architecture of MimePost And how we sending 100k Request hourly( Though Benchmark proves we can scale up to 500k).","title":"Golang basics \u0026 Handling 100k hourly webhooks with golang @MimePost"},{"content":" The Problem: Suddenly got below errors which killed my daemon:\nfatal error: concurrent map writes goroutine 646 [running]: runtime.throw(0x75fd38, 0x15) /usr/local/go/src/runtime/panic.go:774 +0x72 fp=0xc000315e60 sp=0xc000315e30 pc=0x42ecf2 runtime.mapdelete_fast64(0x6f0800, 0xc00008ad50, 0x2b3e) goroutine 1 [sleep]: runtime.goparkunlock(...) /usr/local/go/src/runtime/proc.go:310 time.Sleep(0x12a05f200) /usr/local/go/src/runtime/time.go:105 +0x157 webhook/worker.Manager() goroutine 6 [IO wait]: internal/poll.runtime_pollWait(0x7fc308de6f08, 0x72, 0x0) /usr/local/go/src/runtime/netpoll.go:184 +0x55 internal/poll.(*pollDesc).wait(0xc000110018, 0x72, 0x0, 0x0, 0x75b00b) /usr/local/go/src/internal/poll/fd_poll_runtime.go:87 +0x45 internal/poll.(*pollDesc).waitRead(...) /usr/local/go/src/internal/poll/fd_poll_runtime.go:92 internal/poll.(*FD).Accept(0xc000110000, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0) /usr/local/go/src/internal/poll/fd_unix.go:384 +0x1f8 net.(*netFD).accept(0xc000110000, 0xc000050d50, 0xc000046700, 0x7fc308e426d0) /usr/local/go/src/net/fd_unix.go:238 +0x42 net.(*TCPListener).accept(0xc000126000, 0xc000050d80, 0x40dd08, 0x30) /usr/local/go/src/net/tcpsock_posix.go:139 +0x32 net.(*TCPListener).Accept(0xc000126000, 0x72f560, 0xc0000f0180, 0x6f4f20, 0x9c00c0) /usr/local/go/src/net/tcpsock.go:261 +0x47 net/http.(*Server).Serve(0xc0000f4000, 0x7ccbe0, 0xc000126000, 0x0, 0x0) /usr/local/go/src/net/http/server.go:2896 +0x286 net/http.(*Server).ListenAndServe(0xc0000f4000, 0xc0000f4000, 0x8) /usr/local/go/src/net/http/server.go:2825 +0xb7 net/http.ListenAndServe(...) /usr/local/go/src/net/http/server.go:3080 webhook/handler.HandleRequest() Expected behaviour In starting for a few seconds it was working smoothly.\nActual behaviour After few seconds my service got kill with above mentioned error.\nCode Overview: Initialized one global variable with the type \u0026lsquo;map\u0026rsquo;. Where the key is int and value is channel.\nvar ActiveInstances = make(map[int](chan string)) Having two functions\ngo SetValue() go DeleteValue() In SetValue() ActiveInstances[id] = make(chan string, 5) In DeleteValue() delete(ActiveInstances, id) Both functions running in multiple goroutines.\nObservation: The error itself says concurrent map writes, By which I got the idea that something is wrong with my map variable ActiveInstances. Both functions in multiple goroutines are trying to access the same variable(ActiveInstances) at the same time. Which created race condition. After exploring a few blogs \u0026amp; documentation, I become to know that \u0026ldquo;Maps are not safe for concurrent use\u0026rdquo;.\nAs per golang doc\nMap access is unsafe only when updates are occurring. As long as all goroutines are only reading‚Äîlooking up elements in the map, including iterating through it using a for range loop‚Äîand not changing the map by assigning to elements or doing deletions, it is safe for them to access the map concurrently without synchronization.\nSolution: Here we need to access ActiveInstances synchronously. We want to make sure only one goroutine can access a variable at a time to avoid conflicts, This can be easily achieved by sync.Mutex. This concept is called mutual exclusion which provides methods Lock and Unlock.\nWe can define a block of code to be executed in mutual exclusion by surrounding it with a call to Lock and Unlock It is as simple as below:\nvar mutex = \u0026amp;sync.Mutex{} mutex.Lock() //my block of code mutex.Unlock() Code Modifications: var mutex = \u0026amp;sync.Mutex{} mutex.Lock() ActiveInstances[i_id] = make(chan string, 5) mutex.Unlock() mutex.Lock() delete(ActiveInstances, id) mutex.Unlock() This is how we successfully fix this problem.\nCode to Reproduce To reproduce, Comment Mutex related all operation like line no. 12, 30, 32, 44, 46. Mutex is use to prevent race condition which generates this error.\nReferences: https://blog.golang.org/go-maps-in-action\nhttps://golang.org/doc/faq#atomic_maps\nhttps://gobyexample.com/mutexes\n","permalink":"https://ashish.one/blogs/fatal-error-concurrent-map-writes/","summary":"The Problem: Suddenly got below errors which killed my daemon:\nfatal error: concurrent map writes goroutine 646 [running]: runtime.throw(0x75fd38, 0x15) /usr/local/go/src/runtime/panic.go:774 +0x72 fp=0xc000315e60 sp=0xc000315e30 pc=0x42ecf2 runtime.mapdelete_fast64(0x6f0800, 0xc00008ad50, 0x2b3e) goroutine 1 [sleep]: runtime.goparkunlock(...) /usr/local/go/src/runtime/proc.go:310 time.Sleep(0x12a05f200) /usr/local/go/src/runtime/time.go:105 +0x157 webhook/worker.Manager() goroutine 6 [IO wait]: internal/poll.runtime_pollWait(0x7fc308de6f08, 0x72, 0x0) /usr/local/go/src/runtime/netpoll.go:184 +0x55 internal/poll.(*pollDesc).wait(0xc000110018, 0x72, 0x0, 0x0, 0x75b00b) /usr/local/go/src/internal/poll/fd_poll_runtime.go:87 +0x45 internal/poll.(*pollDesc).waitRead(...) /usr/local/go/src/internal/poll/fd_poll_runtime.go:92 internal/poll.(*FD).Accept(0xc000110000, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0) /usr/local/go/src/internal/poll/fd_unix.go:384 +0x1f8 net.(*netFD).accept(0xc000110000, 0xc000050d50, 0xc000046700, 0x7fc308e426d0) /usr/local/go/src/net/fd_unix.","title":"[SOLVED] Golang fatal error: concurrent map writes"},{"content":"Introduction What this talk is all about? We recently moved from MySQL to Elasticsearch where we got a direct 10x - 15x boost in our performance.\nWe came up with unique use cases of heavy updates in Elasticsearch. That been challenging but yes currently Our Elaticsearch handling 200 million requests per day very efficiently. Our WRITE consist of the partial update, update with script conditions and of course simple indexing.\nOur READ request is 1 million/day which contains Scroll, simple search \u0026amp; Aggregations Query. We achieved to display our email logs in next to real-time. We also worked on Disk optimization by believing in the principle of \u0026ldquo;Know your query\u0026rdquo;. Currently having 6TB + of the cluster with 80 GB ingestion per day.\nSlides Talk Video Feel free to comment below, If you have any doubts or suggestion about this talk. ","permalink":"https://ashish.one/talks/scale-with-massive-updates-queries-in-elasticsearch/","summary":"Introduction What this talk is all about? We recently moved from MySQL to Elasticsearch where we got a direct 10x - 15x boost in our performance.\nWe came up with unique use cases of heavy updates in Elasticsearch. That been challenging but yes currently Our Elaticsearch handling 200 million requests per day very efficiently. Our WRITE consist of the partial update, update with script conditions and of course simple indexing.","title":"How to scale with massive update queries in Elasticsearch?"},{"content":"Below are some challenges \u0026amp; exceptions faced while setting up Elasticsearch. I just shared my experience and learning. Please correct me, If you guys feel somewhere i got wrong OR You can contribute if you have any experiences . Will keep update this gist.\nEvery use case having different solutions. You can try accordingly.\n","permalink":"https://ashish.one/blogs/elasticsearch-exceptions-and-challenges/","summary":"Below are some challenges \u0026amp; exceptions faced while setting up Elasticsearch. I just shared my experience and learning. Please correct me, If you guys feel somewhere i got wrong OR You can contribute if you have any experiences . Will keep update this gist.\nEvery use case having different solutions. You can try accordingly.","title":"Elasticsearch Exceptions \u0026 Challenges"},{"content":"The Requirement Need to install fresh wordpress with same version on wordpress droplet of digitalocean.\nThe Problem My setup (wordpress droplet) was suddenly stop working. I started debugging.\nDebug Checked apache2 and mysql service:\nservice mysql status service apache2 status Both services was active. Then I checked the apache2 processes with below command:\n$ ps -ef | grep apache2 | wc -l 151 Lots of apache child process has been forked. Which was not good.\nchecked apache\u0026rsquo;s error log (tail /var/log/apache2/error.log), But not found anything.\nI took decision to setup new wordpress of same version. I was little sure that some plugin was causing this problem But i don\u0026rsquo;t have so much time to go through all plugins.\nDroplet configuration OS : Ubuntu 18.04.3 LTS Memory : 1GB Disk : 25GB Cost : $5/Monthly Process to install WordPress Step 1: Source directory backup Take backup with cp command.\n$ cd $ mkdir ~/backup_wp $ cp -R /var/www/html ~/backup_wp/ Digitalocean installs the wordpress in /var/www/html/ path.\nStep 2: Database backup $ mysqldump -uroot -p wordpress \u0026gt; ~/backup_wp/wp.sql Here wordpress is the Database name.\nIf you don\u0026rsquo;t remember the password for MySQL, Get it from wp-config.php:\nvim /var/www/html/wp-config.php\nCheck for the line\ndefine( 'DB_PASSWORD', 'xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx' );\nStep 3: Check version of the existing WordPress vim /var/www/html/wp-includes/version.php\nLook for the line\n$wp_version = '5.2.4';\nVersion can be different.\nStep 4: Download WordPress zip file Go to Wordpress Releases page. Check for your version and download with below command:\nwget https://wordpress.org/wordpress-5.2.4.zip Unzip it:\nunzip wordpress-5.2.4.zip It will extract all files in wordpress folder.\nIf unzip is not already installed, Please install it with below command:\napt install unzip Step 5: Replace source of /html with /wordpress rm -rf /var/www/html mv wordpress /var/www/html Step 6: Change configuration Go to your website URL, It will show you below page:\nJust click on the Let's go! Button.\nPut you DB conf as shown in below image\nClick on the Submit Button.\nCreate wp-config.php file\nCreate manually (vim /var/www/html/wp-config.php) if it is not created automatically and click on the Run the installation Button.\nYou will see Already Installed page\nJust click on the Log In button. It will redirect you on the admin panel login page.\nNow you can log in into your admin panel.\nStep 7: Copy important files from backup cp -R ~/backup_wp/html/wp-content/plugins/ /var/www/html/wp-content/ cp -R ~/backup_wp/html/wp-content/themes/ /var/www/html/wp-content/ cp -R ~/backup_wp/html/wp-content/uploads/ /var/www/html/wp-content/ This will copy all existing plugins, themes \u0026amp; all media files. But be careful while copying it, Becuase if it is the cause of your bug it can bring your site down again.\nImportant Configuration\ncp ~/backup_wp/html/.htaccess /var/www/html/ Otherwise, your URL routing won\u0026rsquo;t work.\nStep 8: WordPress asking for FTP details If WordPress asking for FTP details on the each option like Adding themes, Installing plugins etc. Then add below line in your wp-config.php file.\nvim /var/www/html/wp-config.php\nAdd\ndefine(\u0026#39;FS_METHOD\u0026#39;,\u0026#39;direct\u0026#39;); Test complete website If everything went perfect, It will run smoothly. So we have successfully set up new Vanilla WordPress with exisiting database and without touching to apache configurations. If you feel any doubt or problem feel free to comment below and Also you can share your review in comments.\n","permalink":"https://ashish.one/blogs/how-to-reset-1-click-installed-wordpress-on-digitalocean/","summary":"The Requirement Need to install fresh wordpress with same version on wordpress droplet of digitalocean.\nThe Problem My setup (wordpress droplet) was suddenly stop working. I started debugging.\nDebug Checked apache2 and mysql service:\nservice mysql status service apache2 status Both services was active. Then I checked the apache2 processes with below command:\n$ ps -ef | grep apache2 | wc -l 151 Lots of apache child process has been forked.","title":"How to reset 1-Click Installed WordPress on DigitalOcean?"},{"content":"Introduction In PART-1 and PART-2, We have seen how prometheus works and how to setup Prometheus and exporters. We have readymade exporters available on the internet.\nBut sometime there is situation where you need to store your own custom metrics on prometheus. In such case you have to write your own exporters which will exporters the data into Prometheus.\nThere is two way to exporting the data on prometheus: 1. Exporting to a Pushgateway Here we metrics are getting the push to prometheus server. It not exposed over any URL or port. Internally it directly calls to Prometheus host and pushes the metrics to that.\n2. Exposing over HTTP (on specific port) In this option, Exporter will expose all data over HTTP in prometheus data format. Like below:\nIt exposes the data on a specific port e.g http://myurl:8000.\nBasic Example Let\u0026rsquo;s take a very basic example, Suppose there is one script (Daemon service) is running with the name myprocess.go. What do we do in real life to check if some script is running or not? We just hit ps command like below:\nps -ef | grep 'myprocess'\nNow we are going to do the same in our exporter. It will hit the same command and if script is running we will set value 1 else 0.\nWriting Exporter We are going to use the Second option (Exposing over HTTP).\nStep 1 We going to use python_client. Just install it with below pip command:\npip install prometheus_client\nStep 2 We need webserver gateway, For which we are going to use WSGI.\nStep 3 Whenever we are writing exporters, We need to take care of metric type storage. Prometheus offered four types of metrics: Counter, Gauge, Summary, and Histogram. You can explore more here. Decide your metric type before pushing data.\nStep 4 We are going to run a webserver on 8000 port and exposing the result on /metrics path. So to access metrics just hit http://myurl.com:8000/metrics.\nStep 5 Complet sample code from my gist:\nStep 6 Add config in prometheus.yml\n- job_name: service_up_exporter scrape_interval: 2m scrape_timeout: 2m metrics_path: \u0026#34;/metrics\u0026#34; static_configs: - targets: [\u0026#39;exporterurl.com:8000/metrics\u0026#39;] Step 7 Restart prometheus service:\nsudo service prometheus restart\nThis is just a very basic example. It is a simple script to understand the way of writing exporters. You can write your custom exporters according to your use case.\nHere we have successfully write exporter which will expose the metrics. To know more about pushgateway or HTTP expoorter visit here\nIn part - 4, I have explained how you can integrate Grafana with Prometheus.\nShare you comments below :)\n","permalink":"https://ashish.one/blogs/write-custom-exporters-prometheus/","summary":"Introduction In PART-1 and PART-2, We have seen how prometheus works and how to setup Prometheus and exporters. We have readymade exporters available on the internet.\nBut sometime there is situation where you need to store your own custom metrics on prometheus. In such case you have to write your own exporters which will exporters the data into Prometheus.\nThere is two way to exporting the data on prometheus: 1. Exporting to a Pushgateway Here we metrics are getting the push to prometheus server.","title":"[Part 3] How to write custom prometheus exporter?"},{"content":"Introduction In PART - 1, We have successfully setup Prometheus and exporters. In this part, we are going to setup alertmanager and will send our first alert.\nAlertmanager is software that is maintained by the prometheus and it is written in Go. It takes care of deduplicating, grouping, and routing them to the correct receiver integration such as email, PagerDuty, or OpsGenie. It also takes care of silencing and inhibition of alerts.\nLet\u0026rsquo;s setup alertmanage: )\nSetup Alertmanager Installation: Prerequisites will be the same as PART - 1 . We will download the precompiled binary of alertmanager. Although there is a docker image also available which you can use.\nStep 1: Download alertmanager $ wget https://github.com/prometheus/prometheus/releases/download/v2.13.1/prometheus-2.13.1.linux-amd64.tar.gz Step 2: Extract tar $ tar -xvzf alertmanager-0.19.0.linux-amd64.tar.gz Step 3: Folder structure $ mv alertmanager-0.19.0.linux-amd64 alertmanager $ cd alertmanager/ $ ll Folder contains below files:\nalertmanager: It is a binary file that is core Daemon of alertmanager. alertmanager.yml: This is config file for alertmanager service. amtool: It is another binary that can be used as a command line utility to manager or silence the alerts on alertmanager. Step 4: Run alertmanager Execute binary:\n./alertmanager Visit to localhost:9093 on your browser:\nYour alertmanager is up :) Like prometheus it is creating folder with the name data. Alertmanager starts storing data in /data folder.\nTo check alertmanager metrics just visit localhost:9093/metrics\nMy production execution command is\n~/alertmanager/alertmanager --config.file=~/alertmanager/alertmanager.yml --storage.path=/var/lib/alertmanager --web.external-url=http://myurl.com:9093 --storage.tsdb.path : Specify the path where you want to save prometheus data. --web.external-url : You can use this option if you want to bind your address with your URL. Step 5: Run alertmanager as service Create file vim /etc/systemd/system/alertmanager.service Paste Below code [Unit] Description=AlertManager Server Service Wants=network-online.target After=network-online.target [Service] User=root Type=Simple ExecStart=~/alertmanager/alertmanager --config.file=~/alertmanager/alertmanager.yml --storage.path=/var/lib/alertmanager --web.external-url=http://myurl.com:9093 [Install] WantedBy=multi-user.target Save and exit.\nIt won‚Äôt run because alertmanager.yml is not defined yet. alertmanager.yml file is defined below.\nReload the Systemctl Daemon: $ sudo systemctl daemon-reload To start alertmanager service $ sudo systemctl start alertmanager Setup Alerts Step 1: Add settings in prometheus.yml Open prometheus.yml file\n$ vim ~/prometheus/prometheus.yml Add below code\nscrape_configs: # The job name is added as a label `job=\u0026lt;job_name\u0026gt;` to any timeseries scraped from this config. - job_name: \u0026#39;prometheus\u0026#39; # metrics_path defaults to \u0026#39;/metrics\u0026#39; # scheme defaults to \u0026#39;http\u0026#39;. static_configs: - targets: [\u0026#39;localhost:9090\u0026#39;] # Add below block for node_exporter - job_name: node_exporter scrape_interval: 1m scrape_timeout: 1m metrics_path: \u0026#34;/metrics\u0026#34; static_configs: - targets: [\u0026#39;localhost:9100\u0026#39;] #Alertmanager settings rule_files: - \u0026#39;~/prometheus/alert.rules.yml\u0026#39; alerting: alertmanagers: - static_configs: - targets: - \u0026#39;myurl.com:9093\u0026#39; rule_files: These are the files that contain all kinds of rules. Prometheus has own syntax to define rules. We will see this below.\nalerting: This is an option where we have to define alertmanager configuration. In target, we have defined myurl.com:9093 this is the exact port where our alertmanager is running.\nStep 2: How prometheus service will work? In above scrape_config we have defined scrape configurations. In the above example, prometheus will scrape node_exporter job at every 1 minute (scrape_interval is 1m. it will scrape all information which is available on localhost:9100/metrics (It will scrape all targets which is defined in the targets array).\nIt will store the data in the Internal database.\nAt every scraping, it will keep evaluating alert rules which defined in alert.rules.yml. As soon as any alert rules get true it will send an event to alertmanager on myurl.com:9093\nOnce alertmanager received events, It will check by which channel alert needs to be trigger like via slack, email pagerduty, etc which defined in alertmanager.yml \u0026amp; it will trigger an alert via appropriate channel. Now let‚Äôs define two files :\nalert.rules.yml alertmanager.yml Step 3: Create alert.rules.yml Create ~/prometheus/alert.rules.yml file and paste below sample rule:\ngroups: - name: Disk-usage rules: - alert: \u0026#39;Low data disk space\u0026#39; expr: ceil(((node_filesystem_size_bytes{mountpoint!=\u0026#34;/boot\u0026#34;} - node_filesystem_free_bytes{mountpoint!=\u0026#34;/boot\u0026#34;}) / node_filesystem_size_bytes{mountpoint!=\u0026#34;/boot\u0026#34;} * 100)) \u0026gt; 95 labels: severity: \u0026#39;critical\u0026#39; annotations: title: \u0026#34;Disk Usage\u0026#34; description: \u0026#39;Partition : {{$labels.mountpoint}}\u0026#39; summary: \u0026#34;Disk usage is `{{humanize $value}}%`\u0026#34; host: \u0026#34;{{$labels.instance}}\u0026#34; - name: Memory-usage rules: - alert: \u0026#39;High memory usage\u0026#39; expr: ceil((((node_memory_MemTotal_bytes - node_memory_MemFree_bytes - node_memory_Buffers_bytes - node_memory_Cached_bytes) / node_memory_MemTotal_bytes) * 100)) \u0026gt; 80 labels: severity: \u0026#39;critical\u0026#39; annotations: title: \u0026#34;Memory Usage\u0026#34; description: \u0026#39;Memory usage threshold set to `80%`.\u0026#39; summary: \u0026#34;Memory usage is `{{humanize $value}}%`\u0026#34; host: \u0026#34;{{$labels.instance}}\u0026#34; Here we have defined two rules.\nIf memory utilization is exceeded than 80% then it will trigger an email. If any disk partition usage exceeded than 95% then it will trigger an email. You will get more insights on defining alerting rules here.\nNow you might be wondering what is node_filesystem_size_bytes or node_memory_MemTotal_bytes ?\nIf we recall part - 1 when we set up the node exporter and visited on localhost:9100/metrics, It has shown some metric. In that will get the above variable name. If you want to put any alert rules irrespective of any exporters, You have to make expr using these variables.\nYou can compile your alert rule file :\n$ promtool check rules alert.rules.yml promtool is binary which we got in prometheus folder when we extracted it.\nYou can explore some sample rules here.\nStep 4: Create alertmanager.yml Create ~/alertmanager/alertmanager.yml file and paste below code:\nglobal: slack_api_url: \u0026#34;https://hooks.slack.com/services/XXXXXXXXXXXXXXX\u0026#34; route: receiver: \u0026#34;default\u0026#34; routes: - match: severity: info receiver: slack - match: severity: critical receiver: email group_wait: 30s group_interval: 5m repeat_interval: 5m receivers: - name: default email_configs: - to: \u0026#39;to@youremail.com\u0026#39; from: \u0026#39;default-alerts@yourdomain.com\u0026#39; smarthost: \u0026#39;smtp.host.com:2525\u0026#39; auth_username: \u0026#34;smtpusername\u0026#34; auth_password: \u0026#34;smtppassword\u0026#34; html: \u0026#39;{{ template \u0026#34;email\u0026#34; .}}\u0026#39; - name: slack slack_configs: - send_resolved: true username: \u0026#39;{{ template \u0026#34;slack.default.username\u0026#34; . }}\u0026#39; color: \u0026#39;{{ if eq .Status \u0026#34;firing\u0026#34; }}good{{ else }}good{{ end }}\u0026#39; title: \u0026#39;{{ template \u0026#34;slack.default.title\u0026#34; . }}\u0026#39; title_link: \u0026#39;{{ template \u0026#34;slack.default.titlelink\u0026#34; . }}\u0026#39; pretext: \u0026#39;{{ .CommonAnnotations.summary }}\u0026#39; text: \u0026gt;- {{ range .Alerts }} *Alert:* {{ .Annotations.summary }} - `{{ .Labels.severity }}`üìä *Description:* {{ .Annotations.description }} *Details:* {{ range .Labels.SortedPairs }} ‚Ä¢ *{{ .Name }}:* `{{ .Value }}` {{ end }} {{ end }} - name: email email_configs: - to: \u0026#39;to@youremail.com\u0026#39; from: \u0026#39;default-alerts@yourdomain.com\u0026#39; smarthost: \u0026#39;smtp.host.com:2525\u0026#39; auth_username: \u0026#34;smtpusername\u0026#34; auth_password: \u0026#34;smtppassword\u0026#34; html: \u0026#39;{{ template \u0026#34;email\u0026#34; .}}\u0026#39; templates: - \u0026#39;~/prometheus/alert.tmpl\u0026#39; Here we have setup two channel email and slack for alert. if you see alertmanager.yml file, There are four main components I defined.\nglobal: Here we can define any global variable like we defined slack_api_url. route: It is a routing block. I am playing routing on severity. Similarly, it is your choice on which variable you want to route your alerts. So here if severity == ‚Äòinfo‚Äô, Alert will go from slack or if severity == ‚Äòcritical‚Äô, Alert will go via an email. receivers: Here we can define the channel by which alert will go. For now, I have defined only email \u0026amp; slack. You can explore more receivers here. templates: It is an alert template where I have defined the HTML template for an email alert. It is not restricted to email. You can define a template for any channel. Explore more details about templates here. Step 5: Sample template Create and Paste below code in ~/prometheus/alert.tmpl\n{{ define \u0026#34;email\u0026#34; }} \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;style type=\u0026#34;text/css\u0026#34;\u0026gt; table { font-family: verdana,arial,sans-serif; font-size:11px; color:#333333; border-width: 1px; border-color: #999999; border-collapse: collapse; } table th { background-color:#ff6961; border-width: 1px; padding: 8px; border-style: solid; border-color: #F54C44; } table td { border-width: 1px; padding: 8px; border-style: solid; border-color: #F54C44; text-align: right; } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;table border=1\u0026gt; \u0026lt;thead\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;th\u0026gt;Alert name\u0026lt;/th\u0026gt; \u0026lt;th\u0026gt;Host\u0026lt;/th\u0026gt; \u0026lt;th\u0026gt;Summary\u0026lt;/th\u0026gt; \u0026lt;th\u0026gt;Description\u0026lt;/th\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;/thead\u0026gt; \u0026lt;tbody\u0026gt; {{ range .Alerts }} \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;{{ .Labels.alertname }}\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;{{ .Annotations.host }}\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;{{ .Annotations.summary }}\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;{{ .Annotations.description }}\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; {{ end }} \u0026lt;/tbody\u0026gt; \u0026lt;/table\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; {{end}} Step 6: Start alertmanager service \u0026amp; restart prometheus $ sudo systemctl start alertmanager $ sudo systemctl restart prometheus Test complete Integration: If everything is set up correctly and alert rule getting true then it will trigger an alert. You can enable the log of alertmanager for debugging purposes.\nIf you want to test alert, Then simply make threshold to very less 0% or 1% after 30s it should trigger the alert.\nVisit localhost:9093. If there is some alert you will get the list on the dashboard.\nSo we successfully setup alert using alertmanager. In part - 3 I have explained how you can write your own custom exporters \u0026amp; You can check part - 4 to integrate Grafana with Prometheus.\n","permalink":"https://ashish.one/blogs/setup-alertmanager/","summary":"Introduction In PART - 1, We have successfully setup Prometheus and exporters. In this part, we are going to setup alertmanager and will send our first alert.\nAlertmanager is software that is maintained by the prometheus and it is written in Go. It takes care of deduplicating, grouping, and routing them to the correct receiver integration such as email, PagerDuty, or OpsGenie. It also takes care of silencing and inhibition of alerts.","title":"[Part 2] How to setup alertmanager and send alerts ?"},{"content":"As a developer, many times you would have worried, whether your services are up and running or not. Not only that, sometimes as an infrastructure guy you might be also worried about your server‚Äôs health too. What is the current RAM or disk utilization? or whether they are going to be fully occupied which in turn can completely bring the system down. These are just the basics and in fact there are tons of more such things which need to be monitored and fixed in everyday\u0026rsquo;s life.\n‚ÄúNever send a human to do a machine\u0026rsquo;s job‚Äù\nAt some time, deploying humans to check all these and that too for 24 * 7 can be a real pain.\nThat\u0026rsquo;s exactly the purpose of writing this tutorial series around monitoring and alerting. In this tutorial, you will learn how to setup Prometheus as a universal monitoring system and how to use its exporter to define and fetch the metrics which are really important to track.\nWhat is Prometheus? Prometheus is an open-source system for monitoring and alerting. It developed in the GO language. It is currently a standalone open source project and maintained independently by any organization. You can check more details here.\nWhy Prometheus? Open-source and of course freely available :) It is constantly contributed by the community. It is stable and used by many good brands. Check Stackshare. Good community support and well documented. You do not need any big infrastructure to get started. It can be started with 1 GB RAM. It has its own UI to check any metrics. But, many prefer Grafana with Prometheus which gives you better visualization on your Prometheus metrics. Lots of pre-build Grafana dashboard and exporters already written. You have to just reuse those exporters. You can check the list of Prometheus features here. You can check the list of Prometheus features here. Prerequisites for setting up Prometheus Here Is This Tutorial, Prometheus Is Installed And Tested On A Test Server With Minimum Configuration:\n2 GB RAM 10 GB Avg local disk storage. GOLANG (go1.11.5 linux/amd64) Centos7 RHEL Setup Prometheus Installation There are multiple ways to install Prometheus. You can use docker image or use any of the available configuration management systems like Ansible, chef, puppet and salt stack. For more information on installation, visit the official installation guide here.\nIt also has pre-compiled binaries available. I am going to use this binary for installation because it is easy to set up and easy to understand because of we already familiar with the utilization of binary files.\nStep 1: Download the Prometheus binary Visit Prometheus download page. It will give you a list of pre-compiled binaries for drawins, linux, and windows. You can download according to your OS. Below, the installation is explained for Linux OS.\nOR\nYou can simply fire below command in your Linux terminal:\n$ wget https://github.com/prometheus/prometheus/releases/download/v2.11.1/prometheus-2.11.1.linux-amd64.tar.gz Step 2: Extract The Tar $ tar -xvzf prometheus-2.11.1.linux-amd64.tar.gz Step 3: After Extraction Steps $ mv prometheus-2.11.1.linux-amd64 prometheus $ cd prometheus/$ ll Folder contains below file:\nprometheus: It\u0026rsquo;s a binary file which is the core daemon. prometheus.yml: This is the config file for Prometheus service. promtool: This is another binary file which is used to compile the alert rules file. This will be explained in detail in the next series to this tutorial. Step 4: Execute The Binary File Using The Below Command: $ ./prometheus Visit localhost:9090 on your web browser:\nYour Prometheus is up and running!\nIf you notice in prometheus/ folder, It created a folder with the name data. Prometheus starts storing metrics in this /data folder only.\nNow get all metric list by hitting the URL to localhost:9090/metrics\nPrometheus stores data on disk in time series, with its custom format. Behind the scenes, it uses leveldb. You can check more details on storage.\nHere is a sample production command:\n$ ~/prometheus/prometheus --storage.tsdb.path=/var/lib/prometheus/data/ --web.external-url=http://myurl.com:9090 --storage.tsdb.path: Specify the path where you want to save Prometheus data.\n--web.external-url: You can use this option if you want to bind your address with your URL.\nYou can get below error in case of your folder don‚Äôt have appropriate permission:\nlevel=error ts=2019-08-06T14:25:19.791Z caller=main.go:731 err=\u0026quot;opening storage failed: lock DB directory: open /var/lib/lock: permission denied\u0026quot;\nYou can try appending sudo to your command OR you can give appropriate permission to your folder.\nStep 5: Run Prometheus As Service. Create File $ vim /etc/systemd/system/prometheus.service Just paste below code: [Unit] Description=Prometheus Server Documentation=https://prometheus.io/docs/introduction/overview/ After=network-online.target [Service] User=root Restart=on-failure #Change this line if you download the #Prometheus on different path user ExecStart=~/prometheus/prometheus --storage.tsdb.path=/var/lib/prometheus/data/ --web.external-url=http://myurl.com:9090 [Install] WantedBy=multi-user.target Save and exit. Reload the Systemctl Daemon: $ sudo systemctl daemon-reload Start the Prometheus service: sudo systemctl start prometheus Till now you learned how to do basic Prometheus setup. Now, you will learn, how to set up Prometheus exporter.\nPrometheus Exporter Setup What Is Exporter? Exporters can be any scripts or services which will fetch specific metrics from your system and gives data in Prometheus format. There are primarily two ways by which you can fetch metrics and store into Prometheus:\nVia exporter, In which one service will run on a specific port. So whenever Prometheus service will hit exporter URL with the specific port it will give output in Prometheus format. We will see sample response in the below example during setting up the node exporter.\nThe second approach is you can write a script which will push data in time series to the Prometheus server. Any metric which cannot be scrape by the exporter, It can be pushed using the push method. You will get more info here on this.\nYou can use both methods but usually, people prefer the first one to fetch metrics.\nSo now we are going to setup node exporter. It will fetch your server metrics which will be RAM/DISK/CPU utilization, network, io etc.\nNode Exporter Setup Step 1: Download The Binary File And Start Node Exporter: $ wget https://github.com/prometheus/node_exporter/releases/download/v0.18.1/node_exporter-0.18.1.linux-amd64.tar.gz $ tar -xvzf node_exporter-0.18.1.linux-amd64.tar.gz $ mv node_exporter-0.18.1.linux-amd64 node_exporter $ cd node_exporter $ ./node_exporter You should see below output once the node exporter is started:\nJust visit to localhost:9100/metrics\nStep 2: Let\u0026rsquo;s Run Node Exporter As Service: Create a file in below path: /etc/systemd/system/node-exporter.service Just paste below code: [Unit] Description=Node exporter After=network-online.target [Service] User=root Restart=on-failure #Change this line if you download the #Prometheus on different path user ExecStart=~/node_exporter/node_exporter [Install] WantedBy=multi-user.target Reload the systemctl daemon: sudo systemctl daemon-reload Start the Prometheus service: sudo systemctl start node-exporter Step 3: prometheus.yml You Are Set With Node Exporter. Now In Prometheus, We Need To Configure This Node Exporter Reference So That Prometheus Can Collect Metrics From This Exporter.\nOpen file ~/prometheus/prometheus.yml add below configuration:\nscrape_configs: # The job name is added as a label `job=\u0026lt;job_name\u0026gt;` to any timeseries scraped from this config. - job_name: \u0026#39;prometheus\u0026#39; # metrics_path defaults to \u0026#39;/metrics\u0026#39; # scheme defaults to \u0026#39;http\u0026#39;. static_configs: - targets: [\u0026#39;localhost:9090\u0026#39;] # Add below block for node_exporter - job_name: node_exporter scrape_interval: 1m scrape_timeout: 1m metrics_path: \u0026#34;/metrics\u0026#34; static_configs: - targets: [\u0026#39;localhost:9100\u0026#39;] Save and exit.\njob_name: You can give any name to your scrape job.\nscrape_interval: Interval in which Prometheus will scrape the metrics from the specified URL.\nscrape_timeout: If your exporter has taken more than 1m to scrape the metrics it will be a timeout.\nmetric_path: This is what your endpoint‚Äôs path should be (i.e localhost:9100/metrics)\ntargets: Here you can specify the number of servers on which node exporter is running with the same configuration.\nStep 4: Here\u0026rsquo;s The Command To Execute Prometheus: ~/prometheus/prometheus --storage.tsdb.path=/var/lib/prometheus/data/ --config.file=~/prometheus/prometheus.yml --web.external-url=http://myurl.com:9090 Also, don‚Äôt forget to make the same changes in your service file:/etc/systemd/system/prometheus.service\nStep 5: Restart prometheus service sudo systemctl restart prometheus Step 6: Visiting Localhost:9090 Again Now visit the URL localhost:9090. In Expression field you can search for node_filesystem_size_bytes by clicking on the Execute button. You will get the below stats:\nAfter clicking on the graph:\nInstallation with Docker Prometheus docker run \\ -p 9090:9090 \\ -v ~/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml \\ prom/prometheus Node Exporter docker run -d \\ --net=\u0026#34;host\u0026#34; \\ --pid=\u0026#34;host\u0026#34; \\ -v \u0026#34;/:/host:ro,rslave\u0026#34; \\ quay.io/prometheus/node-exporter \\ --path.rootfs=/host The rest of option will remain same as i explained in above parts.\nAt the end Like this, you can explore each metrics like memory, CPU, etc.\nThere are already lots of exporters is available on the internet like Nginx exporter, MongoDB exporter, MySQL server exporter, etc. Just download them and start using it. You can check more info about exporter here.\nYou have now successfully installed Prometheus and node exporters.\nIf you see this official architecture diagram, we have successfully set up the Prometheus, Pushgateway/Exporters, and Prometheus UI.\nIn the Part 2, we will see how we can set up an alert manager and how to setup alert over the metrics.\nStay tuned for the new updates.\n‚ÄúNOTE: PROMETHEUS IS METRIC STORAGE SYSTEM. DON‚ÄôT TRY TO STORE ANY KIND OF LOGS. IT IS NOT RECOMMENDED.‚Äù\nIn case of any confusion or issues leave comments below :)\nArticle partially posted on Pepipost tutorial by me.\n","permalink":"https://ashish.one/blogs/setup-prometheus-and-exporters/","summary":"As a developer, many times you would have worried, whether your services are up and running or not. Not only that, sometimes as an infrastructure guy you might be also worried about your server‚Äôs health too. What is the current RAM or disk utilization? or whether they are going to be fully occupied which in turn can completely bring the system down. These are just the basics and in fact there are tons of more such things which need to be monitored and fixed in everyday\u0026rsquo;s life.","title":"[Part 1] How To Setup Prometheus And Exporters For Alerts And Monitoring?"},{"content":"I was working on elasticsearch and the requirement was to implement like query ‚Äú%text%‚Äù ( like mysql %like% ). We could use wildcard, regex or query string but those are slow. Hence i took decision to use ngram token filter for like query. It was quickly implemented on local and works exactly i want.\nThe problem To know the actual behavior, I implemented the same on staging server. I found some problem while we start indexing on staging.\nStorage size was directly increase by 8x, Which was too risky. In my previous index the string type was ‚Äúkeyword‚Äù. Its took approx 43 gb to store the same data. I implemented a new schema for ‚Äúlike query‚Äù with ngram filter which took below storage to store same data. curl -XGET http://localhost:9200/_cat/indices?v index docs.count pri.store.size ngram-test 459483245 329.5gb Sometime like query was not behaving properly. Not getting exact output. Schema curl -XPUT \u0026#34;localhost:9200/ngram-test?pretty\u0026#34; -H \u0026#39;Content-Type: application/json\u0026#39; -d\u0026#39; { \u0026#34;settings\u0026#34;:{ \u0026#34;index\u0026#34;:{ \u0026#34;number_of_shards\u0026#34;:5, \u0026#34;number_of_replicas\u0026#34;:0, \u0026#34;codec\u0026#34;: \u0026#34;best_compression\u0026#34; }, \u0026#34;analysis\u0026#34;:{ \u0026#34;filter\u0026#34;:{ \u0026#34;like_filter\u0026#34;:{ \u0026#34;type\u0026#34;:\u0026#34;ngram\u0026#34;, \u0026#34;min_gram\u0026#34;:3, \u0026#34;max_gram\u0026#34;:10, \u0026#34;token_chars\u0026#34;:[ \u0026#34;letter\u0026#34;, \u0026#34;digit\u0026#34;, \u0026#34;symbol\u0026#34; ] } }, \u0026#34;analyzer\u0026#34;:{ \u0026#34;like_analyzer\u0026#34;:{ \u0026#34;type\u0026#34;:\u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;:\u0026#34;keyword\u0026#34;, \u0026#34;filter\u0026#34;:[ \u0026#34;lowercase\u0026#34;, \u0026#34;like_filter\u0026#34; ] } } } }, \u0026#34;mappings\u0026#34;:{ \u0026#34;logs\u0026#34;:{ \u0026#34;properties\u0026#34;:{ \u0026#34;email\u0026#34;:{ \u0026#34;type\u0026#34;:\u0026#34;keyword\u0026#34;, \u0026#34;fields\u0026#34;:{ \u0026#34;text\u0026#34;:{ \u0026#34;analyzer\u0026#34;:\u0026#34;like_analyzer\u0026#34;, \u0026#34;search_analyzer\u0026#34;:\u0026#34;like_analyzer\u0026#34;, \u0026#34;type\u0026#34;:\u0026#34;text\u0026#34; } } } } } } }\u0026#39; Analyzing the behavior of ngram filter We made one test index and start monitoring by inserting doc one by one.\nContinue Reading\n","permalink":"https://ashish.one/blogs/min-gram-and-max-gram-elasticsearch/","summary":"I was working on elasticsearch and the requirement was to implement like query ‚Äú%text%‚Äù ( like mysql %like% ). We could use wildcard, regex or query string but those are slow. Hence i took decision to use ngram token filter for like query. It was quickly implemented on local and works exactly i want.\nThe problem To know the actual behavior, I implemented the same on staging server. I found some problem while we start indexing on staging.","title":"What should be the value of max_gram and min_gram in Elasticsearch?"},{"content":" Having experience in Software \u0026amp; Email Industry. Currently, I am working as a Senior Developer Advocate, India at Elastic.\nStarted journey with Software Engineer. Throughout the journey, I got the opportunity to work with great minds on various stacks, databases \u0026amp; programming languages. Worked with various types of use cases and contributed to solutions.\nI have been wearing multiple hats as a startup guy so got a chance to work on system design, scaling, system architect, coding, maintenance, customer support \u0026amp; tickets, etc. It was not just tech but also marketing \u0026amp; Community engagement.\nI am an Open Source fan \u0026amp; Love to contribute to the community. I like to speak about tech.\nIn my spare time, I like to do some R\u0026amp;Ds, Watch Netflix :D, Blogging.\nI am from Mumbai, India. This is my new spot to share my thoughts and findings of technology.\nThanks.\nAshish Tiwari\n","permalink":"https://ashish.one/about/","summary":"Having experience in Software \u0026amp; Email Industry. Currently, I am working as a Senior Developer Advocate, India at Elastic.\nStarted journey with Software Engineer. Throughout the journey, I got the opportunity to work with great minds on various stacks, databases \u0026amp; programming languages. Worked with various types of use cases and contributed to solutions.\nI have been wearing multiple hats as a startup guy so got a chance to work on system design, scaling, system architect, coding, maintenance, customer support \u0026amp; tickets, etc.","title":"About me"}]